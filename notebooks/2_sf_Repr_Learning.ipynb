{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 - Representation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a low-dimensional representations that are transferable to similar\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the input data I decide to use time grid from Q1.3. (Maybe we should explain why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Freeze/fix the weights of your pretrained network and compute a single embedding\n",
    "vector for each patient. Train a logistic regression (i.e. a linear probe) on the training\n",
    "set to predict the target only from your pretrained embeddings. Compare your results\n",
    "to the supervised performances obtained in prior tasks. (2 pts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from project_1.config import PROCESSED_DATA_DIR, PROJ_ROOT\n",
    "from project_1.loading import *\n",
    "from project_1.dataset import *\n",
    "\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.1 - Pretraining and Linear Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Set A: (183416, 42) Set B: (183495, 42) Set C: (183711, 42)\n",
      "Shapes of labels:\n",
      "Set A: (4000, 2) Set B: (4000, 2) Set C: (4000, 2)\n"
     ]
    }
   ],
   "source": [
    "set_a, set_b, set_c = load_final_data_without_ICU()\n",
    "death_a, death_b, death_c = load_outcomes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will train an encoder model. To ensure fairness in the comparison (as requested), the architecture will build from Attempt 1 from file 5_fb_rnn which was the RNN type model which achieved the highest performance (2-layer LSTM, 64 hidden units, dropout 0.3).\n",
    "\n",
    "How do you monitor this pre-training step?.\n",
    "\n",
    "We monitor the reconstruction loss (MSE loss) over epochs. A decreasing loss indicates the model is learning useful representations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the scarce performances in training, we opted for a bit modified version of the LSTM encoder architecture \n",
    "\n",
    "Layers:\n",
    "* LSTM:\n",
    "      - hidden_size=64\n",
    "      - num_layers=2 (stacked)\n",
    "      - dropout=0.3 (between layers)\n",
    "      - batch_first=True\n",
    "      → Outputs final hidden state (hn[-1])\n",
    "\n",
    "* Fully Connected:\n",
    "      - nn.Linear(64, 64)\n",
    "      → Projects to latent space\n",
    "\n",
    "* Normalization/Activation:\n",
    "      - LayerNorm (over hidden_size)\n",
    "      - ReLU activation\n",
    "      - Dropout (p=0.3)\n",
    "\n",
    "Hyperparameters:\n",
    "   - input_size=40 (matches data)\n",
    "   - hidden_size=64 (latent dim)\n",
    "   - num_layers=2 (deep LSTM)\n",
    "   - dropout=0.3 (regularization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4000, 49, 40])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = create_dataset_from_timeseries(set_a, death_a[\"In-hospital_death\"])\n",
    "validation_dataset = create_dataset_from_timeseries(set_b, death_b[\"In-hospital_death\"])\n",
    "test_dataset = create_dataset_from_timeseries(set_c, death_c[\"In-hospital_death\"])\n",
    "\n",
    "train_dataset.tensors[0].shape # (batch_size, seq_len, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# 1. Define the Encoder\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.3):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        last_hidden = hidden[-1]\n",
    "        out = self.fc(last_hidden)\n",
    "        out = self.layer_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "# 2. Define the Decoder\n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers=2, dropout=0.3):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers,\n",
    "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size*2, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, seq_len):\n",
    "        x = x.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out)\n",
    "\n",
    "# 3. Define the Autoencoder\n",
    "class Seq2SeqAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.3):\n",
    "        super(Seq2SeqAutoencoder, self).__init__()\n",
    "        self.encoder = LSTMEncoder(input_size, hidden_size, num_layers, dropout)\n",
    "        self.decoder = LSTMDecoder(hidden_size, input_size, num_layers, dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        recon_x = self.decoder(latent, x.size(1))\n",
    "        return recon_x, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Configuration\n",
    "input_size = 40  # Matches your data's last dimension\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "n_epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 5. Initialize model\n",
    "model = Seq2SeqAutoencoder(input_size, hidden_size, num_layers, dropout).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Autoencoder...\n",
      "Epoch 10/100 | Train Loss: 23.3340\n",
      "Epoch 20/100 | Train Loss: 19.1827\n",
      "Epoch 30/100 | Train Loss: 17.8545\n",
      "Epoch 40/100 | Train Loss: 17.4380\n",
      "Epoch 50/100 | Train Loss: 16.5605\n",
      "Epoch 60/100 | Train Loss: 13.7803\n",
      "Epoch 70/100 | Train Loss: 17.4921\n",
      "Epoch 80/100 | Train Loss: 13.8805\n",
      "Epoch 90/100 | Train Loss: 10.9647\n",
      "Epoch 100/100 | Train Loss: 10.6347\n"
     ]
    }
   ],
   "source": [
    "# 6. Training Loop with DataLoader (Training Loss Only)\n",
    "print(\"Training Autoencoder...\")\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        recon_x, _ = model(data)\n",
    "        loss = criterion(recon_x, data)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Print training progress\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{n_epochs} | Train Loss: {avg_train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Metrics:\n",
      "Training: AUROC = 0.6663, AUPRC = 0.2805\n",
      "Validation: AUROC = 0.6511, AUPRC = 0.2780\n",
      "Test: AUROC = 0.6145, AUPRC = 0.2477\n",
      "\n",
      "Summary Table:\n",
      "+------------+--------+--------+\n",
      "| Dataset    | AUROC  | AUPRC  |\n",
      "+------------+--------+--------+\n",
      "| Training   | 0.6663 | 0.2805 |\n",
      "| Validation | 0.6511 | 0.2780 |\n",
      "| Test       | 0.6145 | 0.2477 |\n",
      "+------------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# 1. Freeze the pretrained autoencoder\n",
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. Extract embeddings\n",
    "def get_embeddings(dataloader):\n",
    "    model.eval()\n",
    "    embeddings, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data = data.to(device)\n",
    "            _, latent = model(data)\n",
    "            embeddings.append(latent.cpu().numpy())\n",
    "            labels.append(target.numpy())\n",
    "    return np.vstack(embeddings), np.concatenate(labels)\n",
    "\n",
    "X_train, y_train = get_embeddings(train_loader)\n",
    "X_val, y_val = get_embeddings(validation_loader)\n",
    "X_test, y_test = get_embeddings(test_loader)\n",
    "\n",
    "# 3. Train Logistic Regression\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# 4. Evaluate and print results\n",
    "def print_metrics(X, y, set_name):\n",
    "    y_proba = logreg.predict_proba(X)[:, 1]\n",
    "    auroc = roc_auc_score(y, y_proba)\n",
    "    auprc = average_precision_score(y, y_proba)\n",
    "    print(f\"{set_name}: AUROC = {auroc:.4f}, AUPRC = {auprc:.4f}\")\n",
    "\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print_metrics(X_train, y_train, \"Training\")\n",
    "print_metrics(X_val, y_val, \"Validation\")\n",
    "print_metrics(X_test, y_test, \"Test\")\n",
    "\n",
    "# 5. Print formatted table\n",
    "print(\"\\nSummary Table:\")\n",
    "print(\"+------------+--------+--------+\")\n",
    "print(\"| Dataset    | AUROC  | AUPRC  |\")\n",
    "print(\"+------------+--------+--------+\")\n",
    "for name, X, y in [(\"Training\", X_train, y_train),\n",
    "                   (\"Validation\", X_val, y_val),\n",
    "                   (\"Test\", X_test, y_test)]:\n",
    "    y_proba = logreg.predict_proba(X)[:, 1]\n",
    "    auroc = roc_auc_score(y, y_proba)\n",
    "    auprc = average_precision_score(y, y_proba)\n",
    "    print(f\"| {name:<10} | {auroc:.4f} | {auprc:.4f} |\")\n",
    "print(\"+------------+--------+--------+\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
