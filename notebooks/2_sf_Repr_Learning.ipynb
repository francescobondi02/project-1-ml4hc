{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 - Representation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a low-dimensional representations that are transferable to similar\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the input data I decide to use time grid from Q1.3. (Maybe we should explain why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.1 - Pretraining and Linear Probes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Set A: (183416, 42) Set B: (183495, 42) Set C: (183711, 42)\n",
      "Shapes of labels:\n",
      "Set A: (4000, 2) Set B: (4000, 2) Set C: (4000, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from project_1.config import PROCESSED_DATA_DIR, PROJ_ROOT\n",
    "from project_1.loading import *\n",
    "from project_1.dataset import *\n",
    "\n",
    "\n",
    "SEED = 42 #Non l ho usato ovunque sta variabile seed, figa\n",
    "\n",
    "set_a, set_b, set_c = load_final_data_without_ICU()\n",
    "death_a, death_b, death_c = load_outcomes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Model Basis\n",
    "To ensure fair comparison (as requested), the architecture builds upon **Attempt 1** from `5_fb_rnn` - the top-performing RNN variant in previous experiments:\n",
    "\n",
    "- **Base Architecture**: 2-layer LSTM\n",
    "- **Hidden Units**: 64\n",
    "- **Dropout**: 0.3\n",
    "\n",
    "#### Monitoring Approach:\n",
    "**Reconstruction Loss (MSE)** tracked across epochs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modified LSTM Encoder Architecture\n",
    "\n",
    "Due to the scarce performances in training (*ask ChatGPT to explain this choice over the vanilla LSTM better*), we opted for a slightly modified version of the LSTM encoder architecture.\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "##### 1. LSTM Layer\n",
    "- **Hidden size**: 64\n",
    "- **Number of layers**: 2 (stacked)\n",
    "- **Dropout**: 0.3 (between layers)\n",
    "- **Batch first**: True\n",
    "- **Output**: Final hidden state (`hn[-1]`)\n",
    "\n",
    "##### 2. Fully Connected Layer\n",
    "- `nn.Linear(64, 64)`\n",
    "- Projects to latent space\n",
    "\n",
    "##### 3. Normalization & Activation\n",
    "- LayerNorm (over hidden_size)\n",
    "- ReLU activation\n",
    "- Dropout (p=0.3)\n",
    "\n",
    "##### Hyperparameters\n",
    "| Parameter       | Value       | Description                     |\n",
    "|-----------------|-------------|---------------------------------|\n",
    "| `input_size`    | 40          | Matches input data dimension    |\n",
    "| `hidden_size`   | 64          | Latent dimension               |\n",
    "| `num_layers`    | 2           | Deep LSTM architecture         |\n",
    "| `dropout`       | 0.3         | Regularization                 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4000, 49, 40])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################\n",
    "##BOND!\n",
    "\n",
    "\n",
    "#####CHECKA se Ã¨ ok sti dati caricati cosi \n",
    "train_dataset = create_dataset_from_timeseries(set_a, death_a[\"In-hospital_death\"])\n",
    "validation_dataset = create_dataset_from_timeseries(set_b, death_b[\"In-hospital_death\"])\n",
    "test_dataset = create_dataset_from_timeseries(set_c, death_c[\"In-hospital_death\"])\n",
    "\n",
    "train_dataset.tensors[0].shape # (batch_size, seq_len, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# 1. Define the Encoder\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.3):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        last_hidden = hidden[-1]\n",
    "        out = self.fc(last_hidden)\n",
    "        out = self.layer_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "# 2. Define the Decoder\n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers=2, dropout=0.3):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers,\n",
    "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size*2, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, seq_len):\n",
    "        x = x.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out)\n",
    "\n",
    "# 3. Define the Autoencoder\n",
    "class Seq2SeqAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.3):\n",
    "        super(Seq2SeqAutoencoder, self).__init__()\n",
    "        self.encoder = LSTMEncoder(input_size, hidden_size, num_layers, dropout)\n",
    "        self.decoder = LSTMDecoder(hidden_size, input_size, num_layers, dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        recon_x = self.decoder(latent, x.size(1))\n",
    "        return recon_x, latent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Configuration\n",
    "input_size = 40  # Matches your data's last dimension\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "n_epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 5. Initialize model\n",
    "model = Seq2SeqAutoencoder(input_size, hidden_size, num_layers, dropout).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's train!! (Puta madre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Autoencoder...\n",
      "Epoch 10/100 | Train Loss: 23.3340\n",
      "Epoch 20/100 | Train Loss: 19.1827\n",
      "Epoch 30/100 | Train Loss: 17.8545\n",
      "Epoch 40/100 | Train Loss: 17.4380\n",
      "Epoch 50/100 | Train Loss: 16.5605\n",
      "Epoch 60/100 | Train Loss: 13.7803\n",
      "Epoch 70/100 | Train Loss: 17.4921\n",
      "Epoch 80/100 | Train Loss: 13.8805\n",
      "Epoch 90/100 | Train Loss: 10.9647\n",
      "Epoch 100/100 | Train Loss: 10.6347\n"
     ]
    }
   ],
   "source": [
    "# 6. Training Loop with DataLoader (Training Loss Only)\n",
    "print(\"Training Autoencoder...\")\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        recon_x, _ = model(data)\n",
    "        loss = criterion(recon_x, data)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Print training progress\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{n_epochs} | Train Loss: {avg_train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.1.1\n",
    "\n",
    "Freeze/fix the weights of your pretrained network and compute a single embedding\n",
    "vector for each patient. Train a logistic regression (i.e. a linear probe) on the training\n",
    "set to predict the target only from your pretrained embeddings. Compare your results\n",
    "to the supervised performances obtained in prior tasks. \n",
    "\n",
    "Prima funzionava, bisogna rirunnare da capo figa la puttana triponina\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[142]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m             \u001b[38;5;66;03m# labels.append(target.numpy())\u001b[39;00m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.vstack(embeddings), np.concatenate(labels)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m X_train, y_train = \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m X_val, y_val = get_embeddings(validation_loader)\n\u001b[32m     25\u001b[39m X_test, y_test = get_embeddings(test_loader)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[142]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mget_embeddings\u001b[39m\u001b[34m(dataloader)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[32m     17\u001b[39m     data = data.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     _, latent = model(data)\n\u001b[32m     19\u001b[39m     embeddings.append(latent.cpu().numpy())\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# labels.append(target.numpy())\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# 1. Freeze the pretrained autoencoder\n",
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. Extract embeddings\n",
    "def get_embeddings(dataloader):\n",
    "    model.eval()\n",
    "    embeddings, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data = data.to(device)\n",
    "            _, latent = model(data)\n",
    "            embeddings.append(latent.cpu().numpy())\n",
    "            # labels.append(target.numpy())\n",
    "    return np.vstack(embeddings), np.concatenate(labels)\n",
    "\n",
    "X_train, y_train = get_embeddings(train_loader)\n",
    "X_val, y_val = get_embeddings(validation_loader)\n",
    "X_test, y_test = get_embeddings(test_loader)\n",
    "\n",
    "# 3. Train Logistic Regression\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# 4. Evaluate and print results\n",
    "def print_metrics(X, y, set_name):\n",
    "    y_proba = logreg.predict_proba(X)[:, 1]\n",
    "    auroc = roc_auc_score(y, y_proba)\n",
    "    auprc = average_precision_score(y, y_proba)\n",
    "    print(f\"{set_name}: AUROC = {auroc:.4f}, AUPRC = {auprc:.4f}\")\n",
    "\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print_metrics(X_train, y_train, \"Training\")\n",
    "print_metrics(X_val, y_val, \"Validation\")\n",
    "print_metrics(X_test, y_test, \"Test\")\n",
    "\n",
    "# 5. Print formatted table\n",
    "print(\"\\nSummary Table:\")\n",
    "print(\"+------------+--------+--------+\")\n",
    "print(\"| Dataset    | AUROC  | AUPRC  |\")\n",
    "print(\"+------------+--------+--------+\")\n",
    "for name, X, y in [(\"Training\", X_train, y_train),\n",
    "                   (\"Validation\", X_val, y_val),\n",
    "                   (\"Test\", X_test, y_test)]:\n",
    "    y_proba = logreg.predict_proba(X)[:, 1]\n",
    "    auroc = roc_auc_score(y, y_proba)\n",
    "    auprc = average_precision_score(y, y_proba)\n",
    "    print(f\"| {name:<10} | {auroc:.4f} | {auprc:.4f} |\")\n",
    "print(\"+------------+--------+--------+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.2 Simulate label scarcity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train three different supervised (as in Q2.x) models with the same (or as similar as\n",
    "possible) architecture as your pretrained network, but only use 100, 500, and 1000\n",
    "patients from the training set and report your full test set performance (2 pts).\n",
    "â Train three linear probes (as in Q3.1 step 2) using only 100, 500, 1000 labelled\n",
    "patients and report the full test set C performance. (2 pts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model with 100 patients...\n",
      "Epoch 10/100 | Loss: 0.4277\n",
      "Epoch 20/100 | Loss: 0.1752\n",
      "Epoch 30/100 | Loss: 0.0389\n",
      "Epoch 40/100 | Loss: 0.0253\n",
      "Epoch 50/100 | Loss: 0.0155\n",
      "Epoch 60/100 | Loss: 0.0137\n",
      "Epoch 70/100 | Loss: 0.0076\n",
      "Epoch 80/100 | Loss: 0.0066\n",
      "Epoch 90/100 | Loss: 0.0059\n",
      "Epoch 100/100 | Loss: 0.0049\n",
      "Test Performance - AUROC: 0.6382, AUPRC: 0.2443\n",
      "\n",
      "Training model with 500 patients...\n",
      "Epoch 10/100 | Loss: 0.1729\n",
      "Epoch 20/100 | Loss: 0.0736\n",
      "Epoch 30/100 | Loss: 0.0350\n",
      "Epoch 40/100 | Loss: 0.0291\n",
      "Epoch 50/100 | Loss: 0.0179\n",
      "Epoch 60/100 | Loss: 0.0636\n",
      "Epoch 70/100 | Loss: 0.0054\n",
      "Epoch 80/100 | Loss: 0.0025\n",
      "Epoch 90/100 | Loss: 0.0017\n",
      "Epoch 100/100 | Loss: 0.0012\n",
      "Test Performance - AUROC: 0.6935, AUPRC: 0.3100\n",
      "\n",
      "Training model with 1000 patients...\n",
      "Epoch 10/100 | Loss: 0.1490\n",
      "Epoch 20/100 | Loss: 0.0593\n",
      "Epoch 30/100 | Loss: 0.0270\n",
      "Epoch 40/100 | Loss: 0.0162\n",
      "Epoch 50/100 | Loss: 0.0079\n",
      "Epoch 60/100 | Loss: 0.0049\n",
      "Epoch 70/100 | Loss: 0.0083\n",
      "Epoch 80/100 | Loss: 0.0137\n",
      "Epoch 90/100 | Loss: 0.0066\n",
      "Epoch 100/100 | Loss: 0.0058\n",
      "Test Performance - AUROC: 0.7091, AUPRC: 0.3216\n",
      "\n",
      "Final Test Performance:\n",
      "+--------+--------+--------+\n",
      "| N      | AUROC  | AUPRC  |\n",
      "+--------+--------+--------+\n",
      "| 100    | 0.6382 | 0.2443 |\n",
      "| 500    | 0.6935 | 0.3100 |\n",
      "| 1000   | 0.7091 | 0.3216 |\n",
      "+--------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# 1. Define supervised model (same architecture as encoder)\n",
    "class SupervisedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.3):\n",
    "        super(SupervisedLSTM, self).__init__()\n",
    "        # Same architecture as encoder part\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                          batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        last_hidden = hidden[-1]\n",
    "        return torch.sigmoid(self.classifier(last_hidden)).squeeze()\n",
    "\n",
    "# 2. Create subset datasets\n",
    "def create_subset(dataset, n_samples):\n",
    "    indices = torch.randperm(len(dataset))[:n_samples]\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "# Configuration\n",
    "input_size = 40\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "n_epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming train_dataset and test_loader are already defined\n",
    "subset_sizes = [100, 500, 1000]\n",
    "results = {}\n",
    "\n",
    "# 3. Train and evaluate models\n",
    "for size in subset_sizes:\n",
    "    print(f\"\\nTraining model with {size} patients...\")\n",
    "    \n",
    "    # Create subset\n",
    "    subset = create_subset(train_dataset, size)\n",
    "    subset_loader = DataLoader(subset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = SupervisedLSTM(input_size, hidden_size, num_layers, dropout).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for data, target in subset_loader:\n",
    "            data, target = data.to(device), target.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            avg_loss = train_loss / len(subset_loader)\n",
    "            print(f'Epoch {epoch+1}/{n_epochs} | Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    y_true, y_proba = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            outputs = model(data).cpu().numpy()\n",
    "            y_proba.extend(outputs)\n",
    "            y_true.extend(target.numpy())\n",
    "    \n",
    "    auroc = roc_auc_score(y_true, y_proba)\n",
    "    auprc = average_precision_score(y_true, y_proba)\n",
    "    results[size] = (auroc, auprc)\n",
    "    print(f\"Test Performance - AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n",
    "\n",
    "# 4. Print final results\n",
    "print(\"\\nFinal Test Performance:\")\n",
    "print(\"+--------+--------+--------+\")\n",
    "print(\"| N      | AUROC  | AUPRC  |\")\n",
    "print(\"+--------+--------+--------+\")\n",
    "for size, (auroc, auprc) in results.items():\n",
    "    print(f\"| {size:<6} | {auroc:.4f} | {auprc:.4f} |\")\n",
    "print(\"+--------+--------+--------+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train three linear probes (as in Q3.1 step 2) using only 100, 500, 1000 labelled\n",
    "patients and report the full test set C performance. (2 pts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#da fare still"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
