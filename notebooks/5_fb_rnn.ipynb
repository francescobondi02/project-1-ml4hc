{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from project_1.config import PROJ_ROOT, PROCESSED_DATA_DIR\n",
    "from project_1.loading import *\n",
    "from project_1.dataset import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "For basic LSTM, we load the final datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Set A: (183416, 42) Set B: (183495, 42) Set C: (183711, 42)\n",
      "Shapes of labels:\n",
      "Set A: (4000, 2) Set B: (4000, 2) Set C: (4000, 2)\n"
     ]
    }
   ],
   "source": [
    "set_a, set_b, set_c = load_final_data_without_ICU()\n",
    "death_a, death_b, death_c = load_outcomes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 1 - LSTM - Model Implementation (Last State)\n",
    "This basic implementation takes the last hidden state to be used for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, num_classes=1, dropout=0.3):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,       # 41 features per time step\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_size)\n",
    "        out, _ = self.lstm(x)           # out: (batch_size, seq_len, hidden_size)\n",
    "        out = out[:, -1, :]             # Take last time step: (batch_size, hidden_size)\n",
    "        out = self.fc(out)              # (batch_size, num_classes)\n",
    "        return out.squeeze()            # (batch_size,) for BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain TensorDatasets from Time Series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4000, 49, 40])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = create_dataset_from_timeseries(set_a, death_a[\"In-hospital_death\"])\n",
    "validation_dataset = create_dataset_from_timeseries(set_b, death_b[\"In-hospital_death\"])\n",
    "test_dataset = create_dataset_from_timeseries(set_c, death_c[\"In-hospital_death\"])\n",
    "\n",
    "train_dataset.tensors[0].shape # (batch_size, seq_len, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  Train Loss: 0.4448 | AUCROC: 0.5839 | AUPRC: 0.1708\n",
      "  Val   Loss: 0.3443 | AUCROC: 0.7706 | AUPRC: 0.4298\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "  Train Loss: 0.3322 | AUCROC: 0.7925 | AUPRC: 0.4176\n",
      "  Val   Loss: 0.3186 | AUCROC: 0.8301 | AUPRC: 0.4692\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "  Train Loss: 0.3024 | AUCROC: 0.8429 | AUPRC: 0.4827\n",
      "  Val   Loss: 0.3260 | AUCROC: 0.8360 | AUPRC: 0.4712\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "  Train Loss: 0.2888 | AUCROC: 0.8588 | AUPRC: 0.5261\n",
      "  Val   Loss: 0.3137 | AUCROC: 0.8370 | AUPRC: 0.4728\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "  Train Loss: 0.2683 | AUCROC: 0.8826 | AUPRC: 0.5939\n",
      "  Val   Loss: 0.3225 | AUCROC: 0.8322 | AUPRC: 0.4551\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "  Train Loss: 0.2527 | AUCROC: 0.8981 | AUPRC: 0.6183\n",
      "  Val   Loss: 0.3455 | AUCROC: 0.8176 | AUPRC: 0.4558\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "  Train Loss: 0.2385 | AUCROC: 0.9087 | AUPRC: 0.6667\n",
      "  Val   Loss: 0.3446 | AUCROC: 0.8135 | AUPRC: 0.4282\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "  Train Loss: 0.2187 | AUCROC: 0.9217 | AUPRC: 0.7218\n",
      "  Val   Loss: 0.3678 | AUCROC: 0.8134 | AUPRC: 0.4401\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "  Train Loss: 0.2024 | AUCROC: 0.9335 | AUPRC: 0.7458\n",
      "  Val   Loss: 0.3784 | AUCROC: 0.8160 | AUPRC: 0.4466\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "  Train Loss: 0.1847 | AUCROC: 0.9438 | AUPRC: 0.8005\n",
      "  Val   Loss: 0.3914 | AUCROC: 0.8094 | AUPRC: 0.4425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size = train_dataset.tensors[0].shape[-1]\n",
    "model = LSTM_Model(input_size=input_size).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Call the trainig loop (default 10 epochs)\n",
    "model = train_model_with_validation(model, train_loader, validation_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation - Loss: 0.4305 - AUCROC: 0.7935 - AUPRC: 0.4053\n",
      "Test Loss: 0.4305, AUC-ROC: 0.7935, AUC-PRC: 0.4053\n"
     ]
    }
   ],
   "source": [
    "avg_loss, aucroc, auprc = evaluate_model(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {avg_loss:.4f}, AUC-ROC: {aucroc:.4f}, AUC-PRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 2 - LSTM - Model Implementation (Mean Pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model_Pooling(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, num_classes=1, dropout=0.3):\n",
    "        super(LSTM_Model_Pooling, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,       # 40 features per time step\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_size)\n",
    "        out, _ = self.lstm(x)           # out: (batch_size, seq_len, hidden_size)\n",
    "        out = out.mean(dim=1)           # Pooling: (batch_size, hidden_size)   \n",
    "        out = self.fc(out)              # (batch_size, num_classes)\n",
    "        return out.squeeze()            # (batch_size,) for BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  Train Loss: 0.4530 | AUCROC: 0.5249 | AUPRC: 0.1445\n",
      "  Val   Loss: 0.3894 | AUCROC: 0.7208 | AUPRC: 0.3152\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "  Train Loss: 0.3552 | AUCROC: 0.7490 | AUPRC: 0.3290\n",
      "  Val   Loss: 0.3491 | AUCROC: 0.7720 | AUPRC: 0.3694\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "  Train Loss: 0.3279 | AUCROC: 0.8019 | AUPRC: 0.4191\n",
      "  Val   Loss: 0.3447 | AUCROC: 0.7939 | AUPRC: 0.3932\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "  Train Loss: 0.3113 | AUCROC: 0.8295 | AUPRC: 0.4636\n",
      "  Val   Loss: 0.3299 | AUCROC: 0.8112 | AUPRC: 0.4199\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "  Train Loss: 0.2957 | AUCROC: 0.8514 | AUPRC: 0.5069\n",
      "  Val   Loss: 0.3317 | AUCROC: 0.8122 | AUPRC: 0.4249\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "  Train Loss: 0.2783 | AUCROC: 0.8711 | AUPRC: 0.5605\n",
      "  Val   Loss: 0.3387 | AUCROC: 0.8116 | AUPRC: 0.4315\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "  Train Loss: 0.2624 | AUCROC: 0.8892 | AUPRC: 0.5917\n",
      "  Val   Loss: 0.3640 | AUCROC: 0.8048 | AUPRC: 0.4089\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "  Train Loss: 0.2416 | AUCROC: 0.9081 | AUPRC: 0.6505\n",
      "  Val   Loss: 0.3730 | AUCROC: 0.8057 | AUPRC: 0.4064\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "  Train Loss: 0.2235 | AUCROC: 0.9221 | AUPRC: 0.6925\n",
      "  Val   Loss: 0.3928 | AUCROC: 0.7947 | AUPRC: 0.3784\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "  Train Loss: 0.2027 | AUCROC: 0.9345 | AUPRC: 0.7506\n",
      "  Val   Loss: 0.4605 | AUCROC: 0.7880 | AUPRC: 0.3850\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the previous data loaders and train the new model\n",
    "model_pooling = LSTM_Model_Pooling(input_size=input_size).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model_pooling.parameters(), lr=0.001)\n",
    "\n",
    "model_pooling = train_model_with_validation(model_pooling, train_loader, validation_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation - Loss: 0.4214 - AUCROC: 0.7782 - AUPRC: 0.3861\n",
      "Test Loss: 0.4214, AUC-ROC: 0.7782, AUC-PRC: 0.3861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Now evaluate the model\n",
    "avg_loss, aucroc, auprc = evaluate_model(model_pooling, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {avg_loss:.4f}, AUC-ROC: {aucroc:.4f}, AUC-PRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM - Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model_Max_Pooling(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, num_classes=1, dropout=0.3):\n",
    "        super(LSTM_Model_Max_Pooling, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,       # 40 features per time step\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_size)\n",
    "        out, _ = self.lstm(x)           # out: (batch_size, seq_len, hidden_size)\n",
    "        out, _ = out.max(dim=1)           # Pooling: (batch_size, hidden_size)   \n",
    "        out = self.fc(out)              # (batch_size, num_classes)\n",
    "        return out.squeeze()            # (batch_size,) for BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  Train Loss: 0.4585 | AUCROC: 0.5158 | AUPRC: 0.1392\n",
      "  Val   Loss: 0.3912 | AUCROC: 0.7228 | AUPRC: 0.3004\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "  Train Loss: 0.3527 | AUCROC: 0.7596 | AUPRC: 0.3244\n",
      "  Val   Loss: 0.3487 | AUCROC: 0.7926 | AUPRC: 0.3817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "  Train Loss: 0.3213 | AUCROC: 0.8123 | AUPRC: 0.4183\n",
      "  Val   Loss: 0.3369 | AUCROC: 0.7984 | AUPRC: 0.3973\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "  Train Loss: 0.3069 | AUCROC: 0.8350 | AUPRC: 0.4501\n",
      "  Val   Loss: 0.3360 | AUCROC: 0.8107 | AUPRC: 0.4096\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "  Train Loss: 0.2984 | AUCROC: 0.8496 | AUPRC: 0.4876\n",
      "  Val   Loss: 0.3313 | AUCROC: 0.8146 | AUPRC: 0.4152\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "  Train Loss: 0.2821 | AUCROC: 0.8707 | AUPRC: 0.5320\n",
      "  Val   Loss: 0.3426 | AUCROC: 0.8116 | AUPRC: 0.4139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "  Train Loss: 0.2655 | AUCROC: 0.8859 | AUPRC: 0.5670\n",
      "  Val   Loss: 0.3458 | AUCROC: 0.8151 | AUPRC: 0.4182\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "  Train Loss: 0.2476 | AUCROC: 0.9038 | AUPRC: 0.6330\n",
      "  Val   Loss: 0.3595 | AUCROC: 0.8103 | AUPRC: 0.4109\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "  Train Loss: 0.2345 | AUCROC: 0.9136 | AUPRC: 0.6582\n",
      "  Val   Loss: 0.3643 | AUCROC: 0.8095 | AUPRC: 0.4065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "  Train Loss: 0.2221 | AUCROC: 0.9222 | AUPRC: 0.7092\n",
      "  Val   Loss: 0.3815 | AUCROC: 0.8077 | AUPRC: 0.4092\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the previous data loaders and train the new model\n",
    "model_max_pooling = LSTM_Model_Max_Pooling(input_size=input_size).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model_max_pooling.parameters(), lr=0.001)\n",
    "\n",
    "model_max_pooling = train_model_with_validation(model_max_pooling, train_loader, validation_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation - Loss: 0.3814 - AUCROC: 0.8089 - AUPRC: 0.4311\n",
      "Test Loss: 0.3814, AUC-ROC: 0.8089, AUC-PRC: 0.4311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Now evaluate the model\n",
    "avg_loss, aucroc, auprc = evaluate_model(model_max_pooling, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {avg_loss:.4f}, AUC-ROC: {aucroc:.4f}, AUC-PRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 3 - Bidirectional LSTM - Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model_Bi(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, num_classes=1, dropout=0.3):\n",
    "        super(LSTM_Model_Bi, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,       # 41 features per time step\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes) # *2 for bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_size)\n",
    "        out, _ = self.lstm(x)           # out: (batch_size, seq_len, hidden_size)\n",
    "        out = out[:, -1, :]             # Take last time step: (batch_size, hidden_size)\n",
    "        out = self.fc(out)              # (batch_size, num_classes)\n",
    "        return out.squeeze()            # (batch_size,) for BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  Train Loss: 0.4357 | AUCROC: 0.6063 | AUPRC: 0.1974\n",
      "  Val   Loss: 0.3376 | AUCROC: 0.7849 | AUPRC: 0.4241\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "  Train Loss: 0.3227 | AUCROC: 0.8101 | AUPRC: 0.4333\n",
      "  Val   Loss: 0.3157 | AUCROC: 0.8297 | AUPRC: 0.4548\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "  Train Loss: 0.2944 | AUCROC: 0.8525 | AUPRC: 0.5175\n",
      "  Val   Loss: 0.3113 | AUCROC: 0.8367 | AUPRC: 0.4732\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "  Train Loss: 0.2767 | AUCROC: 0.8752 | AUPRC: 0.5510\n",
      "  Val   Loss: 0.3242 | AUCROC: 0.8244 | AUPRC: 0.4472\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "  Train Loss: 0.2643 | AUCROC: 0.8858 | AUPRC: 0.6020\n",
      "  Val   Loss: 0.3300 | AUCROC: 0.8246 | AUPRC: 0.4332\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "  Train Loss: 0.2456 | AUCROC: 0.9061 | AUPRC: 0.6318\n",
      "  Val   Loss: 0.3292 | AUCROC: 0.8215 | AUPRC: 0.4475\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "  Train Loss: 0.2266 | AUCROC: 0.9223 | AUPRC: 0.6769\n",
      "  Val   Loss: 0.3692 | AUCROC: 0.8155 | AUPRC: 0.4376\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "  Train Loss: 0.2147 | AUCROC: 0.9299 | AUPRC: 0.7253\n",
      "  Val   Loss: 0.3747 | AUCROC: 0.8074 | AUPRC: 0.4172\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "  Train Loss: 0.1957 | AUCROC: 0.9428 | AUPRC: 0.7626\n",
      "  Val   Loss: 0.3631 | AUCROC: 0.8074 | AUPRC: 0.4196\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "  Train Loss: 0.1675 | AUCROC: 0.9592 | AUPRC: 0.8227\n",
      "  Val   Loss: 0.4653 | AUCROC: 0.7952 | AUPRC: 0.4122\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model_bi = LSTM_Model_Bi(input_size=input_size).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model_bi.parameters(), lr=0.001)\n",
    "\n",
    "model_bi = train_model_with_validation(model_bi, train_loader, validation_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation - Loss: 0.4649 - AUCROC: 0.7875 - AUPRC: 0.4095\n",
      "Test Loss: 0.4649, AUC-ROC: 0.7875, AUC-PRC: 0.4095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Now evaluate\n",
    "avg_loss, aucroc, auprc = evaluate_model(model_bi, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {avg_loss:.4f}, AUC-ROC: {aucroc:.4f}, AUC-PRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers - Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=1, nhead=4, num_layers=2, dim_feedforward=128, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Project input features to model dimension\n",
    "        self.embedding = nn.Linear(input_size, dim_feedforward)\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.pos_encoder = PositionalEncoding(dim_feedforward, dropout)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_feedforward,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward * 2,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Final classifier\n",
    "        self.fc = nn.Linear(dim_feedforward, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_size)\n",
    "\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1) # (batch, 1, input_size)\n",
    "\n",
    "        x = self.embedding(x)                # (batch, seq_len, d_model)\n",
    "        #print(\"After embedding:\", x.shape)  # Debug print\n",
    "        x = self.pos_encoder(x)\n",
    "        #print(\"After pos encoding:\", x.shape)  # Debug print\n",
    "        x = self.transformer_encoder(x)      # (batch, seq_len, d_model)\n",
    "        #print(\"After transformer encoder:\", x.shape)\n",
    "\n",
    "        x = x.mean(dim=1)                    # mean pooling over time\n",
    "        #print(\"After pooling:\", x.shape)     # Debug print\n",
    "        out = self.fc(x).squeeze()           # (batch,)\n",
    "        #print(\"After fc:\", out.shape)        # Debug print\n",
    "        return out\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=500):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  Train Loss: 0.3751 | AUCROC: 0.6917 | AUPRC: 0.2804\n",
      "  Val   Loss: 0.3474 | AUCROC: 0.7974 | AUPRC: 0.3945\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "  Train Loss: 0.3468 | AUCROC: 0.7645 | AUPRC: 0.3664\n",
      "  Val   Loss: 0.3305 | AUCROC: 0.8070 | AUPRC: 0.4199\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "  Train Loss: 0.3320 | AUCROC: 0.7975 | AUPRC: 0.3856\n",
      "  Val   Loss: 0.3406 | AUCROC: 0.8108 | AUPRC: 0.4231\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "  Train Loss: 0.3185 | AUCROC: 0.8179 | AUPRC: 0.4327\n",
      "  Val   Loss: 0.3310 | AUCROC: 0.8222 | AUPRC: 0.4498\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "  Train Loss: 0.3166 | AUCROC: 0.8187 | AUPRC: 0.4544\n",
      "  Val   Loss: 0.3287 | AUCROC: 0.8185 | AUPRC: 0.4276\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "  Train Loss: 0.3059 | AUCROC: 0.8349 | AUPRC: 0.4808\n",
      "  Val   Loss: 0.3317 | AUCROC: 0.8225 | AUPRC: 0.4531\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "  Train Loss: 0.3004 | AUCROC: 0.8457 | AUPRC: 0.4809\n",
      "  Val   Loss: 0.4012 | AUCROC: 0.7772 | AUPRC: 0.4076\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "  Train Loss: 0.2944 | AUCROC: 0.8518 | AUPRC: 0.5068\n",
      "  Val   Loss: 0.4364 | AUCROC: 0.7420 | AUPRC: 0.3674\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "  Train Loss: 0.2895 | AUCROC: 0.8585 | AUPRC: 0.5173\n",
      "  Val   Loss: 0.3946 | AUCROC: 0.7857 | AUPRC: 0.3622\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "  Train Loss: 0.2806 | AUCROC: 0.8673 | AUPRC: 0.5594\n",
      "  Val   Loss: 0.4266 | AUCROC: 0.7459 | AUPRC: 0.3902\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Train the Transformer model\n",
    "model_transformer = TransformerClassifier(input_size=input_size).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model_transformer.parameters(), lr=0.001)\n",
    "\n",
    "model_transformer = train_model_with_validation(model_transformer, train_loader, validation_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation - Loss: 0.3494 - AUCROC: 0.8292 - AUPRC: 0.4783\n",
      "Test Loss: 0.3494, AUC-ROC: 0.8292, AUC-PRC: 0.4783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "avg_loss, aucroc, auprc = evaluate_model(model_transformer, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {avg_loss:.4f}, AUC-ROC: {aucroc:.4f}, AUC-PRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2.3 - Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Set A: (183416, 43) Set B: (183495, 43) Set C: (183711, 43)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecordID</th>\n",
       "      <th>Time</th>\n",
       "      <th>Age</th>\n",
       "      <th>BUN</th>\n",
       "      <th>Creatinine</th>\n",
       "      <th>GCS</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>HCO3</th>\n",
       "      <th>HCT</th>\n",
       "      <th>...</th>\n",
       "      <th>PaCO2</th>\n",
       "      <th>PaO2</th>\n",
       "      <th>pH</th>\n",
       "      <th>DiasABP</th>\n",
       "      <th>MAP</th>\n",
       "      <th>SaO2</th>\n",
       "      <th>SysABP</th>\n",
       "      <th>Lactate</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>TroponinI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132539.0</td>\n",
       "      <td>2025-03-10 00:00:00</td>\n",
       "      <td>54.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132539.0</td>\n",
       "      <td>2025-03-10 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132539.0</td>\n",
       "      <td>2025-03-10 02:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132539.0</td>\n",
       "      <td>2025-03-10 03:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132539.0</td>\n",
       "      <td>2025-03-10 04:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.7</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RecordID                Time   Age  BUN  Creatinine   GCS  Gender  Glucose  \\\n",
       "0  132539.0 2025-03-10 00:00:00  54.0  NaN         NaN   NaN     0.0      NaN   \n",
       "1  132539.0 2025-03-10 01:00:00   NaN  NaN         NaN  15.0     NaN      NaN   \n",
       "2  132539.0 2025-03-10 02:00:00   NaN  NaN         NaN   NaN     NaN      NaN   \n",
       "3  132539.0 2025-03-10 03:00:00   NaN  NaN         NaN   NaN     NaN      NaN   \n",
       "4  132539.0 2025-03-10 04:00:00   NaN  NaN         NaN  15.0     NaN      NaN   \n",
       "\n",
       "   HCO3   HCT  ...  PaCO2  PaO2  pH  DiasABP  MAP  SaO2  SysABP  Lactate  \\\n",
       "0   NaN   NaN  ...    NaN   NaN NaN      NaN  NaN   NaN     NaN      NaN   \n",
       "1   NaN   NaN  ...    NaN   NaN NaN      NaN  NaN   NaN     NaN      NaN   \n",
       "2   NaN   NaN  ...    NaN   NaN NaN      NaN  NaN   NaN     NaN      NaN   \n",
       "3   NaN   NaN  ...    NaN   NaN NaN      NaN  NaN   NaN     NaN      NaN   \n",
       "4   NaN  33.7  ...    NaN   NaN NaN      NaN  NaN   NaN     NaN      NaN   \n",
       "\n",
       "   Cholesterol  TroponinI  \n",
       "0          NaN        NaN  \n",
       "1          NaN        NaN  \n",
       "2          NaN        NaN  \n",
       "3          NaN        NaN  \n",
       "4          NaN        NaN  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For this part, we need to load the initial data\n",
    "set_a_initial, set_b_initial, set_c_initial = load_basic_data()\n",
    "set_a_initial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the TZV Dataframe (following Horn et al.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def build_TZV_dataframe(original_df, label_df, base_time=\"2025-03-10 00:00:00\", duration_hours=48):\n",
    "    \"\"\"\n",
    "    Build a long-format dataframe with columns [T, Z, V, y] from an original wide dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "        original_df (pd.DataFrame): DataFrame with columns [RecordID, Time, f1, f2, ..., f41].\n",
    "        label_df (pd.DataFrame): DataFrame with columns [RecordID, y] containing the label for each RecordID.\n",
    "        base_time (str): Base time used for normalizing the Time column.\n",
    "        duration_hours (int): The duration (in hours) from base_time over which Time is normalized (here, 48 hours).\n",
    "    \n",
    "    Returns:\n",
    "        long_df (pd.DataFrame): Long-format dataframe with columns:\n",
    "                                T: normalized time [0, 1],\n",
    "                                Z: index of the feature,\n",
    "                                V: scaled measurement value,\n",
    "                                y: label corresponding to RecordID.\n",
    "        feature_to_index (dict): Mapping from original feature names to integer indices.\n",
    "    \"\"\"\n",
    "    # Merge the labels with the original dataframe using RecordID.\n",
    "    df = original_df.copy().merge(label_df, on=\"RecordID\", how=\"left\")\n",
    "    \n",
    "    # Convert Time to datetime and compute normalized time T.\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"Time\"])\n",
    "    start_time = pd.to_datetime(base_time)\n",
    "    end_time = start_time + pd.Timedelta(hours=duration_hours)\n",
    "    total_seconds = (end_time - start_time).total_seconds()\n",
    "    df[\"T\"] = (df[\"Time\"] - start_time).dt.total_seconds() / total_seconds\n",
    "    \n",
    "    # Identify feature columns: all columns except RecordID, Time, T, and y.\n",
    "    feature_cols = [col for col in df.columns if col not in [\"RecordID\", \"Time\", \"T\", \"In-hospital_death\"]]\n",
    "    \n",
    "    # Scale each feature individually using MinMaxScaler.\n",
    "    scaler = MinMaxScaler()\n",
    "    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "    \n",
    "    # Melt the dataframe from wide to long format.\n",
    "    # The id_vars (\"T\" and \"y\") are preserved for each measurement.\n",
    "    long_df = pd.melt(df, id_vars=[\"T\", \"In-hospital_death\"], value_vars=feature_cols, \n",
    "                      var_name=\"Z\", value_name=\"V\")\n",
    "    \n",
    "    # Map feature names to indices for the \"Z\" column.\n",
    "    feature_to_index = {feat: idx for idx, feat in enumerate(feature_cols)}\n",
    "    long_df[\"Z\"] = long_df[\"Z\"].map(feature_to_index)\n",
    "    \n",
    "    # Sort the final dataframe by normalized time T and reset the index.\n",
    "    long_df = long_df.sort_values(\"T\").reset_index(drop=True)\n",
    "    long_df = long_df.dropna(subset=[\"V\"])\n",
    "    \n",
    "    return long_df, feature_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1456736, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>In-hospital_death</th>\n",
       "      <th>Z</th>\n",
       "      <th>V</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0.167910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0.186567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.528571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.347619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.390476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1606</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0.160448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.542857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0.175373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        T  In-hospital_death   Z         V\n",
       "0     0.0                  0   0  0.520000\n",
       "417   0.0                  0  34  0.167910\n",
       "551   0.0                  0  34  0.186567\n",
       "565   0.0                  1   8  0.528571\n",
       "566   0.0                  0   8  0.347619\n",
       "1229  0.0                  0   8  0.390476\n",
       "1307  0.0                  0   8  0.433333\n",
       "1606  0.0                  0  34  0.160448\n",
       "1607  0.0                  1   8  0.542857\n",
       "1618  0.0                  0  34  0.175373"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the TZV dataframes\n",
    "TZV_a, feature_to_index_a = build_TZV_dataframe(set_a_initial, death_a)\n",
    "TZV_b, feature_to_index_b = build_TZV_dataframe(set_b_initial, death_b)\n",
    "TZV_c, feature_to_index_c = build_TZV_dataframe(set_c_initial, death_c)\n",
    "\n",
    "print(TZV_a.shape)\n",
    "TZV_a.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1456736"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for the total number of not NaN values under some specified columns\n",
    "selected_cols = [col for col in set_a_initial.columns if col not in [\"RecordID\", \"Time\"]]\n",
    "set_a_initial[selected_cols].notna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checked that the number of not NaN values is the same as the rows of the new dataframe! Let's go\n",
    "(We have to believe in this format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the TZV Format with a Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1456736, 3]) torch.Size([1459862, 3]) torch.Size([1454964, 3])\n"
     ]
    }
   ],
   "source": [
    "# Remove the In-hospital_death column from the TZV dataframes, but save it\n",
    "y_a = TZV_a.pop(\"In-hospital_death\")\n",
    "y_b = TZV_b.pop(\"In-hospital_death\")\n",
    "y_c = TZV_c.pop(\"In-hospital_death\")\n",
    "\n",
    "# Convert the TZV dataframes to PyTorch tensors\n",
    "X_a = torch.tensor(TZV_a[[\"T\", \"Z\", \"V\"]].values, dtype=torch.float32)\n",
    "X_b = torch.tensor(TZV_b[[\"T\", \"Z\", \"V\"]].values, dtype=torch.float32)\n",
    "X_c = torch.tensor(TZV_c[[\"T\", \"Z\", \"V\"]].values, dtype=torch.float32)\n",
    "print(X_a.shape, X_b.shape, X_c.shape)\n",
    "\n",
    "# Create the datasets and dataloaders\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "dataset_a = TensorDataset(X_a, torch.tensor(y_a.values, dtype=torch.float32))\n",
    "dataset_b = TensorDataset(X_b, torch.tensor(y_b.values, dtype=torch.float32))\n",
    "dataset_c = TensorDataset(X_c, torch.tensor(y_c.values, dtype=torch.float32))\n",
    "\n",
    "loader_a = DataLoader(dataset_a, batch_size=64, shuffle=True)\n",
    "loader_b = DataLoader(dataset_b, batch_size=64, shuffle=False)\n",
    "loader_c = DataLoader(dataset_c, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model_tvz\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m model_tvz \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_with_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_tvz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/stuff/ETH/FS25/ML for Healthcare/project-1-ml4hc/project_1/dataset.py:82\u001b[0m, in \u001b[0;36mtrain_model_with_validation\u001b[0;34m(model, train_loader, valid_loader, criterion, optimizer, device, num_epochs, scheduler)\u001b[0m\n\u001b[1;32m     80\u001b[0m elif outputs.shape[-1] == 2:\n\u001b[1;32m     81\u001b[0m     probs = torch.softmax(outputs, dim=-1)[:, 1]\n\u001b[0;32m---> 82\u001b[0m else:\n\u001b[1;32m     83\u001b[0m     raise ValueError(\"Unexpected output shape for binary classification\")\n\u001b[1;32m     85\u001b[0m all_preds_train.append(probs.detach().cpu())\n",
      "File \u001b[0;32m~/anaconda3/envs/TUM/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/TUM/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_tvz = TransformerClassifier(input_size=3).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model_tvz.parameters(), lr=0.001)\n",
    "\n",
    "model_tvz = train_model_with_validation(model_tvz, loader_a, loader_b, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training takes a lot of time, it's 1 million of rows per table...\n",
    "\n",
    "I could make a script.py and run it on Euler..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TUM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
