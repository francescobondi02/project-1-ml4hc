{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1.1 - Data Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-28 20:40:44.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mproject_1.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: /Users/francescobondi/Desktop/stuff/ETH/FS25/ML for Healthcare/project-1-ml4hc\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from project_1.config import PROJ_ROOT, DATA_DIRECTORY, PROCESSED_DATA_DIR, LOGS_DIR\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we extract the three outcomes (A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from /Users/francescobondi/Desktop/stuff/ETH/FS25/ML for Healthcare/project-1-ml4hc/data/data_1/predicting-mortality-of-icu-patients-the-physionet-computing-in-cardiology-challenge-2012-1.0.0/set-a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files in set-a: 100%|██████████| 4000/4000 [00:05<00:00, 676.12file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from /Users/francescobondi/Desktop/stuff/ETH/FS25/ML for Healthcare/project-1-ml4hc/data/data_1/predicting-mortality-of-icu-patients-the-physionet-computing-in-cardiology-challenge-2012-1.0.0/set-b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files in set-b: 100%|██████████| 4000/4000 [00:05<00:00, 704.80file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from /Users/francescobondi/Desktop/stuff/ETH/FS25/ML for Healthcare/project-1-ml4hc/data/data_1/predicting-mortality-of-icu-patients-the-physionet-computing-in-cardiology-challenge-2012-1.0.0/set-c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files in set-c: 100%|██████████| 4000/4000 [00:07<00:00, 568.09file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter  RecordID   Time   Age   BUN  Creatinine   GCS  Gender  Glucose  \\\n",
      "0          132592.0  00:00  35.0   NaN         NaN   NaN     0.0      NaN   \n",
      "1          132592.0  01:20   NaN   NaN         NaN  15.0     NaN      NaN   \n",
      "2          132592.0  02:20   NaN   NaN         NaN   NaN     NaN      NaN   \n",
      "3          132592.0  02:36   NaN  68.0         2.3   NaN     NaN    603.0   \n",
      "4          132592.0  03:20   NaN   NaN         NaN   NaN     NaN      NaN   \n",
      "\n",
      "Parameter  HCO3   HCT  ...  PaCO2  PaO2  pH  DiasABP  MAP  SaO2  SysABP  \\\n",
      "0           NaN   NaN  ...    NaN   NaN NaN      NaN  NaN   NaN     NaN   \n",
      "1           NaN   NaN  ...    NaN   NaN NaN      NaN  NaN   NaN     NaN   \n",
      "2           NaN   NaN  ...    NaN   NaN NaN      NaN  NaN   NaN     NaN   \n",
      "3          11.0  25.5  ...    NaN   NaN NaN      NaN  NaN   NaN     NaN   \n",
      "4           NaN   NaN  ...    NaN   NaN NaN      NaN  NaN   NaN     NaN   \n",
      "\n",
      "Parameter  Lactate  Cholesterol  TroponinI  \n",
      "0              NaN          NaN        NaN  \n",
      "1              NaN          NaN        NaN  \n",
      "2              NaN          NaN        NaN  \n",
      "3              NaN          NaN        NaN  \n",
      "4              NaN          NaN        NaN  \n",
      "\n",
      "[5 rows x 43 columns]\n",
      "(299264, 43)\n"
     ]
    }
   ],
   "source": [
    "# Load the data (for the first time)\n",
    "sets = [\"a\", \"b\", \"c\"]\n",
    "sets_dict = {}\n",
    "for set_name in sets:\n",
    "    set_folder = DATA_DIRECTORY / f\"set-{set_name}\"\n",
    "    print(f\"Reading data from {set_folder}\")\n",
    "\n",
    "    patient_data_list = []\n",
    "\n",
    "    files = [f for f in os.listdir(set_folder) if f.endswith(\".txt\")]\n",
    "    for filename in tqdm(\n",
    "        files, desc=f\"Processing files in set-{set_name}\", unit=\"file\"\n",
    "    ):\n",
    "        file_path = os.path.join(set_folder, filename)\n",
    "\n",
    "        # Read patient file\n",
    "        patient_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Extract RecordID from the 'Parameter' column where value is 'RecordID'\n",
    "        record_id = patient_df.loc[\n",
    "            patient_df[\"Parameter\"] == \"RecordID\", \"Value\"\n",
    "        ].values[0]\n",
    "\n",
    "        # Pivot to transform measurements, using 'first' to resolve duplicates\n",
    "        patient_df = patient_df.pivot_table(\n",
    "            index=\"Time\", columns=\"Parameter\", values=\"Value\", aggfunc=\"first\"\n",
    "        )\n",
    "        patient_df.reset_index(inplace=True)\n",
    "\n",
    "        # Remove any existing RecordID column and insert our extracted one as the first column\n",
    "        if \"RecordID\" in patient_df.columns:\n",
    "            patient_df.drop(columns=[\"RecordID\"], inplace=True)\n",
    "        patient_df.insert(0, \"RecordID\", record_id)\n",
    "\n",
    "        # Append the processed DataFrame to the list\n",
    "        patient_data_list.append(patient_df)\n",
    "\n",
    "    # Combine all patient data into a single DataFrame\n",
    "    patients_df = pd.concat(patient_data_list, ignore_index=True)\n",
    "\n",
    "    # Store in dictionary\n",
    "    sets_dict[\"set_\" + set_name] = patients_df\n",
    "\n",
    "# Output the first 5 rows of the Set A DataFrame\n",
    "print(sets_dict[\"set_a\"].head())\n",
    "print(sets_dict[\"set_a\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory if it doesn't exist\n",
    "set_a_path = Path(PROCESSED_DATA_DIR / \"set_a\")\n",
    "set_b_path = Path(PROCESSED_DATA_DIR / \"set_b\")\n",
    "set_c_path = Path(PROCESSED_DATA_DIR / \"set_c\")\n",
    "\n",
    "set_a_path.mkdir(parents=True, exist_ok=True)\n",
    "set_b_path.mkdir(parents=True, exist_ok=True)\n",
    "set_c_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretize Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 299264/299264 [00:01<00:00, 261194.19it/s]\n",
      "/var/folders/vm/dlvhdp0n0jg_tp24ng5z84080000gn/T/ipykernel_57582/1472351480.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  return ts.ceil(\"H\")\n",
      "100%|██████████| 299264/299264 [00:06<00:00, 45192.46it/s]\n",
      "100%|██████████| 299068/299068 [00:01<00:00, 270458.60it/s]s/set]\n",
      "/var/folders/vm/dlvhdp0n0jg_tp24ng5z84080000gn/T/ipykernel_57582/1472351480.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  return ts.ceil(\"H\")\n",
      "100%|██████████| 299068/299068 [00:06<00:00, 48121.36it/s]\n",
      "100%|██████████| 300020/300020 [00:01<00:00, 270617.43it/s]s/set]\n",
      "/var/folders/vm/dlvhdp0n0jg_tp24ng5z84080000gn/T/ipykernel_57582/1472351480.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  return ts.ceil(\"H\")\n",
      "100%|██████████| 300020/300020 [00:06<00:00, 48990.77it/s]\n",
      "Discretizing time: 100%|██████████| 3/3 [00:22<00:00,  7.55s/set]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter  RecordID                Time   Age  BUN  Creatinine   GCS  Gender  \\\n",
      "0          132539.0 2025-03-10 00:00:00  54.0  NaN         NaN   NaN     0.0   \n",
      "1          132539.0 2025-03-10 01:00:00   NaN  NaN         NaN  15.0     NaN   \n",
      "2          132539.0 2025-03-10 02:00:00   NaN  NaN         NaN   NaN     NaN   \n",
      "3          132539.0 2025-03-10 03:00:00   NaN  NaN         NaN   NaN     NaN   \n",
      "4          132539.0 2025-03-10 04:00:00   NaN  NaN         NaN  15.0     NaN   \n",
      "\n",
      "Parameter  Glucose  HCO3   HCT  ...  PaCO2  PaO2  pH  DiasABP  MAP  SaO2  \\\n",
      "0              NaN   NaN   NaN  ...    NaN   NaN NaN      NaN  NaN   NaN   \n",
      "1              NaN   NaN   NaN  ...    NaN   NaN NaN      NaN  NaN   NaN   \n",
      "2              NaN   NaN   NaN  ...    NaN   NaN NaN      NaN  NaN   NaN   \n",
      "3              NaN   NaN   NaN  ...    NaN   NaN NaN      NaN  NaN   NaN   \n",
      "4              NaN   NaN  33.7  ...    NaN   NaN NaN      NaN  NaN   NaN   \n",
      "\n",
      "Parameter  SysABP  Lactate  Cholesterol  TroponinI  \n",
      "0             NaN      NaN          NaN        NaN  \n",
      "1             NaN      NaN          NaN        NaN  \n",
      "2             NaN      NaN          NaN        NaN  \n",
      "3             NaN      NaN          NaN        NaN  \n",
      "4             NaN      NaN          NaN        NaN  \n",
      "\n",
      "[5 rows x 43 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "base_date = \"2025-03-10\"  # Format is YYYY-MM-DD (date is chosen randomly)\n",
    "\n",
    "\n",
    "# Function to fix invalid times\n",
    "def adjust_time(time_str, base_date):\n",
    "    # Split hours and minutes\n",
    "    hours, minutes = map(int, time_str.split(\":\"))\n",
    "\n",
    "    # Calculate valid hour & days overflow\n",
    "    day_offset = hours // 24  # Number of days to add\n",
    "    new_hour = hours % 24  # Wrapped hour (0-23)\n",
    "\n",
    "    # Create the corrected datetime\n",
    "    corrected_datetime = datetime.strptime(base_date, \"%Y-%m-%d\") + timedelta(\n",
    "        days=day_offset, hours=new_hour, minutes=minutes\n",
    "    )\n",
    "\n",
    "    return corrected_datetime\n",
    "\n",
    "\n",
    "def round_up_next_hour(ts):\n",
    "    # If timestamp is exactly on the hour, return it unchanged.\n",
    "    if ts.minute == 0 and ts.second == 0 and ts.microsecond == 0:\n",
    "        return ts\n",
    "    # Otherwise, round up to the next hour.\n",
    "    return ts.ceil(\"H\")\n",
    "\n",
    "\n",
    "# Apply the functions to the 'Time' column (for each sets)\n",
    "for set_name, set_df in tqdm(sets_dict.items(), desc=\"Discretizing time\", unit=\"set\"):\n",
    "    # Convert 'Time' column from string to datetime using the adjust_time function.\n",
    "    set_df[\"Time\"] = set_df[\"Time\"].progress_apply(lambda x: adjust_time(x, base_date))\n",
    "    # Round up each timestamp to the next hour, except if it is exactly on the hour.\n",
    "    set_df[\"Time\"] = set_df[\"Time\"].progress_apply(round_up_next_hour)\n",
    "    # Group by RecordID and discretized Time, taking the mean in case of multiple measurements.\n",
    "    sets_dict[set_name] = set_df.groupby([\"RecordID\", \"Time\"], as_index=False).mean()\n",
    "\n",
    "# Output the first 5 rows of the Set A DataFrame\n",
    "print(sets_dict[\"set_a\"].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the results into Parquet Format (efficient for large datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Storing DataFrames: 100%|██████████| 3/3 [00:00<00:00, 16.02set/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/francescobondi/Desktop/stuff/ETH/FS25/ML for Healthcare/project-1-ml4hc/data/processed/set_a/set_a.parquet\n",
      "Saved /Users/francescobondi/Desktop/stuff/ETH/FS25/ML for Healthcare/project-1-ml4hc/data/processed/set_b/set_b.parquet\n",
      "Saved /Users/francescobondi/Desktop/stuff/ETH/FS25/ML for Healthcare/project-1-ml4hc/data/processed/set_c/set_c.parquet\n",
      "\n",
      "All DataFrames have been saved to Parquet format.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for set_name, set_df in tqdm(sets_dict.items(), desc=\"Storing DataFrames\", unit=\"set\"):\n",
    "    output_path = PROCESSED_DATA_DIR / f\"{set_name}\" / f\"{set_name}.parquet\"\n",
    "    set_df.to_parquet(output_path, index=False, engine=\"pyarrow\")\n",
    "    print(f\"Saved {output_path}\")\n",
    "\n",
    "print(\"\\nAll DataFrames have been saved to Parquet format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1.3 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that -1 values in the static variables only appear in the first row for each patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vm/dlvhdp0n0jg_tp24ng5z84080000gn/T/ipykernel_57582/2365107117.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = df.groupby(\"RecordID\").apply(check_first_row).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No violations found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vm/dlvhdp0n0jg_tp24ng5z84080000gn/T/ipykernel_57582/2365107117.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = df.groupby(\"RecordID\").apply(check_first_row).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No violations found.\n",
      "No violations found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vm/dlvhdp0n0jg_tp24ng5z84080000gn/T/ipykernel_57582/2365107117.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = df.groupby(\"RecordID\").apply(check_first_row).reset_index()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def check_neg_vals(df):\n",
    "    # Group by RecordID and check for -1\n",
    "    def check_first_row(group):\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"Age_First_Only\": (group[\"Age\"].iloc[1:] != -1).all(),\n",
    "                \"Gender_First_Only\": (group[\"Gender\"].iloc[1:] != -1).all(),\n",
    "                \"Height_First_Only\": (group[\"Height\"].iloc[1:] != -1).all(),\n",
    "                \"Weight_First_Only\": (group[\"Weight\"].iloc[1:] != -1).all(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Apply the function to each group\n",
    "    result = df.groupby(\"RecordID\").apply(check_first_row).reset_index()\n",
    "\n",
    "    # Replace -1 with NA in the specified columns\n",
    "    df.replace(\n",
    "        {\n",
    "            \"Height\": {-1: None},\n",
    "            \"Age\": {-1: None},\n",
    "            \"Weight\": {-1: None},\n",
    "            \"ICUType\": {-1: None},\n",
    "            \"Gender\": {-1: None},\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # Check if any RecordID has -1 in non-first rows\n",
    "    violations = result[\n",
    "        (~result[\"Age_First_Only\"])\n",
    "        | (~result[\"Gender_First_Only\"])\n",
    "        | (~result[\"Height_First_Only\"])\n",
    "        | (~result[\"Weight_First_Only\"])\n",
    "    ]\n",
    "\n",
    "    # Print violations\n",
    "    # print(violations)\n",
    "    if violations.empty:\n",
    "        print(\"No violations found.\")\n",
    "\n",
    "\n",
    "for set_key, df in sets_dict.items():\n",
    "    check_neg_vals(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data for set_a saved as /Users/francescobondi/Desktop/stuff/ETH/FS25/ML for Healthcare/project-1-ml4hc/data/processed/set_a/set_a_before_imputation.parquet\n",
      "Cleaned data for set_b saved as /Users/francescobondi/Desktop/stuff/ETH/FS25/ML for Healthcare/project-1-ml4hc/data/processed/set_b/set_b_before_imputation.parquet\n",
      "Cleaned data for set_c saved as /Users/francescobondi/Desktop/stuff/ETH/FS25/ML for Healthcare/project-1-ml4hc/data/processed/set_c/set_c_before_imputation.parquet\n"
     ]
    }
   ],
   "source": [
    "def clean_df(df):\n",
    "    \"\"\"\n",
    "    Clean the input DataFrame according to the following rules:\n",
    "      - Missing value handling: For Age, Gender, Height, ICUType, Weight, set -1 to NA.\n",
    "      - Height outlier removal: Set Height to NA if < 100 cm or >= 300 cm.\n",
    "      - Weight outlier removal: Set Weight to NA if < 20 kg or >= 300 kg.\n",
    "      - PaO2 corrections: Set PaO2 equal to 0 to NA; if PaO2 equals 7.47, correct it to 74.7 because it's the only value out of range\n",
    "      - pH unit correction: If pH is between 65 and 80, divide by 10; if between 650 and 800, divide by 100.\n",
    "      - Temperature corrections: Set Temp to NA if Temp is less than 20.\n",
    "    \"\"\"\n",
    "    # 1. Missing value handling: Replace -1 with np.nan for selected columns.\n",
    "    missing_cols = [\"Age\", \"Gender\", \"Height\", \"ICUType\", \"Weight\"]\n",
    "    for col in missing_cols:\n",
    "        df.loc[df[col] == -1, col] = np.nan\n",
    "\n",
    "    # 2. Height outlier removal: Set Height to NA if < 100 or >= 300.\n",
    "    df.loc[(df[\"Height\"] < 100) | (df[\"Height\"] >= 300), \"Height\"] = np.nan\n",
    "\n",
    "    # 3. Weight outlier removal: Set Weight to NA if < 20 or >= 300.\n",
    "    df.loc[(df[\"Weight\"] < 20) | (df[\"Weight\"] >= 300), \"Weight\"] = np.nan\n",
    "\n",
    "    # 4. PaO2 corrections:\n",
    "    #    Set PaO2 equal to 0 to NA, and if PaO2 is 7.47, correct it to 74.7.\n",
    "    df.loc[df[\"PaO2\"] == 0, \"PaO2\"] = np.nan\n",
    "    df.loc[df[\"PaO2\"] == 7.47, \"PaO2\"] = 74.7\n",
    "\n",
    "    # 5. pH unit correction:\n",
    "    #    If pH is between 65 and 80, divide by 10; if between 650 and 800, divide by 100.\n",
    "    def correct_ph(ph):\n",
    "        if pd.isna(ph):\n",
    "            return ph\n",
    "        if 65 <= ph <= 80:\n",
    "            return ph / 10.0\n",
    "        elif 650 <= ph <= 800:\n",
    "            return ph / 100.0\n",
    "        else:\n",
    "            return ph\n",
    "\n",
    "    df[\"pH\"] = df[\"pH\"].apply(correct_ph)\n",
    "\n",
    "    # 6. Temperature corrections: Set Temp to NA if Temp is < 20.\n",
    "    df.loc[df[\"Temp\"] < 20, \"Temp\"] = np.nan\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "for set_key, df in sets_dict.items():\n",
    "    # Clean the DataFrame\n",
    "    cleaned_df = clean_df(df)\n",
    "    sets_dict[set_key] = cleaned_df  # Update dictionary (optional)\n",
    "\n",
    "    # Export to Parquet file (e.g., \"set_a_cleaned.parquet\")\n",
    "    output_filename = (\n",
    "        PROCESSED_DATA_DIR / f\"{set_key}\" / f\"{set_key}_before_imputation.parquet\"\n",
    "    )\n",
    "    cleaned_df.to_parquet(output_filename, index=False)\n",
    "    print(f\"Cleaned data for {set_key} saved as {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation for Missing Static Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter  RecordID                Time   Age Weight  Height Gender  BUN  \\\n",
      "0          132539.0 2025-03-10 00:00:00  54.0  76.06  160.79    0.0  NaN   \n",
      "1          132539.0 2025-03-10 01:00:00  54.0  76.06  160.79    0.0  NaN   \n",
      "2          132539.0 2025-03-10 02:00:00  54.0  76.06  160.79    0.0  NaN   \n",
      "3          132539.0 2025-03-10 03:00:00  54.0  76.06  160.79    0.0  NaN   \n",
      "4          132539.0 2025-03-10 04:00:00  54.0  76.06  160.79    0.0  NaN   \n",
      "5          132539.0 2025-03-10 05:00:00  54.0  76.06  160.79    0.0  NaN   \n",
      "6          132539.0 2025-03-10 06:00:00  54.0  76.06  160.79    0.0  NaN   \n",
      "7          132539.0 2025-03-10 08:00:00  54.0  76.06  160.79    0.0  NaN   \n",
      "8          132539.0 2025-03-10 09:00:00  54.0  76.06  160.79    0.0  NaN   \n",
      "9          132539.0 2025-03-10 10:00:00  54.0  76.06  160.79    0.0  NaN   \n",
      "\n",
      "Parameter  Creatinine   GCS  Glucose  ...  PaCO2  PaO2  pH  DiasABP  MAP  \\\n",
      "0                 NaN   NaN      NaN  ...    NaN   NaN NaN      NaN  NaN   \n",
      "1                 NaN  15.0      NaN  ...    NaN   NaN NaN      NaN  NaN   \n",
      "2                 NaN   NaN      NaN  ...    NaN   NaN NaN      NaN  NaN   \n",
      "3                 NaN   NaN      NaN  ...    NaN   NaN NaN      NaN  NaN   \n",
      "4                 NaN  15.0      NaN  ...    NaN   NaN NaN      NaN  NaN   \n",
      "5                 NaN   NaN      NaN  ...    NaN   NaN NaN      NaN  NaN   \n",
      "6                 NaN   NaN      NaN  ...    NaN   NaN NaN      NaN  NaN   \n",
      "7                 NaN  15.0      NaN  ...    NaN   NaN NaN      NaN  NaN   \n",
      "8                 NaN   NaN      NaN  ...    NaN   NaN NaN      NaN  NaN   \n",
      "9                 NaN   NaN      NaN  ...    NaN   NaN NaN      NaN  NaN   \n",
      "\n",
      "Parameter  SaO2  SysABP  Lactate  Cholesterol  TroponinI  \n",
      "0           NaN     NaN      NaN          NaN        NaN  \n",
      "1           NaN     NaN      NaN          NaN        NaN  \n",
      "2           NaN     NaN      NaN          NaN        NaN  \n",
      "3           NaN     NaN      NaN          NaN        NaN  \n",
      "4           NaN     NaN      NaN          NaN        NaN  \n",
      "5           NaN     NaN      NaN          NaN        NaN  \n",
      "6           NaN     NaN      NaN          NaN        NaN  \n",
      "7           NaN     NaN      NaN          NaN        NaN  \n",
      "8           NaN     NaN      NaN          NaN        NaN  \n",
      "9           NaN     NaN      NaN          NaN        NaN  \n",
      "\n",
      "[10 rows x 43 columns]\n",
      "Cleaned data for set_a saved as /Users/francescobondi/Desktop/stuff/ETH/FS25/ML for Healthcare/project-1-ml4hc/data/processed/set_a/set_a_before_ffill.parquet\n",
      "Parameter  RecordID                Time   Age Weight Height Gender   ALP  ALT  \\\n",
      "0          142675.0 2025-03-10 00:00:00  70.0   85.0  175.3    1.0   NaN  NaN   \n",
      "1          142675.0 2025-03-10 01:00:00  70.0   85.0  175.3    1.0   NaN  NaN   \n",
      "2          142675.0 2025-03-10 02:00:00  70.0   85.0  175.3    1.0   NaN  NaN   \n",
      "3          142675.0 2025-03-10 03:00:00  70.0   85.0  175.3    1.0   NaN  NaN   \n",
      "4          142675.0 2025-03-10 04:00:00  70.0   85.0  175.3    1.0   NaN  NaN   \n",
      "5          142675.0 2025-03-10 05:00:00  70.0   85.0  175.3    1.0   NaN  NaN   \n",
      "6          142675.0 2025-03-10 06:00:00  70.0   85.0  175.3    1.0   NaN  NaN   \n",
      "7          142675.0 2025-03-10 07:00:00  70.0   85.0  175.3    1.0   NaN  NaN   \n",
      "8          142675.0 2025-03-10 08:00:00  70.0   85.0  175.3    1.0   NaN  NaN   \n",
      "9          142675.0 2025-03-10 09:00:00  70.0   85.0  175.3    1.0  24.0  NaN   \n",
      "\n",
      "Parameter  AST  Albumin  ...      SysABP       Temp   Urine  WBC     pH  \\\n",
      "0          NaN      NaN  ...         NaN        NaN     NaN  NaN    NaN   \n",
      "1          NaN      NaN  ...         NaN        NaN     NaN  NaN  7.500   \n",
      "2          NaN      NaN  ...  125.500000  35.700000   615.0  NaN  7.500   \n",
      "3          NaN      NaN  ...  125.200000  35.780000   320.0  NaN    NaN   \n",
      "4          NaN      NaN  ...  101.166667  35.566667   305.0  NaN  7.430   \n",
      "5          NaN      NaN  ...   64.800000  34.380000   200.0  NaN  7.295   \n",
      "6          NaN      NaN  ...         NaN        NaN   300.0  7.9  7.340   \n",
      "7          NaN      NaN  ...         NaN        NaN     NaN  NaN  7.420   \n",
      "8          NaN      NaN  ...   79.000000        NaN  1100.0  NaN    NaN   \n",
      "9          NaN      1.7  ...    0.000000  34.600000   300.0  8.7  7.450   \n",
      "\n",
      "Parameter  Lactate  Cholesterol  RespRate  TroponinT  TroponinI  \n",
      "0              NaN          NaN       NaN        NaN        NaN  \n",
      "1              NaN          NaN       NaN        NaN        NaN  \n",
      "2              NaN          NaN       NaN        NaN        NaN  \n",
      "3              NaN          NaN       NaN        NaN        NaN  \n",
      "4              NaN          NaN       NaN        NaN        NaN  \n",
      "5              NaN          NaN       NaN        NaN        NaN  \n",
      "6             14.4          NaN       NaN        NaN        NaN  \n",
      "7             10.7          NaN       NaN        NaN        NaN  \n",
      "8              7.6          NaN       NaN        NaN        NaN  \n",
      "9              NaN          NaN       NaN        NaN        NaN  \n",
      "\n",
      "[10 rows x 43 columns]\n",
      "Cleaned data for set_b saved as /Users/francescobondi/Desktop/stuff/ETH/FS25/ML for Healthcare/project-1-ml4hc/data/processed/set_b/set_b_before_ffill.parquet\n",
      "Parameter  RecordID                Time   Age Weight Height Gender   BUN  \\\n",
      "0          152871.0 2025-03-10 00:00:00  71.0   79.2  167.6    1.0   NaN   \n",
      "1          152871.0 2025-03-10 01:00:00  71.0   79.2  167.6    1.0   NaN   \n",
      "2          152871.0 2025-03-10 02:00:00  71.0   79.2  167.6    1.0  36.0   \n",
      "3          152871.0 2025-03-10 03:00:00  71.0   79.2  167.6    1.0   NaN   \n",
      "4          152871.0 2025-03-10 04:00:00  71.0   79.2  167.6    1.0   NaN   \n",
      "5          152871.0 2025-03-10 05:00:00  71.0   79.2  167.6    1.0   NaN   \n",
      "6          152871.0 2025-03-10 06:00:00  71.0   79.2  167.6    1.0  36.0   \n",
      "7          152871.0 2025-03-10 07:00:00  71.0   79.2  167.6    1.0   NaN   \n",
      "8          152871.0 2025-03-10 08:00:00  71.0   79.2  167.6    1.0   NaN   \n",
      "9          152871.0 2025-03-10 09:00:00  71.0   79.2  167.6    1.0   NaN   \n",
      "\n",
      "Parameter  Creatinine    DiasABP  FiO2  ...    pH  MechVent  TroponinT  ALP  \\\n",
      "0                 NaN        NaN   NaN  ...   NaN       NaN        NaN  NaN   \n",
      "1                 NaN  59.000000   0.4  ...   NaN       1.0        NaN  NaN   \n",
      "2                 3.7  49.333333   NaN  ...   NaN       NaN        NaN  NaN   \n",
      "3                 NaN  59.000000   NaN  ...  7.34       NaN        NaN  NaN   \n",
      "4                 NaN        NaN   0.4  ...   NaN       1.0        NaN  NaN   \n",
      "5                 NaN  58.000000   NaN  ...   NaN       NaN        NaN  NaN   \n",
      "6                 3.5  53.000000   NaN  ...  7.34       NaN        NaN  NaN   \n",
      "7                 NaN  57.500000   NaN  ...   NaN       1.0        NaN  NaN   \n",
      "8                 NaN  59.000000   NaN  ...   NaN       NaN        NaN  NaN   \n",
      "9                 NaN  51.000000   0.4  ...   NaN       1.0        NaN  NaN   \n",
      "\n",
      "Parameter  ALT  AST  Albumin  Bilirubin  Cholesterol  TroponinI  \n",
      "0          NaN  NaN      NaN        NaN          NaN        NaN  \n",
      "1          NaN  NaN      NaN        NaN          NaN        NaN  \n",
      "2          NaN  NaN      NaN        NaN          NaN        NaN  \n",
      "3          NaN  NaN      NaN        NaN          NaN        NaN  \n",
      "4          NaN  NaN      NaN        NaN          NaN        NaN  \n",
      "5          NaN  NaN      NaN        NaN          NaN        NaN  \n",
      "6          NaN  NaN      NaN        NaN          NaN        NaN  \n",
      "7          NaN  NaN      NaN        NaN          NaN        NaN  \n",
      "8          NaN  NaN      NaN        NaN          NaN        NaN  \n",
      "9          NaN  NaN      NaN        NaN          NaN        NaN  \n",
      "\n",
      "[10 rows x 43 columns]\n",
      "Cleaned data for set_c saved as /Users/francescobondi/Desktop/stuff/ETH/FS25/ML for Healthcare/project-1-ml4hc/data/processed/set_c/set_c_before_ffill.parquet\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "def knn_impute_static_features(\n",
    "    df, static_features=[\"Age\", \"Weight\", \"Height\", \"Gender\"], n_neighbors=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Impute missing static values (currently indicated by -1) using KNN imputation with n_neighbors.\n",
    "\n",
    "    Parameters:\n",
    "      df (pd.DataFrame): DataFrame with one row per patient.\n",
    "      static_features (list): List of static feature column names to impute.\n",
    "      n_neighbors (int): Number of neighbors to use for KNN imputation.\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame: The DataFrame with missing static feature values imputed.\n",
    "    \"\"\"\n",
    "    # Work on a copy to avoid modifying the original DataFrame.\n",
    "    df_impute = df.copy()\n",
    "\n",
    "    # Replace missing values (-1) with np.nan in the static columns.\n",
    "    df_impute[static_features] = df_impute[static_features].replace(-1, np.nan)\n",
    "\n",
    "    # Initialize the KNN imputer.\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "\n",
    "    # Fit and transform the static features.\n",
    "    imputed_array = imputer.fit_transform(df_impute[static_features])\n",
    "\n",
    "    # Create a new DataFrame with the imputed static features.\n",
    "    df_imputed_static = pd.DataFrame(\n",
    "        imputed_array, columns=static_features, index=df_impute.index\n",
    "    )\n",
    "\n",
    "    # Update the original DataFrame with the imputed values.\n",
    "    df_impute.update(df_imputed_static)\n",
    "\n",
    "    return df_impute\n",
    "\n",
    "\n",
    "for set_key, df in sets_dict.items():\n",
    "    # Impute missing static features\n",
    "    # Get the static features in df\n",
    "    static_df = (\n",
    "        df.groupby(\"RecordID\", as_index=False)\n",
    "        .first()[[\"RecordID\", \"Age\", \"Weight\", \"Height\", \"Gender\"]]\n",
    "        .copy()\n",
    "    )\n",
    "    imputed_df = knn_impute_static_features(static_df)\n",
    "\n",
    "    if imputed_df.index.name == \"RecordID\":\n",
    "        imputed_df = imputed_df.reset_index()\n",
    "\n",
    "    # Check if there are any NaN values\n",
    "    if imputed_df.isnull().values.any():\n",
    "        print(f\"NaN values found in imputed static features for {set_key}.\")\n",
    "\n",
    "    # Update the original DataFrame with the imputed values\n",
    "    static_cols = [\"Age\", \"Weight\", \"Height\", \"Gender\"]\n",
    "    # Create a mapping for each column and update df_full.\n",
    "    for col in static_cols:\n",
    "        mapping = imputed_df.set_index(\"RecordID\")[col]\n",
    "        df[col] = df[\"RecordID\"].map(mapping)\n",
    "\n",
    "    # sets_dict[set_key] = imputed_df  # Update dictionary (optional)\n",
    "\n",
    "    # Reorder columns\n",
    "\n",
    "    # Assume that the first two columns should remain in place.\n",
    "    # For example, we assume these are the first two columns of the DataFrame.\n",
    "    first_two = list(df.columns[:2])\n",
    "\n",
    "    # The rest of the columns, excluding the static columns.\n",
    "    remaining = [\n",
    "        col for col in df.columns if col not in static_cols and col not in first_two\n",
    "    ]\n",
    "\n",
    "    # Create the new order: first two columns, then the static columns, then the remaining columns.\n",
    "    new_order = first_two + static_cols + remaining\n",
    "\n",
    "    # Reorder the DataFrame and return\n",
    "    df = df[new_order]\n",
    "\n",
    "    # Check if new dataframe has NaN values on the static columns\n",
    "    if df[static_cols].isnull().values.any():\n",
    "        print(f\"NaN values found in imputed static features for {set_key}.\")\n",
    "    print(f\"{df.head(10)}\")\n",
    "\n",
    "    # Export to Parquet file (e.g., \"set_a....parquet\")\n",
    "    output_filename = (\n",
    "        PROCESSED_DATA_DIR / f\"{set_key}\" / f\"{set_key}_before_ffill.parquet\"\n",
    "    )\n",
    "    df.to_parquet(output_filename, index=False)\n",
    "    print(f\"Cleaned data for {set_key} saved as {output_filename}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MechVent Imputation\n",
    "\n",
    "MechVent is a feature with a peculiar behaviour, so we adjust it manually.\n",
    "The website assures the values are 0/1. This is not true, there are only 1 values. So we assume that those patients that have the 1 somewhere has had the MechVentilation for the whole time. While those patients that have only NA didn't have any MechVent. In that way it becomes a static variable, but this choice seems more reasonable to us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        NaN\n",
      "1        NaN\n",
      "2        NaN\n",
      "3        NaN\n",
      "4        NaN\n",
      "          ..\n",
      "183411   NaN\n",
      "183412   NaN\n",
      "183413   NaN\n",
      "183414   NaN\n",
      "183415   NaN\n",
      "Name: MechVent, Length: 183416, dtype: float64\n",
      "[nan  1.]\n"
     ]
    }
   ],
   "source": [
    "# Check MechVent column in Set A\n",
    "set_a_df = sets_dict[\"set_a\"]\n",
    "print(set_a_df[\"MechVent\"])\n",
    "print(set_a_df[\"MechVent\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO the imputation\n",
    "for set_key, df in sets_dict.items():\n",
    "    df[\"MechVent\"] = df.groupby(\"RecordID\")[\"MechVent\"].transform(\n",
    "        lambda x: 1 if x.eq(1).any() else 0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that each patient has only one value for MechVent and it is 0 or 1\n",
    "for set_key, df in sets_dict.items():\n",
    "    unique_values = df.groupby(\"RecordID\")[\"MechVent\"].nunique()\n",
    "    if not all(unique_values <= 1):\n",
    "        print(f\"More than one unique value for MechVent in {set_key}.\")\n",
    "\n",
    "    if not all(df[\"MechVent\"].isin([0, 1])):\n",
    "        print(f\"Invalid values for MechVent in {set_key}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward-filled data for set_a saved as /Users/francescobondi/Desktop/stuff/ETH/FS25/ML for Healthcare/project-1-ml4hc/data/processed/set_a/set_a_before_backward.parquet\n",
      "Forward-filled data for set_b saved as /Users/francescobondi/Desktop/stuff/ETH/FS25/ML for Healthcare/project-1-ml4hc/data/processed/set_b/set_b_before_backward.parquet\n",
      "Forward-filled data for set_c saved as /Users/francescobondi/Desktop/stuff/ETH/FS25/ML for Healthcare/project-1-ml4hc/data/processed/set_c/set_c_before_backward.parquet\n"
     ]
    }
   ],
   "source": [
    "def forward_fill(df):\n",
    "    # Ensure the DataFrame is sorted by RecordID and Time\n",
    "    df.sort_values(by=[\"RecordID\", \"Time\"], inplace=True)\n",
    "\n",
    "    # Get a list of all columns except \"RecordID\" and \"Time\"\n",
    "    other_cols = [col for col in df.columns if col != \"RecordID\" and col != \"Time\"]\n",
    "\n",
    "    # Group by RecordID and apply forward fill for each group.\n",
    "    df[other_cols] = df.groupby(\"RecordID\")[other_cols].ffill()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "for set_key, df in sets_dict.items():\n",
    "    # Forward fill the DataFrame\n",
    "    filled_df = forward_fill(df)\n",
    "    sets_dict[set_key] = filled_df  # Update dictionary (optional)\n",
    "\n",
    "    # Export to Parquet file (e.g., \"set_a....parquet\")\n",
    "    output_filename = (\n",
    "        PROCESSED_DATA_DIR / f\"{set_key}\" / f\"{set_key}_before_backward.parquet\"\n",
    "    )\n",
    "    filled_df.to_parquet(output_filename, index=False)\n",
    "    print(f\"Forward-filled data for {set_key} saved as {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Filling using Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vm/dlvhdp0n0jg_tp24ng5z84080000gn/T/ipykernel_56302/4224284654.py:28: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df[cols_to_interp] = df[cols_to_interp].interpolate(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After interpolation, set_a has shape: (183416, 43)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vm/dlvhdp0n0jg_tp24ng5z84080000gn/T/ipykernel_56302/4224284654.py:28: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df[cols_to_interp] = df[cols_to_interp].interpolate(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After interpolation, set_b has shape: (183495, 43)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vm/dlvhdp0n0jg_tp24ng5z84080000gn/T/ipykernel_56302/4224284654.py:28: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df[cols_to_interp] = df[cols_to_interp].interpolate(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After interpolation, set_c has shape: (183711, 43)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Storing DataFrames:  33%|███▎      | 1/3 [00:00<00:00,  9.91set/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/francescobondi/Desktop/stuff/ETH/FS25/ML for Healthcare/project-1-ml4hc/data/processed/set_a/set_a_before_scaling.parquet\n",
      "Saved /Users/francescobondi/Desktop/stuff/ETH/FS25/ML for Healthcare/project-1-ml4hc/data/processed/set_b/set_b_before_scaling.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Storing DataFrames: 100%|██████████| 3/3 [00:00<00:00, 10.69set/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/francescobondi/Desktop/stuff/ETH/FS25/ML for Healthcare/project-1-ml4hc/data/processed/set_c/set_c_before_scaling.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def time_based_interpolation(df):\n",
    "    \"\"\"\n",
    "    Perform time-based interpolation on the DataFrame.\n",
    "\n",
    "    This function:\n",
    "      - Converts the \"Time\" column to datetime,\n",
    "      - Sets \"Time\" as the index,\n",
    "      - Interpolates numeric columns (excluding \"RecordID\") using method='time'\n",
    "        with limit_direction='both',\n",
    "      - Resets the index to restore \"Time\" as a regular column.\n",
    "\n",
    "    Parameters:\n",
    "      df (pd.DataFrame): Input DataFrame with at least \"Time\" and \"RecordID\" columns.\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame: The DataFrame with interpolated values.\n",
    "    \"\"\"\n",
    "    # Ensure the \"Time\" column is in datetime format.\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"Time\"])\n",
    "\n",
    "    # Set \"Time\" as the DataFrame index for time-based interpolation.\n",
    "    df = df.set_index(\"Time\")\n",
    "\n",
    "    # Identify the columns to interpolate (exclude non-numeric columns like \"RecordID\").\n",
    "    cols_to_interp = [col for col in df.columns if col not in [\"RecordID\", \"Time\"]]\n",
    "\n",
    "    # Apply time-based interpolation; limit_direction='both' fills NaNs at the start and end too.\n",
    "    df[cols_to_interp] = df[cols_to_interp].interpolate(\n",
    "        method=\"time\", limit_direction=\"both\"\n",
    "    )\n",
    "\n",
    "    # Restore \"Time\" as a regular column by resetting the index.\n",
    "    df = df.reset_index()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "for key, df in sets_dict.items():\n",
    "    sets_dict[key] = time_based_interpolation(df)\n",
    "    print(f\"After interpolation, {key} has shape: {sets_dict[key].shape}\")\n",
    "\n",
    "# Saving temporary data for plotting\n",
    "for set_name, set_df in tqdm(sets_dict.items(), desc=\"Storing DataFrames\", unit=\"set\"):\n",
    "    output_path = (\n",
    "        PROCESSED_DATA_DIR / f\"{set_name}\" / f\"{set_name}_before_scaling.parquet\"\n",
    "    )\n",
    "    set_df.to_parquet(output_path, index=False, engine=\"pyarrow\")\n",
    "    print(f\"Saved {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RecordID                Time Gender    Height   Weight       Age   Albumin  \\\n",
      "0  132539.0 2025-03-10 00:00:00    0.0 -0.950526 -0.23008 -0.596332  1.671639   \n",
      "1  132539.0 2025-03-10 01:00:00    0.0 -0.950526 -0.23008 -0.596332  1.967793   \n",
      "2  132539.0 2025-03-10 02:00:00    0.0 -0.950526 -0.23008 -0.596332 -1.734132   \n",
      "3  132539.0 2025-03-10 03:00:00    0.0 -0.950526 -0.23008 -0.596332  1.523562   \n",
      "4  132539.0 2025-03-10 04:00:00    0.0 -0.950526 -0.23008 -0.596332  0.487023   \n",
      "5  132539.0 2025-03-10 05:00:00    0.0 -0.950526 -0.23008 -0.596332  0.042792   \n",
      "6  132539.0 2025-03-10 06:00:00    0.0 -0.950526 -0.23008 -0.596332  0.635100   \n",
      "7  132539.0 2025-03-10 08:00:00    0.0 -0.950526 -0.23008 -0.596332  0.635100   \n",
      "8  132539.0 2025-03-10 09:00:00    0.0 -0.950526 -0.23008 -0.596332 -0.401439   \n",
      "9  132539.0 2025-03-10 10:00:00    0.0 -0.950526 -0.23008 -0.596332 -0.105285   \n",
      "\n",
      "   Cholesterol   DiasABP      HCO3  ...      Urine       WBC     pH  MechVent  \\\n",
      "0    -0.013487 -0.832594 -0.109176  ...  11.571429  0.753623  1.125       0.0   \n",
      "1     0.172112 -0.608431 -0.109176  ...   2.857143 -0.420290  0.125       0.0   \n",
      "2     0.125712  0.848629  0.830987  ...  -0.357143 -0.014493 -0.875       0.0   \n",
      "3     0.380910 -0.832594 -0.579257  ...   0.642857  0.188406 -0.375       0.0   \n",
      "4    -0.964680  1.483758 -0.814297  ...  -0.142857 -1.144928  1.000       0.0   \n",
      "5    -0.013487  0.736548 -0.579257  ...  -0.142857  0.420290  0.375       0.0   \n",
      "6    -1.405476  0.213500 -0.344216  ...   0.642857 -0.521739 -0.875       0.0   \n",
      "7     0.984105 -0.290867  0.360906  ...   0.285714  0.637681 -0.625       0.0   \n",
      "8    -1.591075  1.483758  1.301068  ...   0.000000  0.492754 -0.125       0.0   \n",
      "9    -1.196678  0.064058 -0.814297  ...   0.142857 -0.521739 -1.125       0.0   \n",
      "\n",
      "   TroponinT       ALP       ALT        AST  Bilirubin  TroponinI  \n",
      "0   1.923077  0.132075 -0.176471   0.450704   1.545455   0.285714  \n",
      "1  -0.246154  0.000000 -0.294118   0.436620   0.000000  -0.126984  \n",
      "2   0.000000  0.773585 -0.205882  -0.380282   0.181818  -0.095238  \n",
      "3   0.215385 -0.698113 -0.588235   1.126761  -0.181818   4.650794  \n",
      "4   2.738462 -0.490566 -0.558824  -0.225352   0.363636   0.904762  \n",
      "5  -0.123077  0.509434  5.323529  35.577465  24.909091   1.587302  \n",
      "6  -0.246154 -0.452830 -0.088235  25.661972  -0.454545  -0.206349  \n",
      "7   0.138462  1.584906  0.058824   1.450704  -0.090909  -0.095238  \n",
      "8   1.046154 -0.735849 -0.176471  -0.309859   3.363636  -0.047619  \n",
      "9   0.661538  1.188679  0.058824  -0.352113  -0.363636  -0.095238  \n",
      "\n",
      "[10 rows x 43 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "cols_to_scale = [col for col in df.columns if col not in [\"RecordID\", \"Time\", \"Gender\"]]\n",
    "\n",
    "### For normally-distributed columns, we use the StandardScaler\n",
    "### For non-normally-distributed columns, we use the RobustScaler\n",
    "\n",
    "nd_cols = [\n",
    "    \"Height\",\n",
    "    \"Weight\",\n",
    "    \"Age\",\n",
    "    \"Albumin\",\n",
    "    \"Cholesterol\",\n",
    "    \"DiasABP\",\n",
    "    \"HCO3\",\n",
    "    \"HCT\",\n",
    "    \"HR\",\n",
    "    \"Mg\",\n",
    "    \"MAP\",\n",
    "    \"Na\",\n",
    "    \"NIDiasABP\",\n",
    "    \"NIMAP\",\n",
    "    \"NISysABP\",\n",
    "    \"SysABP\",\n",
    "    \"PaCO2\",\n",
    "    \"PaO2\",\n",
    "    \"Platelets\",\n",
    "    \"RespRate\",\n",
    "    \"Temp\",\n",
    "]\n",
    "nnd_cols = [col for col in cols_to_scale if col not in nd_cols]\n",
    "\n",
    "scaler_nd = StandardScaler()\n",
    "scaler_nnd = RobustScaler()\n",
    "\n",
    "# Process each set: fit on set_a, then only transform on the others.\n",
    "for set_key in [\"set_a\", \"set_b\", \"set_c\"]:\n",
    "    df = sets_dict[set_key]\n",
    "    if set_key == \"set_a\":\n",
    "        # Fit on the first set\n",
    "        scaled_values_nd = scaler_nd.fit_transform(df[nd_cols])\n",
    "        scaled_values_nnd = scaler_nnd.fit_transform(df[nnd_cols])\n",
    "    else:\n",
    "        # Transform the other sets using the fitted scalers\n",
    "        scaled_values_nd = scaler_nd.transform(df[nd_cols])\n",
    "        scaled_values_nnd = scaler_nnd.transform(df[nnd_cols])\n",
    "\n",
    "    # Convert the scaled numpy arrays to DataFrames while preserving the index\n",
    "    df_scaled_nd = pd.DataFrame(scaled_values_nd, columns=nd_cols, index=df.index)\n",
    "    df_scaled_nnd = pd.DataFrame(scaled_values_nnd, columns=nnd_cols, index=df.index)\n",
    "\n",
    "    # Combine the scaled DataFrames along the columns axis\n",
    "    df_scaled = pd.concat([df_scaled_nd, df_scaled_nnd], axis=1)\n",
    "\n",
    "    # Combine the unmodified columns with the scaled columns.\n",
    "    df_final = pd.concat(\n",
    "        [\n",
    "            df[[\"RecordID\", \"Time\", \"Gender\"]].reset_index(drop=True),\n",
    "            df_scaled.reset_index(drop=True),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Update the dictionary with the final DataFrame\n",
    "    sets_dict[set_key] = df_final\n",
    "\n",
    "# Optionally, print the first 10 rows of set_a to check the result.\n",
    "print(sets_dict[\"set_a\"].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Storing DataFrames: 100%|██████████| 3/3 [00:00<00:00, 11.66set/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/francescobondi/Desktop/stuff/ETH/FS25/ML for Healthcare/project-1-ml4hc/data/processed/set_a/set_a_final.parquet\n",
      "Saved /Users/francescobondi/Desktop/stuff/ETH/FS25/ML for Healthcare/project-1-ml4hc/data/processed/set_b/set_b_final.parquet\n",
      "Saved /Users/francescobondi/Desktop/stuff/ETH/FS25/ML for Healthcare/project-1-ml4hc/data/processed/set_c/set_c_final.parquet\n",
      "\n",
      "All DataFrames have been saved to Parquet format.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for set_name, set_df in tqdm(sets_dict.items(), desc=\"Storing DataFrames\", unit=\"set\"):\n",
    "    output_path = PROCESSED_DATA_DIR / f\"{set_name}\" / f\"{set_name}_final.parquet\"\n",
    "    set_df.to_parquet(output_path, index=False, engine=\"pyarrow\")\n",
    "    print(f\"Saved {output_path}\")\n",
    "\n",
    "print(\"\\nAll DataFrames have been saved to Parquet format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TUM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
