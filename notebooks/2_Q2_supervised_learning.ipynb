{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from project_1.config import PROJ_ROOT, PROCESSED_DATA_DIR\n",
    "from project_1.loading import *\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Set A: (183416, 42) Set B: (183495, 42) Set C: (183711, 42)\n",
      "Shapes of labels:\n",
      "Set A: (4000, 2) Set B: (4000, 2) Set C: (4000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load the data using the functions from the loading module\n",
    "\n",
    "# Load final data without ICU (not used in training)\n",
    "set_a, set_b, set_c = load_final_data_without_ICU()\n",
    "death_a, death_b, death_c = load_outcomes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in set_a: 0\n",
      "Missing values in set_b: 0\n",
      "Missing values in set_c: 0\n",
      "Missing values in death_a: 0\n",
      "Missing values in death_b: 0\n",
      "Missing values in death_c: 0\n"
     ]
    }
   ],
   "source": [
    "# Check if there are any missing values in the data\n",
    "print(f\"Missing values in set_a: {set_a.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in set_b: {set_b.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in set_c: {set_c.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in death_a: {death_a.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in death_b: {death_b.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in death_c: {death_c.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death A contains only 0 and 1: True\n",
      "Death B contains only 0 and 1: True\n",
      "Death C contains only 0 and 1: True\n"
     ]
    }
   ],
   "source": [
    "# Check if the outcomes only contain 0s and 1s\n",
    "print(\"Death A contains only 0 and 1:\", death_a[\"In-hospital_death\"].isin([0, 1]).all())\n",
    "print(\"Death B contains only 0 and 1:\", death_b[\"In-hospital_death\"].isin([0, 1]).all())\n",
    "print(\"Death C contains only 0 and 1:\", death_c[\"In-hospital_death\"].isin([0, 1]).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.1 Classic Machine Learning Methods\n",
    "\n",
    "### Part 1 - Train two distinct ML classifiers after aggregating rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecordID</th>\n",
       "      <th>Time</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Age</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>DiasABP</th>\n",
       "      <th>HCO3</th>\n",
       "      <th>...</th>\n",
       "      <th>WBC</th>\n",
       "      <th>pH</th>\n",
       "      <th>MechVent</th>\n",
       "      <th>TroponinT</th>\n",
       "      <th>ALP</th>\n",
       "      <th>ALT</th>\n",
       "      <th>AST</th>\n",
       "      <th>Bilirubin</th>\n",
       "      <th>TroponinI</th>\n",
       "      <th>In-hospital_death</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132539</td>\n",
       "      <td>2025-03-10 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.950526</td>\n",
       "      <td>-0.23008</td>\n",
       "      <td>-0.596332</td>\n",
       "      <td>1.671639</td>\n",
       "      <td>-0.013487</td>\n",
       "      <td>-0.832594</td>\n",
       "      <td>-0.109176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753623</td>\n",
       "      <td>1.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.923077</td>\n",
       "      <td>0.132075</td>\n",
       "      <td>-0.176471</td>\n",
       "      <td>0.450704</td>\n",
       "      <td>1.545455</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132539</td>\n",
       "      <td>2025-03-10 01:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.950526</td>\n",
       "      <td>-0.23008</td>\n",
       "      <td>-0.596332</td>\n",
       "      <td>1.967793</td>\n",
       "      <td>0.172112</td>\n",
       "      <td>-0.608431</td>\n",
       "      <td>-0.109176</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.420290</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.246154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.294118</td>\n",
       "      <td>0.436620</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.126984</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132539</td>\n",
       "      <td>2025-03-10 02:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.950526</td>\n",
       "      <td>-0.23008</td>\n",
       "      <td>-0.596332</td>\n",
       "      <td>-1.734132</td>\n",
       "      <td>0.125712</td>\n",
       "      <td>0.848629</td>\n",
       "      <td>0.830987</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014493</td>\n",
       "      <td>-0.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.773585</td>\n",
       "      <td>-0.205882</td>\n",
       "      <td>-0.380282</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>-0.095238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132539</td>\n",
       "      <td>2025-03-10 03:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.950526</td>\n",
       "      <td>-0.23008</td>\n",
       "      <td>-0.596332</td>\n",
       "      <td>1.523562</td>\n",
       "      <td>0.380910</td>\n",
       "      <td>-0.832594</td>\n",
       "      <td>-0.579257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188406</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.215385</td>\n",
       "      <td>-0.698113</td>\n",
       "      <td>-0.588235</td>\n",
       "      <td>1.126761</td>\n",
       "      <td>-0.181818</td>\n",
       "      <td>4.650794</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132539</td>\n",
       "      <td>2025-03-10 04:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.950526</td>\n",
       "      <td>-0.23008</td>\n",
       "      <td>-0.596332</td>\n",
       "      <td>0.487023</td>\n",
       "      <td>-0.964680</td>\n",
       "      <td>1.483758</td>\n",
       "      <td>-0.814297</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.144928</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.738462</td>\n",
       "      <td>-0.490566</td>\n",
       "      <td>-0.558824</td>\n",
       "      <td>-0.225352</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RecordID                Time  Gender    Height   Weight       Age  \\\n",
       "0    132539 2025-03-10 00:00:00     0.0 -0.950526 -0.23008 -0.596332   \n",
       "1    132539 2025-03-10 01:00:00     0.0 -0.950526 -0.23008 -0.596332   \n",
       "2    132539 2025-03-10 02:00:00     0.0 -0.950526 -0.23008 -0.596332   \n",
       "3    132539 2025-03-10 03:00:00     0.0 -0.950526 -0.23008 -0.596332   \n",
       "4    132539 2025-03-10 04:00:00     0.0 -0.950526 -0.23008 -0.596332   \n",
       "\n",
       "    Albumin  Cholesterol   DiasABP      HCO3  ...       WBC     pH  MechVent  \\\n",
       "0  1.671639    -0.013487 -0.832594 -0.109176  ...  0.753623  1.125       0.0   \n",
       "1  1.967793     0.172112 -0.608431 -0.109176  ... -0.420290  0.125       0.0   \n",
       "2 -1.734132     0.125712  0.848629  0.830987  ... -0.014493 -0.875       0.0   \n",
       "3  1.523562     0.380910 -0.832594 -0.579257  ...  0.188406 -0.375       0.0   \n",
       "4  0.487023    -0.964680  1.483758 -0.814297  ... -1.144928  1.000       0.0   \n",
       "\n",
       "   TroponinT       ALP       ALT       AST  Bilirubin  TroponinI  \\\n",
       "0   1.923077  0.132075 -0.176471  0.450704   1.545455   0.285714   \n",
       "1  -0.246154  0.000000 -0.294118  0.436620   0.000000  -0.126984   \n",
       "2   0.000000  0.773585 -0.205882 -0.380282   0.181818  -0.095238   \n",
       "3   0.215385 -0.698113 -0.588235  1.126761  -0.181818   4.650794   \n",
       "4   2.738462 -0.490566 -0.558824 -0.225352   0.363636   0.904762   \n",
       "\n",
       "   In-hospital_death  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the datasets\n",
    "\n",
    "# Change RecordID to int32\n",
    "set_a[\"RecordID\"] = set_a[\"RecordID\"].astype(np.int32)\n",
    "set_b[\"RecordID\"] = set_b[\"RecordID\"].astype(np.int32)\n",
    "set_c[\"RecordID\"] = set_c[\"RecordID\"].astype(np.int32)\n",
    "\n",
    "# Include the outcomes in the datasets\n",
    "set_a = set_a.merge(death_a, on=\"RecordID\", how=\"left\")\n",
    "set_b = set_b.merge(death_b, on=\"RecordID\", how=\"left\")\n",
    "set_c = set_c.merge(death_c, on=\"RecordID\", how=\"left\")\n",
    "\n",
    "# Change names of the datasets\n",
    "train_set = set_a\n",
    "val_set = set_b\n",
    "test_set = set_c\n",
    "\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the aggregated training set: (4000, 42)\n",
      "Shape of the aggregated validation set: (4000, 42)\n",
      "Shape of the aggregated test set: (4000, 42)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecordID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>ALP</th>\n",
       "      <th>ALT</th>\n",
       "      <th>AST</th>\n",
       "      <th>Bilirubin</th>\n",
       "      <th>BUN</th>\n",
       "      <th>...</th>\n",
       "      <th>RespRate</th>\n",
       "      <th>SaO2</th>\n",
       "      <th>SysABP</th>\n",
       "      <th>Temp</th>\n",
       "      <th>TroponinI</th>\n",
       "      <th>TroponinT</th>\n",
       "      <th>Urine</th>\n",
       "      <th>WBC</th>\n",
       "      <th>Weight</th>\n",
       "      <th>In-hospital_death</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132539</td>\n",
       "      <td>-0.596332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.950526</td>\n",
       "      <td>-1.289901</td>\n",
       "      <td>-0.716981</td>\n",
       "      <td>-0.411765</td>\n",
       "      <td>0.802817</td>\n",
       "      <td>-0.181818</td>\n",
       "      <td>-0.588235</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.302039</td>\n",
       "      <td>-0.427083</td>\n",
       "      <td>-0.108444</td>\n",
       "      <td>1.501788</td>\n",
       "      <td>6.095238</td>\n",
       "      <td>34.892308</td>\n",
       "      <td>43.142857</td>\n",
       "      <td>-0.318841</td>\n",
       "      <td>-0.230080</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132540</td>\n",
       "      <td>0.667051</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.617613</td>\n",
       "      <td>-1.289901</td>\n",
       "      <td>-0.716981</td>\n",
       "      <td>-0.411765</td>\n",
       "      <td>0.802817</td>\n",
       "      <td>-0.181818</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019156</td>\n",
       "      <td>-0.632653</td>\n",
       "      <td>0.001751</td>\n",
       "      <td>1.123672</td>\n",
       "      <td>6.095238</td>\n",
       "      <td>34.892308</td>\n",
       "      <td>37.022619</td>\n",
       "      <td>0.246377</td>\n",
       "      <td>-0.232705</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132541</td>\n",
       "      <td>-1.170597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.429614</td>\n",
       "      <td>-1.141824</td>\n",
       "      <td>0.471698</td>\n",
       "      <td>1.529412</td>\n",
       "      <td>1.718310</td>\n",
       "      <td>1.909091</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018740</td>\n",
       "      <td>-1.166667</td>\n",
       "      <td>-0.053937</td>\n",
       "      <td>2.510098</td>\n",
       "      <td>6.095238</td>\n",
       "      <td>34.892308</td>\n",
       "      <td>30.971429</td>\n",
       "      <td>-0.782609</td>\n",
       "      <td>-1.077244</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132543</td>\n",
       "      <td>0.207639</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.157978</td>\n",
       "      <td>1.967793</td>\n",
       "      <td>0.471698</td>\n",
       "      <td>-0.323529</td>\n",
       "      <td>-0.380282</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>-0.470588</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.722062</td>\n",
       "      <td>-0.408163</td>\n",
       "      <td>-0.152114</td>\n",
       "      <td>-0.010676</td>\n",
       "      <td>6.095238</td>\n",
       "      <td>34.892308</td>\n",
       "      <td>135.657143</td>\n",
       "      <td>-0.536232</td>\n",
       "      <td>0.143617</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132545</td>\n",
       "      <td>1.356169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.252050</td>\n",
       "      <td>0.338946</td>\n",
       "      <td>-0.716981</td>\n",
       "      <td>-0.411765</td>\n",
       "      <td>0.802817</td>\n",
       "      <td>-0.181818</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023980</td>\n",
       "      <td>-0.414894</td>\n",
       "      <td>-0.184575</td>\n",
       "      <td>0.997634</td>\n",
       "      <td>6.095238</td>\n",
       "      <td>34.892308</td>\n",
       "      <td>7.114286</td>\n",
       "      <td>-0.985507</td>\n",
       "      <td>-0.809442</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RecordID       Age  Gender    Height   Albumin       ALP       ALT  \\\n",
       "0    132539 -0.596332     0.0 -0.950526 -1.289901 -0.716981 -0.411765   \n",
       "1    132540  0.667051     1.0  0.617613 -1.289901 -0.716981 -0.411765   \n",
       "2    132541 -1.170597     0.0 -0.429614 -1.141824  0.471698  1.529412   \n",
       "3    132543  0.207639     1.0  1.157978  1.967793  0.471698 -0.323529   \n",
       "4    132545  1.356169     0.0 -1.252050  0.338946 -0.716981 -0.411765   \n",
       "\n",
       "        AST  Bilirubin       BUN  ...  RespRate      SaO2    SysABP      Temp  \\\n",
       "0  0.802817  -0.181818 -0.588235  ... -0.302039 -0.427083 -0.108444  1.501788   \n",
       "1  0.802817  -0.181818  0.176471  ... -0.019156 -0.632653  0.001751  1.123672   \n",
       "2  1.718310   1.909091 -0.882353  ... -0.018740 -1.166667 -0.053937  2.510098   \n",
       "3 -0.380282  -0.454545 -0.470588  ... -0.722062 -0.408163 -0.152114 -0.010676   \n",
       "4  0.802817  -0.181818  0.411765  ... -0.023980 -0.414894 -0.184575  0.997634   \n",
       "\n",
       "   TroponinI  TroponinT       Urine       WBC    Weight  In-hospital_death  \n",
       "0   6.095238  34.892308   43.142857 -0.318841 -0.230080                  0  \n",
       "1   6.095238  34.892308   37.022619  0.246377 -0.232705                  0  \n",
       "2   6.095238  34.892308   30.971429 -0.782609 -1.077244                  0  \n",
       "3   6.095238  34.892308  135.657143 -0.536232  0.143617                  0  \n",
       "4   6.095238  34.892308    7.114286 -0.985507 -0.809442                  0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define aggregation rules\n",
    "aggregation_rules = {\n",
    "    \"Age\": \"last\",\n",
    "    \"Gender\": \"last\",\n",
    "    \"Height\": \"last\",\n",
    "    \"Albumin\": \"last\",\n",
    "    \"ALP\": \"last\",\n",
    "    \"ALT\": \"last\",\n",
    "    \"AST\": \"last\",\n",
    "    \"Bilirubin\": \"last\",\n",
    "    \"BUN\": \"last\",\n",
    "    \"Cholesterol\": \"last\",\n",
    "    \"Creatinine\": \"last\",\n",
    "    \"DiasABP\": \"mean\",\n",
    "    \"FiO2\": \"mean\",\n",
    "    \"GCS\": \"min\",\n",
    "    \"Glucose\": \"mean\",\n",
    "    \"HCO3\": \"last\",\n",
    "    \"HCT\": \"last\",\n",
    "    \"HR\": \"mean\",\n",
    "    \"K\": \"last\",\n",
    "    \"Lactate\": \"max\",\n",
    "    \"Mg\": \"last\",\n",
    "    \"MAP\": \"mean\",\n",
    "    \"MechVent\": \"last\",\n",
    "    \"Na\": \"last\",\n",
    "    \"NIDiasABP\": \"mean\",\n",
    "    \"NIMAP\": \"mean\",\n",
    "    \"NISysABP\": \"mean\",\n",
    "    \"PaCO2\": \"last\",\n",
    "    \"PaO2\": \"mean\",\n",
    "    \"pH\": \"last\",\n",
    "    \"Platelets\": \"last\",\n",
    "    \"RespRate\": \"mean\",\n",
    "    \"SaO2\": \"mean\",\n",
    "    \"SysABP\": \"mean\",\n",
    "    \"Temp\": \"max\",\n",
    "    \"TroponinI\": \"max\",\n",
    "    \"TroponinT\": \"max\",\n",
    "    \"Urine\": \"sum\",\n",
    "    \"WBC\": \"last\",\n",
    "    \"Weight\": \"last\",\n",
    "    \"In-hospital_death\": \"max\"  # If any 1 exists for a patient, return 1\n",
    "}\n",
    "\n",
    "# Perform aggregation\n",
    "train_aggregated = train_set.groupby(\"RecordID\").agg(aggregation_rules).reset_index()\n",
    "val_aggregated = val_set.groupby(\"RecordID\").agg(aggregation_rules).reset_index()\n",
    "test_aggregated = test_set.groupby(\"RecordID\").agg(aggregation_rules).reset_index()\n",
    "\n",
    "\n",
    "# Display the processed dataset\n",
    "print(\"Shape of the aggregated training set:\", train_aggregated.shape)\n",
    "print(\"Shape of the aggregated validation set:\", val_aggregated.shape)\n",
    "print(\"Shape of the aggregated test set:\", test_aggregated.shape)\n",
    "# Display the first few rows of the aggregated training set\n",
    "train_aggregated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 40) (4000,)\n"
     ]
    }
   ],
   "source": [
    "# Separate Predictors (X) and Target (y)\n",
    "X_train = train_aggregated.drop(columns=[\"RecordID\", \"In-hospital_death\"])\n",
    "y_train = train_aggregated[\"In-hospital_death\"]\n",
    "\n",
    "X_val = val_aggregated.drop(columns=[\"RecordID\", \"In-hospital_death\"])\n",
    "y_val = val_aggregated[\"In-hospital_death\"]\n",
    "\n",
    "X_test = test_aggregated.drop(columns=[\"RecordID\", \"In-hospital_death\"])\n",
    "y_test = test_aggregated[\"In-hospital_death\"]\n",
    "\n",
    "# Visualize the shape of the datasets\n",
    "print(X_train.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.790, AUPRC: 0.419\n",
      "Test ROC AUC: 0.770, AUPRC: 0.392\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# Separate features and target for each set.\n",
    "\n",
    "# Create and train the Logistic Regression classifier.\n",
    "clf = LogisticRegression(random_state=SEED, max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_valid_proba = clf.predict_proba(X_val)[:, 1]  # probability for the positive class\n",
    "roc_auc_valid = roc_auc_score(y_val, y_valid_proba)\n",
    "auprc_valid = average_precision_score(y_val, y_valid_proba)\n",
    "print(f\"Validation ROC AUC: {roc_auc_valid:.3f}, AUPRC: {auprc_valid:.3f}\")\n",
    "\n",
    "# Evaluate on the test set.\n",
    "y_test_proba = clf.predict_proba(X_test)[:, 1]\n",
    "roc_auc_test = roc_auc_score(y_test, y_test_proba)\n",
    "auprc_test = average_precision_score(y_test, y_test_proba)\n",
    "print(f\"Test ROC AUC: {roc_auc_test:.3f}, AUPRC: {auprc_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.791, AUPRC: 0.431\n",
      "Test ROC AUC: 0.778, AUPRC: 0.410\n"
     ]
    }
   ],
   "source": [
    "# Use Random Forest to predict the target\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create and train the Random Forest classifier.\n",
    "clf = RandomForestClassifier(random_state=SEED)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Optionally, evaluate on the validation set.\n",
    "y_valid_proba = clf.predict_proba(X_val)[:, 1]  # probability for the positive class\n",
    "roc_auc_valid = roc_auc_score(y_val, y_valid_proba)\n",
    "auprc_valid = average_precision_score(y_val, y_valid_proba)\n",
    "print(f\"Validation ROC AUC: {roc_auc_valid:.3f}, AUPRC: {auprc_valid:.3f}\")\n",
    "\n",
    "# Evaluate on the test set.\n",
    "y_test_proba = clf.predict_proba(X_test)[:, 1]\n",
    "roc_auc_test = roc_auc_score(y_test, y_test_proba)\n",
    "auprc_test = average_precision_score(y_test, y_test_proba)\n",
    "print(f\"Test ROC AUC: {roc_auc_test:.3f}, AUPRC: {auprc_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Feature Engineering\n",
    "\n",
    "We use Feature Lagging and also TSFresh for time-series relevant feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Lagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescobondi/anaconda3/envs/TUM/lib/python3.12/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/francescobondi/anaconda3/envs/TUM/lib/python3.12/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "/Users/francescobondi/anaconda3/envs/TUM/lib/python3.12/site-packages/numpy/lib/function_base.py:2889: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "/Users/francescobondi/anaconda3/envs/TUM/lib/python3.12/site-packages/numpy/lib/function_base.py:2748: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "/Users/francescobondi/anaconda3/envs/TUM/lib/python3.12/site-packages/numpy/lib/function_base.py:2748: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected for lag augmentation: ['HCO3', 'HCT', 'HR', 'Mg', 'Na', 'Temp', 'BUN', 'FiO2', 'GCS', 'K']\n"
     ]
    }
   ],
   "source": [
    "# Investigate the best features to expand with lagged values\n",
    "def compute_patientwise_avg_acf(df, feature, lag=1):\n",
    "    \"\"\"\n",
    "    Compute the average lag-1 autocorrelation for a given feature across patients.\n",
    "    \"\"\"\n",
    "    acf_values = []\n",
    "    for rid, group in df.groupby(\"RecordID\"):\n",
    "        series = group[feature].dropna()\n",
    "        if len(series) < lag + 1:\n",
    "            continue\n",
    "        acf_val = series.corr(series.shift(lag))\n",
    "        if pd.notna(acf_val):\n",
    "            acf_values.append(acf_val)\n",
    "    if len(acf_values) > 0:\n",
    "        return np.mean(acf_values)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#! Using train_set because it's before the aggregation\n",
    "# Define the candidate features: exclude static ones (\"RecordID\", \"Time\", \"In-hospital_death\").\n",
    "candidate_features = [col for col in train_set.columns if col not in [\"RecordID\", \"Time\", \"In-hospital_death\", \"Age\", \"Weight\", \"Height\", \"Gender\"]]\n",
    "\n",
    "# Set threshold for absolute autocorrelation (lag 1)\n",
    "threshold = 0.5\n",
    "selected_features = []\n",
    "for feature in candidate_features:\n",
    "    avg_acf = compute_patientwise_avg_acf(train_set, feature, lag=1)\n",
    "    if avg_acf is not None and abs(avg_acf) >= threshold:\n",
    "        selected_features.append(feature)\n",
    "        #print(f\"Selected {feature} with average lag-1 ACF = {avg_acf:.3f}\")\n",
    "\n",
    "print(\"Features selected for lag augmentation:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check the number of NaN values\n",
    "print(train_set.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add the lag columns for those selected features.\n",
    "WARNING! This operation creates NaN values inside the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183416, 63) (183495, 63) (183711, 63)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecordID</th>\n",
       "      <th>Time</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Age</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>DiasABP</th>\n",
       "      <th>HCO3</th>\n",
       "      <th>...</th>\n",
       "      <th>Temp_lag1</th>\n",
       "      <th>Temp_lag2</th>\n",
       "      <th>BUN_lag1</th>\n",
       "      <th>BUN_lag2</th>\n",
       "      <th>FiO2_lag1</th>\n",
       "      <th>FiO2_lag2</th>\n",
       "      <th>GCS_lag1</th>\n",
       "      <th>GCS_lag2</th>\n",
       "      <th>K_lag1</th>\n",
       "      <th>K_lag2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132539</td>\n",
       "      <td>2025-03-10 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.950526</td>\n",
       "      <td>-0.23008</td>\n",
       "      <td>-0.596332</td>\n",
       "      <td>1.671639</td>\n",
       "      <td>-0.013487</td>\n",
       "      <td>-0.832594</td>\n",
       "      <td>-0.109176</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132539</td>\n",
       "      <td>2025-03-10 01:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.950526</td>\n",
       "      <td>-0.23008</td>\n",
       "      <td>-0.596332</td>\n",
       "      <td>1.967793</td>\n",
       "      <td>0.172112</td>\n",
       "      <td>-0.608431</td>\n",
       "      <td>-0.109176</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.514830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.176471</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.875</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132539</td>\n",
       "      <td>2025-03-10 02:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.950526</td>\n",
       "      <td>-0.23008</td>\n",
       "      <td>-0.596332</td>\n",
       "      <td>-1.734132</td>\n",
       "      <td>0.125712</td>\n",
       "      <td>0.848629</td>\n",
       "      <td>0.830987</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.090314</td>\n",
       "      <td>-0.514830</td>\n",
       "      <td>-0.058824</td>\n",
       "      <td>-0.176471</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132539</td>\n",
       "      <td>2025-03-10 03:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.950526</td>\n",
       "      <td>-0.23008</td>\n",
       "      <td>-0.596332</td>\n",
       "      <td>1.523562</td>\n",
       "      <td>0.380910</td>\n",
       "      <td>-0.832594</td>\n",
       "      <td>-0.579257</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.090314</td>\n",
       "      <td>-2.090314</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>-0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132539</td>\n",
       "      <td>2025-03-10 04:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.950526</td>\n",
       "      <td>-0.23008</td>\n",
       "      <td>-0.596332</td>\n",
       "      <td>0.487023</td>\n",
       "      <td>-0.964680</td>\n",
       "      <td>1.483758</td>\n",
       "      <td>-0.814297</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.090314</td>\n",
       "      <td>-2.090314</td>\n",
       "      <td>-0.588235</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RecordID                Time  Gender    Height   Weight       Age  \\\n",
       "0    132539 2025-03-10 00:00:00     0.0 -0.950526 -0.23008 -0.596332   \n",
       "1    132539 2025-03-10 01:00:00     0.0 -0.950526 -0.23008 -0.596332   \n",
       "2    132539 2025-03-10 02:00:00     0.0 -0.950526 -0.23008 -0.596332   \n",
       "3    132539 2025-03-10 03:00:00     0.0 -0.950526 -0.23008 -0.596332   \n",
       "4    132539 2025-03-10 04:00:00     0.0 -0.950526 -0.23008 -0.596332   \n",
       "\n",
       "    Albumin  Cholesterol   DiasABP      HCO3  ...  Temp_lag1  Temp_lag2  \\\n",
       "0  1.671639    -0.013487 -0.832594 -0.109176  ...        NaN        NaN   \n",
       "1  1.967793     0.172112 -0.608431 -0.109176  ...  -0.514830        NaN   \n",
       "2 -1.734132     0.125712  0.848629  0.830987  ...  -2.090314  -0.514830   \n",
       "3  1.523562     0.380910 -0.832594 -0.579257  ...  -2.090314  -2.090314   \n",
       "4  0.487023    -0.964680  1.483758 -0.814297  ...  -2.090314  -2.090314   \n",
       "\n",
       "   BUN_lag1  BUN_lag2  FiO2_lag1  FiO2_lag2  GCS_lag1  GCS_lag2  K_lag1  \\\n",
       "0       NaN       NaN        NaN        NaN       NaN       NaN     NaN   \n",
       "1 -0.176471       NaN        2.5        NaN  0.166667       NaN   1.875   \n",
       "2 -0.058824 -0.176471        1.5        2.5  0.166667  0.166667   0.125   \n",
       "3  0.117647 -0.058824        0.0        1.5  0.166667  0.166667   0.000   \n",
       "4 -0.588235  0.117647        0.0        0.0  0.166667  0.166667   0.625   \n",
       "\n",
       "   K_lag2  \n",
       "0     NaN  \n",
       "1     NaN  \n",
       "2   1.875  \n",
       "3   0.125  \n",
       "4   0.000  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_lag_features_for_selected(df, selected_features, lags=[1,2]):\n",
    "    \"\"\"\n",
    "    For each feature in selected_features, add lag features computed patient-wise.\n",
    "    \"\"\"\n",
    "    df_augmented = df.copy()\n",
    "    for feature in selected_features:\n",
    "        if feature not in df_augmented.columns:\n",
    "            continue\n",
    "        for lag in lags:\n",
    "            lag_col = f\"{feature}_lag{lag}\"\n",
    "            df_augmented[lag_col] = df_augmented.groupby(\"RecordID\")[feature].shift(lag)\n",
    "    return df_augmented\n",
    "\n",
    "# Augment train, validation, and test sets with lag features using the selected_features list.\n",
    "train_set_aug = add_lag_features_for_selected(train_set, selected_features, lags=[1,2])\n",
    "valid_set_aug = add_lag_features_for_selected(val_set, selected_features, lags=[1,2])\n",
    "test_set_aug  = add_lag_features_for_selected(test_set,  selected_features, lags=[1,2])\n",
    "\n",
    "# Print shapes\n",
    "print(train_set_aug.shape, valid_set_aug.shape, test_set_aug.shape)\n",
    "\n",
    "\"\"\"train_set_clean = train_set_aug.dropna()\n",
    "valid_set_clean = valid_set_aug.dropna()\n",
    "test_set_clean  = test_set_aug.dropna()\n",
    "\n",
    "print(f\"Train set shape after dropping NaNs: {train_set_clean.shape}\")\n",
    "print(f\"Validation set shape after dropping NaNs: {valid_set_clean.shape}\")\n",
    "print(f\"Test set shape after dropping NaNs: {test_set_clean.shape}\")\"\"\"\n",
    "train_set_aug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119970 119930 119980\n"
     ]
    }
   ],
   "source": [
    "# Print the number of NaN values after augmenting the features\n",
    "print(train_set_aug.isnull().sum().sum(), valid_set_aug.isnull().sum().sum(), test_set_aug.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Aggregation on the new augmented datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended aggregation rules:\n",
      "{'Age': 'last', 'Gender': 'last', 'Height': 'last', 'Albumin': 'last', 'ALP': 'last', 'ALT': 'last', 'AST': 'last', 'Bilirubin': 'last', 'BUN': 'last', 'Cholesterol': 'last', 'Creatinine': 'last', 'DiasABP': 'mean', 'FiO2': 'mean', 'GCS': 'min', 'Glucose': 'mean', 'HCO3': 'last', 'HCT': 'last', 'HR': 'mean', 'K': 'last', 'Lactate': 'max', 'Mg': 'last', 'MAP': 'mean', 'MechVent': 'last', 'Na': 'last', 'NIDiasABP': 'mean', 'NIMAP': 'mean', 'NISysABP': 'mean', 'PaCO2': 'last', 'PaO2': 'mean', 'pH': 'last', 'Platelets': 'last', 'RespRate': 'mean', 'SaO2': 'mean', 'SysABP': 'mean', 'Temp': 'max', 'TroponinI': 'max', 'TroponinT': 'max', 'Urine': 'sum', 'WBC': 'last', 'Weight': 'last', 'In-hospital_death': 'max', 'HCO3_lag1': 'last', 'HCO3_lag2': 'last', 'HCT_lag1': 'last', 'HCT_lag2': 'last', 'HR_lag1': 'last', 'HR_lag2': 'last', 'Mg_lag1': 'last', 'Mg_lag2': 'last', 'Na_lag1': 'last', 'Na_lag2': 'last', 'Temp_lag1': 'last', 'Temp_lag2': 'last', 'BUN_lag1': 'last', 'BUN_lag2': 'last', 'FiO2_lag1': 'last', 'FiO2_lag2': 'last', 'GCS_lag1': 'last', 'GCS_lag2': 'last', 'K_lag1': 'last', 'K_lag2': 'last'}\n",
      "(4000, 62)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecordID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>ALP</th>\n",
       "      <th>ALT</th>\n",
       "      <th>AST</th>\n",
       "      <th>Bilirubin</th>\n",
       "      <th>BUN</th>\n",
       "      <th>...</th>\n",
       "      <th>Temp_lag1</th>\n",
       "      <th>Temp_lag2</th>\n",
       "      <th>BUN_lag1</th>\n",
       "      <th>BUN_lag2</th>\n",
       "      <th>FiO2_lag1</th>\n",
       "      <th>FiO2_lag2</th>\n",
       "      <th>GCS_lag1</th>\n",
       "      <th>GCS_lag2</th>\n",
       "      <th>K_lag1</th>\n",
       "      <th>K_lag2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132539</td>\n",
       "      <td>-0.596332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.950526</td>\n",
       "      <td>-1.289901</td>\n",
       "      <td>-0.716981</td>\n",
       "      <td>-0.411765</td>\n",
       "      <td>0.802817</td>\n",
       "      <td>-0.181818</td>\n",
       "      <td>-0.588235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.745556</td>\n",
       "      <td>0.745556</td>\n",
       "      <td>-0.588235</td>\n",
       "      <td>-0.588235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132540</td>\n",
       "      <td>0.667051</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.617613</td>\n",
       "      <td>-1.289901</td>\n",
       "      <td>-0.716981</td>\n",
       "      <td>-0.411765</td>\n",
       "      <td>0.802817</td>\n",
       "      <td>-0.181818</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136714</td>\n",
       "      <td>-0.136714</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.750</td>\n",
       "      <td>-0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132541</td>\n",
       "      <td>-1.170597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.429614</td>\n",
       "      <td>-1.141824</td>\n",
       "      <td>0.471698</td>\n",
       "      <td>1.529412</td>\n",
       "      <td>1.718310</td>\n",
       "      <td>1.909091</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241402</td>\n",
       "      <td>1.375750</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132543</td>\n",
       "      <td>0.207639</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.157978</td>\n",
       "      <td>1.967793</td>\n",
       "      <td>0.471698</td>\n",
       "      <td>-0.323529</td>\n",
       "      <td>-0.380282</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>-0.470588</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.775217</td>\n",
       "      <td>-1.775217</td>\n",
       "      <td>-0.470588</td>\n",
       "      <td>-0.470588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>-0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132545</td>\n",
       "      <td>1.356169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.252050</td>\n",
       "      <td>0.338946</td>\n",
       "      <td>-0.716981</td>\n",
       "      <td>-0.411765</td>\n",
       "      <td>0.802817</td>\n",
       "      <td>-0.181818</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.388792</td>\n",
       "      <td>-0.388792</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RecordID       Age  Gender    Height   Albumin       ALP       ALT  \\\n",
       "0    132539 -0.596332     0.0 -0.950526 -1.289901 -0.716981 -0.411765   \n",
       "1    132540  0.667051     1.0  0.617613 -1.289901 -0.716981 -0.411765   \n",
       "2    132541 -1.170597     0.0 -0.429614 -1.141824  0.471698  1.529412   \n",
       "3    132543  0.207639     1.0  1.157978  1.967793  0.471698 -0.323529   \n",
       "4    132545  1.356169     0.0 -1.252050  0.338946 -0.716981 -0.411765   \n",
       "\n",
       "        AST  Bilirubin       BUN  ...  Temp_lag1  Temp_lag2  BUN_lag1  \\\n",
       "0  0.802817  -0.181818 -0.588235  ...   0.745556   0.745556 -0.588235   \n",
       "1  0.802817  -0.181818  0.176471  ...  -0.136714  -0.136714  0.176471   \n",
       "2  1.718310   1.909091 -0.882353  ...   0.241402   1.375750 -0.882353   \n",
       "3 -0.380282  -0.454545 -0.470588  ...  -1.775217  -1.775217 -0.470588   \n",
       "4  0.802817  -0.181818  0.411765  ...  -0.388792  -0.388792  0.411765   \n",
       "\n",
       "   BUN_lag2  FiO2_lag1  FiO2_lag2  GCS_lag1  GCS_lag2  K_lag1  K_lag2  \n",
       "0 -0.588235        0.0       -0.5  0.166667  0.166667  -0.125  -0.125  \n",
       "1  0.176471       -0.5       -0.5  0.166667  0.166667  -0.750  -0.750  \n",
       "2 -0.882353       -0.5       -0.5 -1.500000 -1.500000  -0.500  -0.500  \n",
       "3 -0.470588        0.0       -0.5  0.166667  0.166667  -0.375  -0.375  \n",
       "4  0.411765        0.0       -0.5  0.166667  0.166667   0.000   0.000  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_aggregation_rules = aggregation_rules.copy()\n",
    "for col in train_set_aug.columns:\n",
    "    if \"lag\" in col:\n",
    "        extended_aggregation_rules[col] = \"last\"\n",
    "\n",
    "print(\"Extended aggregation rules:\")\n",
    "print(extended_aggregation_rules)\n",
    "\n",
    "# Now perform aggregation on the train, validation, and test sets.\n",
    "train_aggregated = train_set_aug.groupby(\"RecordID\").agg(extended_aggregation_rules).reset_index()\n",
    "val_aggregated   = valid_set_aug.groupby(\"RecordID\").agg(extended_aggregation_rules).reset_index()\n",
    "test_aggregated  = test_set_aug.groupby(\"RecordID\").agg(extended_aggregation_rules).reset_index()\n",
    "\n",
    "# Display the processed dataset (for example, for the train set)\n",
    "print(train_aggregated.shape)\n",
    "train_aggregated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using \"last\" for the newly inserted columns, the NaN values should disappear. However, we need to check if there are patients that have NaN values in their last row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients with NaN values in their last row:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecordID</th>\n",
       "      <th>Time</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Age</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>DiasABP</th>\n",
       "      <th>HCO3</th>\n",
       "      <th>...</th>\n",
       "      <th>Temp_lag1</th>\n",
       "      <th>Temp_lag2</th>\n",
       "      <th>BUN_lag1</th>\n",
       "      <th>BUN_lag2</th>\n",
       "      <th>FiO2_lag1</th>\n",
       "      <th>FiO2_lag2</th>\n",
       "      <th>GCS_lag1</th>\n",
       "      <th>GCS_lag2</th>\n",
       "      <th>K_lag1</th>\n",
       "      <th>K_lag2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19632</th>\n",
       "      <td>133628</td>\n",
       "      <td>2025-03-10 05:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.447939</td>\n",
       "      <td>-0.515385</td>\n",
       "      <td>1.356169</td>\n",
       "      <td>0.042792</td>\n",
       "      <td>-0.013487</td>\n",
       "      <td>-2.700621</td>\n",
       "      <td>-0.579257</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.51483</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.176471</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.875</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107728</th>\n",
       "      <td>138477</td>\n",
       "      <td>2025-03-10 10:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.704120</td>\n",
       "      <td>-0.141250</td>\n",
       "      <td>-0.194347</td>\n",
       "      <td>-0.105285</td>\n",
       "      <td>-1.196678</td>\n",
       "      <td>0.064058</td>\n",
       "      <td>0.360906</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.51483</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.176471</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.875</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117892</th>\n",
       "      <td>139060</td>\n",
       "      <td>2025-03-10 06:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.723525</td>\n",
       "      <td>0.775927</td>\n",
       "      <td>-1.285450</td>\n",
       "      <td>0.635100</td>\n",
       "      <td>-1.405476</td>\n",
       "      <td>0.213500</td>\n",
       "      <td>-0.344216</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.51483</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.176471</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.875</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144254</th>\n",
       "      <td>140501</td>\n",
       "      <td>2025-03-10 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.447939</td>\n",
       "      <td>0.351470</td>\n",
       "      <td>-1.917142</td>\n",
       "      <td>1.671639</td>\n",
       "      <td>-0.013487</td>\n",
       "      <td>-0.832594</td>\n",
       "      <td>-0.109176</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151735</th>\n",
       "      <td>140936</td>\n",
       "      <td>2025-03-10 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.327653</td>\n",
       "      <td>0.074041</td>\n",
       "      <td>-0.883465</td>\n",
       "      <td>1.671639</td>\n",
       "      <td>-0.013487</td>\n",
       "      <td>-0.832594</td>\n",
       "      <td>-0.109176</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157699</th>\n",
       "      <td>141264</td>\n",
       "      <td>2025-03-10 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.526556</td>\n",
       "      <td>-0.800253</td>\n",
       "      <td>1.471022</td>\n",
       "      <td>1.671639</td>\n",
       "      <td>-0.013487</td>\n",
       "      <td>-0.832594</td>\n",
       "      <td>-0.109176</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        RecordID                Time  Gender    Height    Weight       Age  \\\n",
       "19632     133628 2025-03-10 05:00:00     1.0  0.447939 -0.515385  1.356169   \n",
       "107728    138477 2025-03-10 10:00:00     0.0 -0.704120 -0.141250 -0.194347   \n",
       "117892    139060 2025-03-10 06:00:00     1.0  0.723525  0.775927 -1.285450   \n",
       "144254    140501 2025-03-10 00:00:00     1.0  0.447939  0.351470 -1.917142   \n",
       "151735    140936 2025-03-10 00:00:00     1.0  1.327653  0.074041 -0.883465   \n",
       "157699    141264 2025-03-10 00:00:00     0.0 -1.526556 -0.800253  1.471022   \n",
       "\n",
       "         Albumin  Cholesterol   DiasABP      HCO3  ...  Temp_lag1  Temp_lag2  \\\n",
       "19632   0.042792    -0.013487 -2.700621 -0.579257  ...   -0.51483        NaN   \n",
       "107728 -0.105285    -1.196678  0.064058  0.360906  ...   -0.51483        NaN   \n",
       "117892  0.635100    -1.405476  0.213500 -0.344216  ...   -0.51483        NaN   \n",
       "144254  1.671639    -0.013487 -0.832594 -0.109176  ...        NaN        NaN   \n",
       "151735  1.671639    -0.013487 -0.832594 -0.109176  ...        NaN        NaN   \n",
       "157699  1.671639    -0.013487 -0.832594 -0.109176  ...        NaN        NaN   \n",
       "\n",
       "        BUN_lag1  BUN_lag2  FiO2_lag1  FiO2_lag2  GCS_lag1  GCS_lag2  K_lag1  \\\n",
       "19632  -0.176471       NaN        2.5        NaN  0.166667       NaN   1.875   \n",
       "107728 -0.176471       NaN        2.5        NaN  0.166667       NaN   1.875   \n",
       "117892 -0.176471       NaN        2.5        NaN  0.166667       NaN   1.875   \n",
       "144254       NaN       NaN        NaN        NaN       NaN       NaN     NaN   \n",
       "151735       NaN       NaN        NaN        NaN       NaN       NaN     NaN   \n",
       "157699       NaN       NaN        NaN        NaN       NaN       NaN     NaN   \n",
       "\n",
       "        K_lag2  \n",
       "19632      NaN  \n",
       "107728     NaN  \n",
       "117892     NaN  \n",
       "144254     NaN  \n",
       "151735     NaN  \n",
       "157699     NaN  \n",
       "\n",
       "[6 rows x 63 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the entire row of patients (RecordID) that have NaN values in their last row (last time point)\n",
    "# Get the last row for each patient (grouped by RecordID)\n",
    "last_rows = train_set_aug.groupby(\"RecordID\").tail(1)\n",
    "\n",
    "# Filter to get only those rows with at least one NaN value\n",
    "missing_last = last_rows[last_rows.isna().any(axis=1)]\n",
    "\n",
    "# Print the full rows for those patients\n",
    "print(\"Patients with NaN values in their last row:\")\n",
    "missing_last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that some patients have NaN values as last row. This happens only for patients with < 3 measurements (hence the lag does not work). We can simply remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3994, 62), (3992, 62), (3995, 62))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rows that include NaN values\n",
    "train_aggregated_clean = train_aggregated.dropna()\n",
    "val_aggregated_clean = val_aggregated.dropna()\n",
    "test_aggregated_clean = test_aggregated.dropna()\n",
    "train_aggregated_clean.shape, val_aggregated_clean.shape, test_aggregated_clean.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't loose much, namely 6, 8 and 5 patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression w/ Feature Lagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.840, AUPRC: 0.492\n",
      "Test ROC AUC: 0.837, AUPRC: 0.491\n"
     ]
    }
   ],
   "source": [
    "def prepare_xy(df):\n",
    "    X = df.drop(columns=[\"In-hospital_death\", \"RecordID\"])  # drop non-feature columns\n",
    "    y = df[\"In-hospital_death\"]  # target column\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = prepare_xy(train_aggregated_clean)\n",
    "X_valid, y_valid = prepare_xy(val_aggregated_clean)\n",
    "X_test,  y_test  = prepare_xy(test_aggregated_clean)\n",
    "\n",
    "# Initialize and train Logistic Regression.\n",
    "clf = LogisticRegression(random_state=SEED, max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation set.\n",
    "y_valid_proba = clf.predict_proba(X_valid)[:, 1]\n",
    "roc_auc_valid = roc_auc_score(y_valid, y_valid_proba)\n",
    "auprc_valid = average_precision_score(y_valid, y_valid_proba)\n",
    "print(f\"Validation ROC AUC: {roc_auc_valid:.3f}, AUPRC: {auprc_valid:.3f}\")\n",
    "\n",
    "# Evaluate on test set.\n",
    "y_test_proba = clf.predict_proba(X_test)[:, 1]\n",
    "roc_auc_test = roc_auc_score(y_test, y_test_proba)\n",
    "auprc_test = average_precision_score(y_test, y_test_proba)\n",
    "print(f\"Test ROC AUC: {roc_auc_test:.3f}, AUPRC: {auprc_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest w/ Feature Lagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.837, AUPRC: 0.487\n",
      "Test ROC AUC: 0.831, AUPRC: 0.497\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train Random Forest.\n",
    "clf = RandomForestClassifier(random_state=SEED)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation set.\n",
    "y_valid_proba = clf.predict_proba(X_valid)[:, 1]\n",
    "roc_auc_valid = roc_auc_score(y_valid, y_valid_proba)\n",
    "auprc_valid = average_precision_score(y_valid, y_valid_proba)\n",
    "print(f\"Validation ROC AUC: {roc_auc_valid:.3f}, AUPRC: {auprc_valid:.3f}\")\n",
    "\n",
    "# Evaluate on test set.\n",
    "y_test_proba = clf.predict_proba(X_test)[:, 1]\n",
    "roc_auc_test = roc_auc_score(y_test, y_test_proba)\n",
    "auprc_test = average_precision_score(y_test, y_test_proba)\n",
    "print(f\"Test ROC AUC: {roc_auc_test:.3f}, AUPRC: {auprc_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal processing-based features\n",
    "\n",
    "Use tsfresh to extract a large amount of features. \n",
    "\n",
    "*Note* the use of MinimalFCParameters, to reduce the number of variables inspected (and so the high computational cost) \n",
    "\n",
    "Also tryied to substitute w/ EfficientFCParameters but still the run time is too much.\n",
    "\n",
    "Maybe we should try to use the student cluster but we'll see\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 20/20 [00:10<00:00,  1.92it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:10<00:00,  2.00it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:09<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set relevant features: (4000, 230)\n",
      "Validation set relevant features: (4000, 230)\n",
      "Test set relevant features: (4000, 230)\n"
     ]
    }
   ],
   "source": [
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.feature_extraction.settings import MinimalFCParameters\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "# Define the column names\n",
    "id_column = \"RecordID\"\n",
    "time_column = \"Time\"\n",
    "target_column = \"In-hospital_death\"\n",
    "\n",
    "# Step 1: Extract all features from train_set using MinimalFCParameters\n",
    "X_train_extracted = extract_features(\n",
    "    train_set.drop(columns=[target_column]),\n",
    "    column_id=id_column,\n",
    "    column_sort=time_column,\n",
    "    default_fc_parameters=MinimalFCParameters()  # Use minimal features\n",
    ")\n",
    "\n",
    "# Impute missing values (tsfresh requires no NaNs)\n",
    "X_train_extracted = impute(X_train_extracted)\n",
    "\n",
    "# Step 2: Select relevant features based on the target variable\n",
    "y_train = train_set.groupby(id_column)[target_column].max()\n",
    "X_train_relevant = select_features(X_train_extracted, y_train)\n",
    "\n",
    "# Step 3: Store the relevant feature names\n",
    "relevant_feature_names = X_train_relevant.columns\n",
    "\n",
    "# Step 4: Extract the same features for val_set\n",
    "X_val_extracted = extract_features(\n",
    "    val_set.drop(columns=[target_column]),\n",
    "    column_id=id_column,\n",
    "    column_sort=time_column,\n",
    "    default_fc_parameters=MinimalFCParameters()  # Use the same settings\n",
    ")\n",
    "X_val_extracted = impute(X_val_extracted)  # Impute missing values\n",
    "X_val_relevant = X_val_extracted[relevant_feature_names]  # Keep only relevant features\n",
    "\n",
    "# Step 5: Extract the same features for test_set\n",
    "X_test_extracted = extract_features(\n",
    "    test_set.drop(columns=[target_column]),\n",
    "    column_id=id_column,\n",
    "    column_sort=time_column,\n",
    "    default_fc_parameters=MinimalFCParameters()  # Use the same settings\n",
    ")\n",
    "X_test_extracted = impute(X_test_extracted)  # Impute missing values\n",
    "X_test_relevant = X_test_extracted[relevant_feature_names]  # Keep only relevant features\n",
    "\n",
    "# Now X_train_relevant, X_val_relevant, and X_test_relevant have the same features\n",
    "print(\"Train set relevant features:\", X_train_relevant.shape)\n",
    "print(\"Validation set relevant features:\", X_val_relevant.shape)\n",
    "print(\"Test set relevant features:\", X_test_relevant.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For train_set\n",
    "y_train = train_set.groupby(id_column)[target_column].max()\n",
    "\n",
    "# For val_set\n",
    "y_val = val_set.groupby(id_column)[target_column].max()\n",
    "\n",
    "# For test_set\n",
    "y_test = test_set.groupby(id_column)[target_column].max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression w/ tsfresh features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.735, AUPRC: 0.333\n",
      "Test ROC AUC: 0.797, AUPRC: 0.445\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Scale the features ---\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform\n",
    "X_train_scaled = scaler.fit_transform(X_train_relevant)\n",
    "\n",
    "# Transform the validation and test sets using the same scaler\n",
    "X_val_scaled = scaler.transform(X_val_relevant)\n",
    "X_test_scaled = scaler.transform(X_test_relevant)\n",
    "\n",
    "# --- Create and train the Logistic Regression classifier ---\n",
    "clf = LogisticRegression(random_state=42, max_iter=5000)  # Increase iterations to 5000\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# --- Evaluate on the validation set ---\n",
    "y_val_proba = clf.predict_proba(X_val_scaled)[:, 1]  # Probability for the positive class\n",
    "roc_auc_val = roc_auc_score(y_val, y_val_proba)\n",
    "auprc_val = average_precision_score(y_val, y_val_proba)\n",
    "print(f\"Validation ROC AUC: {roc_auc_val:.3f}, AUPRC: {auprc_val:.3f}\")\n",
    "\n",
    "# --- Evaluate on the test set ---\n",
    "y_test_proba = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "roc_auc_test = roc_auc_score(y_test, y_test_proba)\n",
    "auprc_test = average_precision_score(y_test, y_test_proba)\n",
    "print(f\"Test ROC AUC: {roc_auc_test:.3f}, AUPRC: {auprc_test:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is a total of 215 features exctracted, we'll try with some regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression w/ tsfresh features && L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best regularization parameter (C): 0.01\n",
      "Validation ROC AUC: 0.768, AUPRC: 0.383\n",
      "Test ROC AUC: 0.813, AUPRC: 0.476\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# --- Scale the features ---\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform\n",
    "X_train_scaled = scaler.fit_transform(X_train_relevant)\n",
    "\n",
    "# Transform the validation and test sets using the same scaler\n",
    "X_val_scaled = scaler.transform(X_val_relevant)\n",
    "X_test_scaled = scaler.transform(X_test_relevant)\n",
    "\n",
    "# --- Define the Logistic Regression model ---\n",
    "# Use L2 regularization (default) and set up the solver\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=5000, penalty='l2', solver='liblinear')\n",
    "\n",
    "# --- Set up the grid search for hyperparameter tuning ---\n",
    "# Define the range of C values to test\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "\n",
    "# Use ROC AUC as the scoring metric for grid search\n",
    "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, scoring='roc_auc', cv=5, verbose=1)\n",
    "\n",
    "# --- Perform grid search on the training data ---\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# --- Get the best model and its parameters ---\n",
    "best_clf = grid_search.best_estimator_\n",
    "best_C = grid_search.best_params_['C']\n",
    "print(f\"Best regularization parameter (C): {best_C}\")\n",
    "\n",
    "# --- Evaluate on the validation set using the best model ---\n",
    "y_val_proba = best_clf.predict_proba(X_val_scaled)[:, 1]  # Probability for the positive class\n",
    "roc_auc_val = roc_auc_score(y_val, y_val_proba)\n",
    "auprc_val = average_precision_score(y_val, y_val_proba)\n",
    "print(f\"Validation ROC AUC: {roc_auc_val:.3f}, AUPRC: {auprc_val:.3f}\")\n",
    "\n",
    "# --- Evaluate on the test set using the best model ---\n",
    "y_test_proba = best_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "roc_auc_test = roc_auc_score(y_test, y_test_proba)\n",
    "auprc_test = average_precision_score(y_test, y_test_proba)\n",
    "print(f\"Test ROC AUC: {roc_auc_test:.3f}, AUPRC: {auprc_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression w/ tsfresh features && L1 Regularization\n",
    "\n",
    "Since Lasso convergence is slower, we improved it by using the saga solver, increasing the iterations, adjusting the tolerance and also doing the grid search with every processor (n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best regularization parameter (C): 0.1\n",
      "Validation ROC AUC: 0.777, AUPRC: 0.397\n",
      "Test ROC AUC: 0.816, AUPRC: 0.479\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# --- Scale the features ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_relevant)\n",
    "X_val_scaled = scaler.transform(X_val_relevant)\n",
    "X_test_scaled = scaler.transform(X_test_relevant)\n",
    "\n",
    "# --- Define the Logistic Regression model with L1 regularization ---\n",
    "log_reg = LogisticRegression(\n",
    "    penalty='l1', \n",
    "    solver='saga', \n",
    "    max_iter=5000,  # Increase max_iter\n",
    "    tol=1e-3,       # Adjust tolerance\n",
    "    warm_start=True, # Enable warm start\n",
    "    random_state=42\n",
    ")\n",
    "# --- Set up the grid search for hyperparameter tuning ---\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, scoring='roc_auc', cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# --- Perform grid search on the training data ---\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# --- Get the best model and its parameters ---\n",
    "best_clf = grid_search.best_estimator_\n",
    "best_C = grid_search.best_params_['C']\n",
    "print(f\"Best regularization parameter (C): {best_C}\")\n",
    "\n",
    "# --- Evaluate on the validation set using the best model ---\n",
    "y_val_proba = best_clf.predict_proba(X_val_scaled)[:, 1]\n",
    "roc_auc_val = roc_auc_score(y_val, y_val_proba)\n",
    "auprc_val = average_precision_score(y_val, y_val_proba)\n",
    "print(f\"Validation ROC AUC: {roc_auc_val:.3f}, AUPRC: {auprc_val:.3f}\")\n",
    "\n",
    "# --- Evaluate on the test set using the best model ---\n",
    "y_test_proba = best_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "roc_auc_test = roc_auc_score(y_test, y_test_proba)\n",
    "auprc_test = average_precision_score(y_test, y_test_proba)\n",
    "print(f\"Test ROC AUC: {roc_auc_test:.3f}, AUPRC: {auprc_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest w/ tsfresh features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best hyperparameters: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 20}\n",
      "Validation ROC AUC: 0.820, AUPRC: 0.452\n",
      "Test ROC AUC: 0.810, AUPRC: 0.465\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "# --- Define the Random Forest Classifier ---\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)  # Use all CPU cores\n",
    "\n",
    "# --- Set up the randomized search for hyperparameter tuning ---\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200],  # Number of trees\n",
    "    'max_depth': [10, 20],       # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5], # Minimum samples to split a node\n",
    "    'min_samples_leaf': [1, 2],  # Minimum samples at each leaf node\n",
    "    'max_features': ['sqrt', 'log2']  # Number of features to consider at each split\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV with a fixed number of iterations\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,  # Number of hyperparameter combinations to try\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- Perform randomized search on the training data ---\n",
    "random_search.fit(X_train_relevant, y_train)\n",
    "\n",
    "# --- Get the best model and its parameters ---\n",
    "best_clf = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "# --- Evaluate on the validation set using the best model ---\n",
    "y_val_proba = best_clf.predict_proba(X_val_relevant)[:, 1]  # Probability for the positive class\n",
    "roc_auc_val = roc_auc_score(y_val, y_val_proba)\n",
    "auprc_val = average_precision_score(y_val, y_val_proba)\n",
    "print(f\"Validation ROC AUC: {roc_auc_val:.3f}, AUPRC: {auprc_val:.3f}\")\n",
    "\n",
    "# --- Evaluate on the test set using the best model ---\n",
    "y_test_proba = best_clf.predict_proba(X_test_relevant)[:, 1]\n",
    "roc_auc_test = roc_auc_score(y_test, y_test_proba)\n",
    "auprc_test = average_precision_score(y_test, y_test_proba)\n",
    "print(f\"Test ROC AUC: {roc_auc_test:.3f}, AUPRC: {auprc_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.806, AUPRC: 0.421\n",
      "Test ROC AUC: 0.778, AUPRC: 0.422\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# --- Definisci il modello Random Forest con parametri predefiniti ---\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)  # Usa tutti i core della CPU\n",
    "\n",
    "# --- Addestra il modello sui dati di training ---\n",
    "rf.fit(X_train_relevant, y_train)\n",
    "\n",
    "# --- Valuta sul validation set ---\n",
    "y_val_proba = rf.predict_proba(X_val_relevant)[:, 1]  # Probabilità per la classe positiva\n",
    "roc_auc_val = roc_auc_score(y_val, y_val_proba)\n",
    "auprc_val = average_precision_score(y_val, y_val_proba)\n",
    "print(f\"Validation ROC AUC: {roc_auc_val:.3f}, AUPRC: {auprc_val:.3f}\")\n",
    "\n",
    "# --- Valuta sul test set ---\n",
    "y_test_proba = rf.predict_proba(X_test_relevant)[:, 1]\n",
    "roc_auc_test = roc_auc_score(y_test, y_test_proba)\n",
    "auprc_test = average_precision_score(y_test, y_test_proba)\n",
    "print(f\"Test ROC AUC: {roc_auc_test:.3f}, AUPRC: {auprc_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2.2 - Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with LSTM, we figured out it was better scaling the data using only MinMaxScaler. Let's do it then!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Set A: (183416, 43) Set B: (183495, 43) Set C: (183711, 43)\n",
      "Shapes of labels:\n",
      "Set A: (4000, 2) Set B: (4000, 2) Set C: (4000, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from project_1.config import PROJ_ROOT, PROCESSED_DATA_DIR\n",
    "from project_1.loading import *\n",
    "from project_1.dataset import *\n",
    "from project_1.features import *\n",
    "\n",
    "set_a, set_b, set_c = load_before_scaling()\n",
    "# Scale using basic scaling\n",
    "set_a_scaled, set_b_scaled, set_c_scaled = scale_features_basic(set_a, set_b, set_c)\n",
    "\n",
    "# Remove the ICUType feature\n",
    "set_a_scaled = set_a_scaled.drop(columns=['ICUType'])\n",
    "set_b_scaled = set_b_scaled.drop(columns=['ICUType'])\n",
    "set_c_scaled = set_c_scaled.drop(columns=['ICUType'])\n",
    "death_a, death_b, death_c = load_outcomes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (Long Short Term Memory) w/ Taking last Hidden State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, num_classes=1, dropout=0.3):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,       # 41 features per time step\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_size)\n",
    "        out, _ = self.lstm(x)           # out: (batch_size, seq_len, hidden_size)\n",
    "        out = out[:, -1, :]             # Take last time step: (batch_size, hidden_size)\n",
    "        out = self.fc(out)              # (batch_size, num_classes)\n",
    "        return out.squeeze()            # (batch_size,) for BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TensorDatasets from Time-Series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4000, 49, 40])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = create_dataset_from_timeseries(set_a_scaled, death_a[\"In-hospital_death\"])\n",
    "validation_dataset = create_dataset_from_timeseries(set_b_scaled, death_b[\"In-hospital_death\"])\n",
    "test_dataset = create_dataset_from_timeseries(set_c_scaled, death_c[\"In-hospital_death\"])\n",
    "\n",
    "train_dataset.tensors[0].shape # (batch_size, seq_len, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescobondi/anaconda3/envs/TUM/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  Train Loss: 0.4519 | AUCROC: 0.5865 | AUPRC: 0.1663\n",
      "  Val   Loss: 0.3444 | AUCROC: 0.7785 | AUPRC: 0.4238\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "  Train Loss: 0.1661 | AUCROC: 0.9586 | AUPRC: 0.8283\n",
      "  Val   Loss: 0.4117 | AUCROC: 0.7958 | AUPRC: 0.3913\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size = train_dataset.tensors[0].shape[-1]\n",
    "model = LSTM_Model(input_size=input_size).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Call the trainig loop (default 10 epochs)\n",
    "model = train_model_with_validation(model, train_loader, validation_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation - Loss: 0.6900 - AUCROC: 0.7586 - AUPRC: 0.3453\n",
      "Test Loss: 0.6900, AUC-ROC: 0.7586, AUC-PRC: 0.3453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "avg_loss, aucroc, auprc = evaluate_model(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {avg_loss:.4f}, AUC-ROC: {aucroc:.4f}, AUC-PRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM w/ Mean Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model_Pooling(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, num_classes=1, dropout=0.3):\n",
    "        super(LSTM_Model_Pooling, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,       # 40 features per time step\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_size)\n",
    "        out, _ = self.lstm(x)           # out: (batch_size, seq_len, hidden_size)\n",
    "        out = out.mean(dim=1)           # Pooling: (batch_size, hidden_size)   \n",
    "        out = self.fc(out)              # (batch_size, num_classes)\n",
    "        return out.squeeze()            # (batch_size,) for BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  Train Loss: 0.4577 | AUCROC: 0.5412 | AUPRC: 0.1512\n",
      "  Val   Loss: 0.3753 | AUCROC: 0.7340 | AUPRC: 0.3125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "  Train Loss: 0.1942 | AUCROC: 0.9426 | AUPRC: 0.7749\n",
      "  Val   Loss: 0.4011 | AUCROC: 0.7944 | AUPRC: 0.3882\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    }
   ],
   "source": [
    "# Use the previous data loaders and train the new model\n",
    "model_pooling = LSTM_Model_Pooling(input_size=input_size).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model_pooling.parameters(), lr=0.001)\n",
    "\n",
    "model_pooling = train_model_with_validation(model_pooling, train_loader, validation_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation - Loss: 0.7406 - AUCROC: 0.7532 - AUPRC: 0.3628\n",
      "Test Loss: 0.7406, AUC-ROC: 0.7532, AUC-PRC: 0.3628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Now evaluate the model\n",
    "avg_loss, aucroc, auprc = evaluate_model(model_pooling, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {avg_loss:.4f}, AUC-ROC: {aucroc:.4f}, AUC-PRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM w/ Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model_Max_Pooling(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, num_classes=1, dropout=0.3):\n",
    "        super(LSTM_Model_Max_Pooling, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,       # 40 features per time step\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_size)\n",
    "        out, _ = self.lstm(x)           # out: (batch_size, seq_len, hidden_size)\n",
    "        out, _ = out.max(dim=1)           # Pooling: (batch_size, hidden_size)   \n",
    "        out = self.fc(out)              # (batch_size, num_classes)\n",
    "        return out.squeeze()            # (batch_size,) for BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  Train Loss: 0.4482 | AUCROC: 0.5428 | AUPRC: 0.1499\n",
      "  Val   Loss: 0.3999 | AUCROC: 0.7142 | AUPRC: 0.3002\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "  Train Loss: 0.1960 | AUCROC: 0.9376 | AUPRC: 0.7655\n",
      "  Val   Loss: 0.4062 | AUCROC: 0.8076 | AUPRC: 0.4017\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    }
   ],
   "source": [
    "# Use the previous data loaders and train the new model\n",
    "model_max_pooling = LSTM_Model_Max_Pooling(input_size=input_size).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model_max_pooling.parameters(), lr=0.001)\n",
    "\n",
    "model_max_pooling = train_model_with_validation(model_max_pooling, train_loader, validation_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation - Loss: 0.5317 - AUCROC: 0.7786 - AUPRC: 0.3793\n",
      "Test Loss: 0.5317, AUC-ROC: 0.7786, AUC-PRC: 0.3793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Now evaluate the model\n",
    "avg_loss, aucroc, auprc = evaluate_model(model_max_pooling, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {avg_loss:.4f}, AUC-ROC: {aucroc:.4f}, AUC-PRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model_Bi(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, num_classes=1, dropout=0.3):\n",
    "        super(LSTM_Model_Bi, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,       # 41 features per time step\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes) # *2 for bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_size)\n",
    "        out, _ = self.lstm(x)           # out: (batch_size, seq_len, hidden_size)\n",
    "        out = out[:, -1, :]             # Take last time step: (batch_size, hidden_size)\n",
    "        out = self.fc(out)              # (batch_size, num_classes)\n",
    "        return out.squeeze()            # (batch_size,) for BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  Train Loss: 0.4522 | AUCROC: 0.5534 | AUPRC: 0.1687\n",
      "  Val   Loss: 0.3486 | AUCROC: 0.7666 | AUPRC: 0.3994\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "  Train Loss: 0.1743 | AUCROC: 0.9523 | AUPRC: 0.8123\n",
      "  Val   Loss: 0.3777 | AUCROC: 0.8174 | AUPRC: 0.4125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model_bi = LSTM_Model_Bi(input_size=input_size).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model_bi.parameters(), lr=0.001)\n",
    "\n",
    "model_bi = train_model_with_validation(model_bi, train_loader, validation_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now evaluate\n",
    "avg_loss, aucroc, auprc = evaluate_model(model_bi, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {avg_loss:.4f}, AUC-ROC: {aucroc:.4f}, AUC-PRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2.3a: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer w/ Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=1, nhead=4, num_layers=2, dim_feedforward=128, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Project input features to model dimension\n",
    "        self.embedding = nn.Linear(input_size, dim_feedforward)\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.pos_encoder = PositionalEncoding(dim_feedforward, dropout)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_feedforward,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward * 2,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Final classifier\n",
    "        self.fc = nn.Linear(dim_feedforward, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_size)\n",
    "\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1) # (batch, 1, input_size)\n",
    "\n",
    "        x = self.embedding(x)                # (batch, seq_len, d_model)\n",
    "        #print(\"After embedding:\", x.shape)  # Debug print\n",
    "        x = self.pos_encoder(x)\n",
    "        #print(\"After pos encoding:\", x.shape)  # Debug print\n",
    "        x = self.transformer_encoder(x)      # (batch, seq_len, d_model)\n",
    "        #print(\"After transformer encoder:\", x.shape)\n",
    "\n",
    "        x = x.mean(dim=1)                    # mean pooling over time\n",
    "        #print(\"After pooling:\", x.shape)     # Debug print\n",
    "        out = self.fc(x).squeeze()           # (batch,)\n",
    "        #print(\"After fc:\", out.shape)        # Debug print\n",
    "        return out\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=500):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Transformer model\n",
    "model_transformer = TransformerClassifier(input_size=input_size).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model_transformer.parameters(), lr=0.001)\n",
    "\n",
    "model_transformer = train_model_with_validation(model_transformer, train_loader, validation_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "avg_loss, aucroc, auprc = evaluate_model(model_transformer, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {avg_loss:.4f}, AUC-ROC: {aucroc:.4f}, AUC-PRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2.3b: Tokenizing Time-series Data and Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this part, we need to load the initial data\n",
    "set_a_initial, set_b_initial, set_c_initial = load_basic_data()\n",
    "set_a_initial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the TZV Dataframe (following Horn et al.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def build_TZV_dataframe(original_df, label_df, base_time=\"2025-03-10 00:00:00\", duration_hours=48):\n",
    "    \"\"\"\n",
    "    Build a long-format dataframe with columns [T, Z, V, y] from an original wide dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "        original_df (pd.DataFrame): DataFrame with columns [RecordID, Time, f1, f2, ..., f41].\n",
    "        label_df (pd.DataFrame): DataFrame with columns [RecordID, y] containing the label for each RecordID.\n",
    "        base_time (str): Base time used for normalizing the Time column.\n",
    "        duration_hours (int): The duration (in hours) from base_time over which Time is normalized (here, 48 hours).\n",
    "    \n",
    "    Returns:\n",
    "        long_df (pd.DataFrame): Long-format dataframe with columns:\n",
    "                                T: normalized time [0, 1],\n",
    "                                Z: index of the feature,\n",
    "                                V: scaled measurement value,\n",
    "                                y: label corresponding to RecordID.\n",
    "        feature_to_index (dict): Mapping from original feature names to integer indices.\n",
    "    \"\"\"\n",
    "    # Merge the labels with the original dataframe using RecordID.\n",
    "    df = original_df.copy().merge(label_df, on=\"RecordID\", how=\"left\")\n",
    "    \n",
    "    # Convert Time to datetime and compute normalized time T.\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"Time\"])\n",
    "    start_time = pd.to_datetime(base_time)\n",
    "    end_time = start_time + pd.Timedelta(hours=duration_hours)\n",
    "    total_seconds = (end_time - start_time).total_seconds()\n",
    "    df[\"T\"] = (df[\"Time\"] - start_time).dt.total_seconds() / total_seconds\n",
    "    \n",
    "    # Identify feature columns: all columns except RecordID, Time, T, and y.\n",
    "    feature_cols = [col for col in df.columns if col not in [\"RecordID\", \"Time\", \"T\", \"In-hospital_death\"]]\n",
    "    \n",
    "    # Scale each feature individually using MinMaxScaler.\n",
    "    scaler = MinMaxScaler()\n",
    "    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "    \n",
    "    # Melt the dataframe from wide to long format.\n",
    "    # The id_vars (\"T\" and \"y\") are preserved for each measurement.\n",
    "    long_df = pd.melt(df, id_vars=[\"T\", \"In-hospital_death\"], value_vars=feature_cols, \n",
    "                      var_name=\"Z\", value_name=\"V\")\n",
    "    \n",
    "    # Map feature names to indices for the \"Z\" column.\n",
    "    feature_to_index = {feat: idx for idx, feat in enumerate(feature_cols)}\n",
    "    long_df[\"Z\"] = long_df[\"Z\"].map(feature_to_index)\n",
    "    \n",
    "    # Sort the final dataframe by normalized time T and reset the index.\n",
    "    long_df = long_df.sort_values(\"T\").reset_index(drop=True)\n",
    "    long_df = long_df.dropna(subset=[\"V\"])\n",
    "    \n",
    "    return long_df, feature_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the TZV dataframes\n",
    "TZV_a, feature_to_index_a = build_TZV_dataframe(set_a_initial, death_a)\n",
    "TZV_b, feature_to_index_b = build_TZV_dataframe(set_b_initial, death_b)\n",
    "TZV_c, feature_to_index_c = build_TZV_dataframe(set_c_initial, death_c)\n",
    "\n",
    "print(TZV_a.shape)\n",
    "TZV_a.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the total number of not NaN values under some specified columns\n",
    "selected_cols = [col for col in set_a_initial.columns if col not in [\"RecordID\", \"Time\"]]\n",
    "set_a_initial[selected_cols].notna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checked that the number of not NaN values is the same as the rows of the new dataframe! Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the TZV Format with a Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the In-hospital_death column from the TZV dataframes, but save it\n",
    "y_a = TZV_a.pop(\"In-hospital_death\")\n",
    "y_b = TZV_b.pop(\"In-hospital_death\")\n",
    "y_c = TZV_c.pop(\"In-hospital_death\")\n",
    "\n",
    "# Convert the TZV dataframes to PyTorch tensors\n",
    "X_a = torch.tensor(TZV_a[[\"T\", \"Z\", \"V\"]].values, dtype=torch.float32)\n",
    "X_b = torch.tensor(TZV_b[[\"T\", \"Z\", \"V\"]].values, dtype=torch.float32)\n",
    "X_c = torch.tensor(TZV_c[[\"T\", \"Z\", \"V\"]].values, dtype=torch.float32)\n",
    "print(X_a.shape, X_b.shape, X_c.shape)\n",
    "\n",
    "# Create the datasets and dataloaders\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "dataset_a = TensorDataset(X_a, torch.tensor(y_a.values, dtype=torch.float32))\n",
    "dataset_b = TensorDataset(X_b, torch.tensor(y_b.values, dtype=torch.float32))\n",
    "dataset_c = TensorDataset(X_c, torch.tensor(y_c.values, dtype=torch.float32))\n",
    "\n",
    "# Use pin_memory and increase the num_workers for faster data loading on GPU\n",
    "loader_a = DataLoader(dataset_a, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "loader_b = DataLoader(dataset_b, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "loader_c = DataLoader(dataset_c, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop (beware, the size of the DFs makes it hard to run on simple laptops. We used the ETH Cluster to run it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tvz = TransformerClassifier(input_size=3).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model_tvz.parameters(), lr=0.001)\n",
    "\n",
    "model_tvz = train_model_with_validation(model_tvz, loader_a, loader_b, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "avg_loss, aucroc, auprc = evaluate_model(model_tvz, loader_c, criterion, device)\n",
    "print(f\"Test Loss: {avg_loss:.4f}, AUC-ROC: {aucroc:.4f}, AUC-PRC: {auprc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TUM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
