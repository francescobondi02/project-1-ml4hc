{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import parqet files",
   "id": "bce8c117462d5f5a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-27T16:05:15.719793Z",
     "start_time": "2025-03-27T16:05:14.683125Z"
    }
   },
   "source": [
    "# Load data from parquet file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from project_1.config import PROJ_ROOT, PROCESSED_DATA_DIR\n",
    "\n",
    "sets_dict = {}\n",
    "sets = [\"a\", \"b\", \"c\"]\n",
    "\n",
    "for set_name in sets:\n",
    "    directory = PROCESSED_DATA_DIR / f\"set_{set_name}/set_{set_name}_final.parquet\"\n",
    "    temp_set = pd.read_parquet(directory)\n",
    "    sets_dict[f\"set_{set_name}\"] = temp_set\n",
    "\n",
    "# Assure the loading was correct\n",
    "print(sets_dict[\"set_a\"].shape)\n",
    "print(type(sets_dict[\"set_a\"]))\n",
    "sets_dict[\"set_a\"].head(10)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-03-27 17:05:15.470\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mproject_1.config\u001B[0m:\u001B[36m<module>\u001B[0m:\u001B[36m11\u001B[0m - \u001B[1mPROJ_ROOT path is: C:\\Users\\tilma\\OneDrive\\Studium\\_Machine Learning for Healthcare\\Projects\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183416, 43)\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   RecordID                Time  Gender    Height    Weight       Age  \\\n",
       "0  132539.0 2025-03-10 00:00:00     0.0 -0.953317  0.283445 -0.596332   \n",
       "1  132539.0 2025-03-10 01:00:00     0.0 -0.953317  0.283445 -0.596332   \n",
       "2  132539.0 2025-03-10 02:00:00     0.0 -0.953317  0.283445 -0.596332   \n",
       "3  132539.0 2025-03-10 03:00:00     0.0 -0.953317  0.283445 -0.596332   \n",
       "4  132539.0 2025-03-10 04:00:00     0.0 -0.953317  0.283445 -0.596332   \n",
       "5  132539.0 2025-03-10 05:00:00     0.0 -0.953317  0.283445 -0.596332   \n",
       "6  132539.0 2025-03-10 06:00:00     0.0 -0.953317  0.283445 -0.596332   \n",
       "7  132539.0 2025-03-10 08:00:00     0.0 -0.953317  0.283445 -0.596332   \n",
       "8  132539.0 2025-03-10 09:00:00     0.0 -0.953317  0.283445 -0.596332   \n",
       "9  132539.0 2025-03-10 10:00:00     0.0 -0.953317  0.283445 -0.596332   \n",
       "\n",
       "    Albumin  Cholesterol   DiasABP      HCO3  ...  ICUType      K   Lactate  \\\n",
       "0  1.004449    -0.069047 -0.739159 -1.269243  ...      0.5  0.625 -0.583333   \n",
       "1 -0.232975     1.997003  0.232700 -2.924060  ...      0.5  0.375 -0.166667   \n",
       "2 -1.132919     2.628972  2.440210  0.385573  ...      0.5  0.625  2.000000   \n",
       "3 -0.120482    -1.721888  1.169851 -0.441835  ...      0.5 -1.000 -0.416667   \n",
       "4 -0.120482     1.462261 -0.392067 -2.510356  ...      0.5  0.125  0.000000   \n",
       "5  1.341928     1.146276 -0.235875  0.178721  ...      0.5 -2.125  1.666667   \n",
       "6  1.341928    -0.506564 -0.322648 -0.234983  ...      0.5  0.625  0.833333   \n",
       "7 -1.357905    -0.117660 -0.010265  1.419833  ...      0.5 -0.250  0.250000   \n",
       "8 -1.132919     0.344163  0.232700 -0.028131  ...      0.5 -0.250 -0.416667   \n",
       "9 -1.470399     0.076791  0.302119  0.385573  ...      0.5 -0.375  4.000000   \n",
       "\n",
       "   MechVent  Urine       WBC     pH  SaO2  TroponinT  TroponinI  \n",
       "0       0.0   2.96  0.271429 -0.750   1.0   0.933333  -0.224490  \n",
       "1       0.0   3.20  0.000000  0.750 -15.5  -0.177778   0.408163  \n",
       "2       0.0  -0.40  2.185714 -0.250   1.0   0.333333  -0.285714  \n",
       "3       0.0   0.72  2.185714 -0.250   0.5   0.333333   2.040816  \n",
       "4       0.0  -0.16 -0.957143  0.375   1.0   0.200000   0.795918  \n",
       "5       0.0  -0.16  1.557143 -0.375   0.0  -0.244444   0.183673  \n",
       "6       0.0   0.72  1.557143 -0.125   0.0   0.800000  -0.081633  \n",
       "7       0.0   0.32 -0.657143 -0.625   0.0  20.911111   8.408163  \n",
       "8       0.0   0.00 -0.657143 -0.250   0.0  -0.088889  -0.020408  \n",
       "9       0.0   0.16 -0.014286 -1.000   0.0   1.555556   0.040816  \n",
       "\n",
       "[10 rows x 43 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecordID</th>\n",
       "      <th>Time</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Age</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>DiasABP</th>\n",
       "      <th>HCO3</th>\n",
       "      <th>...</th>\n",
       "      <th>ICUType</th>\n",
       "      <th>K</th>\n",
       "      <th>Lactate</th>\n",
       "      <th>MechVent</th>\n",
       "      <th>Urine</th>\n",
       "      <th>WBC</th>\n",
       "      <th>pH</th>\n",
       "      <th>SaO2</th>\n",
       "      <th>TroponinT</th>\n",
       "      <th>TroponinI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132539.0</td>\n",
       "      <td>2025-03-10 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.953317</td>\n",
       "      <td>0.283445</td>\n",
       "      <td>-0.596332</td>\n",
       "      <td>1.004449</td>\n",
       "      <td>-0.069047</td>\n",
       "      <td>-0.739159</td>\n",
       "      <td>-1.269243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.625</td>\n",
       "      <td>-0.583333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.271429</td>\n",
       "      <td>-0.750</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>-0.224490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132539.0</td>\n",
       "      <td>2025-03-10 01:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.953317</td>\n",
       "      <td>0.283445</td>\n",
       "      <td>-0.596332</td>\n",
       "      <td>-0.232975</td>\n",
       "      <td>1.997003</td>\n",
       "      <td>0.232700</td>\n",
       "      <td>-2.924060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.375</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750</td>\n",
       "      <td>-15.5</td>\n",
       "      <td>-0.177778</td>\n",
       "      <td>0.408163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132539.0</td>\n",
       "      <td>2025-03-10 02:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.953317</td>\n",
       "      <td>0.283445</td>\n",
       "      <td>-0.596332</td>\n",
       "      <td>-1.132919</td>\n",
       "      <td>2.628972</td>\n",
       "      <td>2.440210</td>\n",
       "      <td>0.385573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.625</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>2.185714</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132539.0</td>\n",
       "      <td>2025-03-10 03:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.953317</td>\n",
       "      <td>0.283445</td>\n",
       "      <td>-0.596332</td>\n",
       "      <td>-0.120482</td>\n",
       "      <td>-1.721888</td>\n",
       "      <td>1.169851</td>\n",
       "      <td>-0.441835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>2.185714</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2.040816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132539.0</td>\n",
       "      <td>2025-03-10 04:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.953317</td>\n",
       "      <td>0.283445</td>\n",
       "      <td>-0.596332</td>\n",
       "      <td>-0.120482</td>\n",
       "      <td>1.462261</td>\n",
       "      <td>-0.392067</td>\n",
       "      <td>-2.510356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.957143</td>\n",
       "      <td>0.375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.795918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>132539.0</td>\n",
       "      <td>2025-03-10 05:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.953317</td>\n",
       "      <td>0.283445</td>\n",
       "      <td>-0.596332</td>\n",
       "      <td>1.341928</td>\n",
       "      <td>1.146276</td>\n",
       "      <td>-0.235875</td>\n",
       "      <td>0.178721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-2.125</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>1.557143</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.244444</td>\n",
       "      <td>0.183673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>132539.0</td>\n",
       "      <td>2025-03-10 06:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.953317</td>\n",
       "      <td>0.283445</td>\n",
       "      <td>-0.596332</td>\n",
       "      <td>1.341928</td>\n",
       "      <td>-0.506564</td>\n",
       "      <td>-0.322648</td>\n",
       "      <td>-0.234983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.557143</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.081633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>132539.0</td>\n",
       "      <td>2025-03-10 08:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.953317</td>\n",
       "      <td>0.283445</td>\n",
       "      <td>-0.596332</td>\n",
       "      <td>-1.357905</td>\n",
       "      <td>-0.117660</td>\n",
       "      <td>-0.010265</td>\n",
       "      <td>1.419833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.657143</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.911111</td>\n",
       "      <td>8.408163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>132539.0</td>\n",
       "      <td>2025-03-10 09:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.953317</td>\n",
       "      <td>0.283445</td>\n",
       "      <td>-0.596332</td>\n",
       "      <td>-1.132919</td>\n",
       "      <td>0.344163</td>\n",
       "      <td>0.232700</td>\n",
       "      <td>-0.028131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.657143</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.088889</td>\n",
       "      <td>-0.020408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>132539.0</td>\n",
       "      <td>2025-03-10 10:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.953317</td>\n",
       "      <td>0.283445</td>\n",
       "      <td>-0.596332</td>\n",
       "      <td>-1.470399</td>\n",
       "      <td>0.076791</td>\n",
       "      <td>0.302119</td>\n",
       "      <td>0.385573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.014286</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.555556</td>\n",
       "      <td>0.040816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 43 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:10:45.293654Z",
     "start_time": "2025-03-27T16:10:44.865133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_a = sets_dict[\"set_a\"]\n",
    "print(df_a.describe())\n",
    "print(df_a.columns)"
   ],
   "id": "3db8123fedfd784b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            RecordID                           Time         Gender  \\\n",
      "count  183416.000000                         183416  183416.000000   \n",
      "mean   137604.313779  2025-03-10 23:59:34.798273280       0.562081   \n",
      "min    132539.000000            2025-03-10 00:00:00       0.000000   \n",
      "25%    135072.000000            2025-03-10 12:00:00       0.000000   \n",
      "50%    137590.000000            2025-03-11 00:00:00       1.000000   \n",
      "75%    140101.000000            2025-03-11 12:00:00       1.000000   \n",
      "max    142673.000000            2025-03-12 00:00:00       1.000000   \n",
      "std      2923.639911                            NaN       0.496002   \n",
      "\n",
      "             Height        Weight           Age       Albumin   Cholesterol  \\\n",
      "count  1.834160e+05  1.834160e+05  1.834160e+05  1.834160e+05  1.834160e+05   \n",
      "mean   6.454295e-15  4.456582e-16 -2.810931e-16 -5.493248e-16  2.112266e-16   \n",
      "min   -5.158858e+00 -2.611045e+00 -2.835966e+00 -2.482836e+00 -3.083051e+00   \n",
      "25%   -7.575847e-01 -6.533555e-01 -6.537587e-01 -7.954402e-01 -6.524029e-01   \n",
      "50%    6.427466e-02 -1.124727e-01  1.502123e-01 -1.204819e-01 -9.335391e-02   \n",
      "75%    7.487970e-01  4.415489e-01  7.819038e-01  6.669695e-01  6.115340e-01   \n",
      "max    3.903223e+00  8.701508e+00  1.471022e+00  2.354365e+00  4.257506e+00   \n",
      "std    1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00   \n",
      "\n",
      "            DiasABP          HCO3  ...        ICUType              K  \\\n",
      "count  1.834160e+05  1.834160e+05  ...  183416.000000  183416.000000   \n",
      "mean  -9.833611e-16  7.778873e-17  ...      -0.117732       0.033591   \n",
      "min   -4.210086e+00 -3.751468e+00  ...      -1.000000      -2.875000   \n",
      "25%   -6.003222e-01 -4.418354e-01  ...      -0.500000      -0.500000   \n",
      "50%   -1.143924e-01 -2.813134e-02  ...       0.000000       0.000000   \n",
      "75%    5.103745e-01  5.924247e-01  ...       0.500000       0.500000   \n",
      "max    1.439408e+01  5.556873e+00  ...       0.500000      23.500000   \n",
      "std    1.000003e+00  1.000003e+00  ...       0.500697       0.775992   \n",
      "\n",
      "             Lactate  MechVent          Urine            WBC             pH  \\\n",
      "count  183416.000000  183416.0  183416.000000  183416.000000  183416.000000   \n",
      "mean        0.470553       0.0       0.455527       0.201051       0.399041   \n",
      "min        -1.250000       0.0      -0.640000      -1.628571     -79.875000   \n",
      "25%        -0.416667       0.0      -0.320000      -0.428571      -0.500000   \n",
      "50%         0.000000       0.0       0.000000       0.000000       0.000000   \n",
      "75%         0.583333       0.0       0.680000       0.571429       0.500000   \n",
      "max        22.916667       0.0      87.360000      25.142857    2245.125000   \n",
      "std         1.545430       0.0       1.501123       1.011034      23.908399   \n",
      "\n",
      "                SaO2      TroponinT      TroponinI  \n",
      "count  183416.000000  183416.000000  183416.000000  \n",
      "mean       -0.401532       1.321817       0.965093  \n",
      "min       -35.500000      -0.266667      -0.285714  \n",
      "25%        -0.500000      -0.200000      -0.204082  \n",
      "50%         0.000000       0.000000       0.000000  \n",
      "75%         0.500000       0.800000       0.795918  \n",
      "max         1.500000      55.066667       9.693878  \n",
      "std         2.288825       4.000606       2.085210  \n",
      "\n",
      "[8 rows x 43 columns]\n",
      "Index(['RecordID', 'Time', 'Gender', 'Height', 'Weight', 'Age', 'Albumin',\n",
      "       'Cholesterol', 'DiasABP', 'HCO3', 'HCT', 'HR', 'Mg', 'MAP', 'Na',\n",
      "       'NIDiasABP', 'NIMAP', 'NISysABP', 'SysABP', 'PaCO2', 'PaO2',\n",
      "       'Platelets', 'RespRate', 'Temp', 'ALP', 'ALT', 'AST', 'BUN',\n",
      "       'Bilirubin', 'Creatinine', 'FiO2', 'GCS', 'Glucose', 'ICUType', 'K',\n",
      "       'Lactate', 'MechVent', 'Urine', 'WBC', 'pH', 'SaO2', 'TroponinT',\n",
      "       'TroponinI'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Q4.1 Prompting an LLM to solve a time-series problem",
   "id": "f42c91efec2071cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Patient Summary short\n",
    "only includes the most important data, without much noise"
   ],
   "id": "4dfeb97539d07fb1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:12:16.198187Z",
     "start_time": "2025-03-27T16:12:16.189897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_summary_statistical(patient_df):\n",
    "    features = {}\n",
    "    # Assuming df is already filtered for a single patient\n",
    "    features['age'] = patient_df['Age'].iloc[0]\n",
    "    features['gender'] = \"male\" if patient_df['Gender'].iloc[0] == 1 else \"female\"\n",
    "\n",
    "    summary = [\n",
    "        f\"Patient is a {int(features['age']*40+60)}-year-old {features['gender']}. Over the first 48 hours:\"\n",
    "    ]\n",
    "\n",
    "    vital_vars = ['HeartRate', 'SaO2', 'WBC', 'Urine', 'pH', 'Lactate']\n",
    "    for var in vital_vars:\n",
    "        if var not in patient_df.columns: continue\n",
    "        values = patient_df[var].dropna()\n",
    "        if len(values) == 0: continue\n",
    "        min_val, max_val = values.min(), values.max()\n",
    "        mean_val, last_val = values.iloc[-1], values.iloc[-1]\n",
    "        summary.append(f\"- {var} ranged from {min_val:.2f} to {max_val:.2f}, avg: {mean_val:.2f}, last: {last_val:.2f}\")\n",
    "\n",
    "    summary.append(\"Predict the risk of in-hospital death on a scale from 1 to 10.\")\n",
    "    return \"\\n\".join(summary)\n"
   ],
   "id": "407db89e4ad17a6c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Patient Summary trends\n",
    "Summarize variable behavior over three time segments (0–16h, 16–32h, 32–48h) using trends: increasing, decreasing, stable, or fluctuating. Adds a temporal element that can help the LLM infer progression."
   ],
   "id": "4c59195e8cf99789"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:17:10.164646Z",
     "start_time": "2025-03-27T16:17:10.154218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_trend_label(seq):\n",
    "    if len(seq) < 2: return \"unknown\"\n",
    "    d1, d2, d3 = np.mean(seq[:len(seq)//3]), np.mean(seq[len(seq)//3:2*len(seq)//3]), np.mean(seq[2*len(seq)//3:])\n",
    "    if d1 < d2 < d3: return \"↑\"\n",
    "    elif d1 > d2 > d3: return \"↓\"\n",
    "    elif abs(d1 - d2) < 0.1 and abs(d2 - d3) < 0.1: return \"→\"\n",
    "    else: return \"~\"\n",
    "\n",
    "def generate_summary_trend(patient_df):\n",
    "    features = {}\n",
    "    features['age'] = patient_df['Age'].iloc[0]\n",
    "    features['gender'] = \"male\" if patient_df['Gender'].iloc[0] == 1 else \"female\"\n",
    "\n",
    "    summary = [\n",
    "        f\"Patient is a {int(features['age']*40+60)}-year-old {features['gender']}. Over the first 48 hours:\"\n",
    "    ]\n",
    "\n",
    "    vital_vars = ['HeartRate', 'SaO2', 'WBC', 'Urine', 'pH', 'Lactate']\n",
    "    for var in vital_vars:\n",
    "        if var not in patient_df.columns: continue\n",
    "        series = patient_df[var].dropna().values\n",
    "        if len(series) < 3: continue\n",
    "        trend = get_trend_label(series)\n",
    "        summary.append(f\"- {var} shows a trend of {trend}\")\n",
    "\n",
    "    summary.append(\"Predict the patient's in-hospital death risk.\")\n",
    "    return \"\\n\".join(summary)\n"
   ],
   "id": "acc24390a7a6c87a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:17:50.971038Z",
     "start_time": "2025-03-27T16:17:46.440223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_patient_summaries(df):\n",
    "    result_rows = []\n",
    "\n",
    "    for rid, group in df.groupby(\"RecordID\"):\n",
    "        summary_stat = generate_summary_statistical(group)\n",
    "        summary_trend = generate_summary_trend(group)\n",
    "\n",
    "        result_rows.append({\n",
    "            \"RecordID\": rid,\n",
    "            \"summary_statistical\": summary_stat,\n",
    "            \"summary_trend\": summary_trend\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(result_rows)\n",
    "\n",
    "# ---------- Usage ---------- #\n",
    "\n",
    "# Assuming df_a is your full DataFrame\n",
    "summary_df = generate_patient_summaries(df_a)\n",
    "\n",
    "# Show a few example summaries\n",
    "summary_df.head(2)"
   ],
   "id": "4d9c0387622af26d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   RecordID                                summary_statistical  \\\n",
       "0  132539.0  Patient is a 36-year-old female. Over the firs...   \n",
       "1  132540.0  Patient is a 86-year-old male. Over the first ...   \n",
       "\n",
       "                                       summary_trend  \n",
       "0  Patient is a 36-year-old female. Over the firs...  \n",
       "1  Patient is a 86-year-old male. Over the first ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecordID</th>\n",
       "      <th>summary_statistical</th>\n",
       "      <th>summary_trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132539.0</td>\n",
       "      <td>Patient is a 36-year-old female. Over the firs...</td>\n",
       "      <td>Patient is a 36-year-old female. Over the firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132540.0</td>\n",
       "      <td>Patient is a 86-year-old male. Over the first ...</td>\n",
       "      <td>Patient is a 86-year-old male. Over the first ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:34:14.592497Z",
     "start_time": "2025-03-27T16:34:14.493098Z"
    }
   },
   "cell_type": "code",
   "source": "(summary_df.head(1)[\"summary_trend\"])",
   "id": "ae439ea2ab57e9c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Patient is a 36-year-old female. Over the firs...\n",
       "Name: summary_trend, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Ollama Test",
   "id": "843b8ad6805e4bd3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:54:27.570399Z",
     "start_time": "2025-03-27T16:54:07.364088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='gemma3:1b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "# print(response.message.content)"
   ],
   "id": "8aa2ebe6f920330f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky is blue due to a phenomenon called **Rayleigh scattering**. Here's a breakdown of why it happens:\n",
      "\n",
      "* **Sunlight is made of all colors:**  Sunlight appears white to us, but it's actually composed of all the colors of the rainbow – red, orange, yellow, green, blue, indigo, and violet.\n",
      "\n",
      "* **Entering the atmosphere:** As sunlight enters the Earth's atmosphere, it collides with tiny air molecules (mostly nitrogen and oxygen).\n",
      "\n",
      "* **Rayleigh scattering:**  This collision causes the light to scatter in different directions.  **Rayleigh scattering is much more effective at shorter wavelengths of light, like blue and violet.**  Think of it like this: smaller particles scatter light more easily than larger ones.\n",
      "\n",
      "* **Why blue and not violet?** Violet light is scattered even more than blue, but our eyes are more sensitive to blue light, and the sun emits slightly less violet.  Also, some violet is absorbed by the upper atmosphere.\n",
      "\n",
      "**In simpler terms:** Imagine throwing a handful of bouncy balls (blue light) and marbles (red light) at a bumpy surface. The bouncy balls will bounce around more easily, spreading out in all directions.  That's what happens with sunlight.\n",
      "\n",
      "**Therefore, the blue color we see is the result of blue light being scattered more than other colors by the air molecules in the atmosphere.**\n",
      "\n",
      "**Why are sunsets red/orange?**\n",
      "\n",
      "At sunset, the sunlight has to travel through *more* of the atmosphere. This longer path causes even *more* scattering of blue light, leaving the longer wavelengths like red and orange to dominate.\n",
      "\n",
      "---\n",
      "\n",
      "**Resources for more information:**\n",
      "\n",
      "* **NASA - Rayleigh Scattering:** [https://science.nasa.gov/explorations/space/sun/rayleigh-scattering/](https://science.nasa.gov/explorations/space/sun/rayleigh-scattering/)\n",
      "* **Wikipedia - Rayleigh Scattering:** [https://en.wikipedia.org/wiki/Rayleigh_scattering](https://en.wikipedia.org/wiki/Rayleigh_scattering)\n",
      "\n",
      "Do you want to know more about:\n",
      "\n",
      "*   Why are sunsets red?\n",
      "*   How does the atmosphere affect the color of the sky?\n",
      "The sky is blue due to a phenomenon called **Rayleigh scattering**. Here's a breakdown of why it happens:\n",
      "\n",
      "* **Sunlight is made of all colors:**  Sunlight appears white to us, but it's actually composed of all the colors of the rainbow – red, orange, yellow, green, blue, indigo, and violet.\n",
      "\n",
      "* **Entering the atmosphere:** As sunlight enters the Earth's atmosphere, it collides with tiny air molecules (mostly nitrogen and oxygen).\n",
      "\n",
      "* **Rayleigh scattering:**  This collision causes the light to scatter in different directions.  **Rayleigh scattering is much more effective at shorter wavelengths of light, like blue and violet.**  Think of it like this: smaller particles scatter light more easily than larger ones.\n",
      "\n",
      "* **Why blue and not violet?** Violet light is scattered even more than blue, but our eyes are more sensitive to blue light, and the sun emits slightly less violet.  Also, some violet is absorbed by the upper atmosphere.\n",
      "\n",
      "**In simpler terms:** Imagine throwing a handful of bouncy balls (blue light) and marbles (red light) at a bumpy surface. The bouncy balls will bounce around more easily, spreading out in all directions.  That's what happens with sunlight.\n",
      "\n",
      "**Therefore, the blue color we see is the result of blue light being scattered more than other colors by the air molecules in the atmosphere.**\n",
      "\n",
      "**Why are sunsets red/orange?**\n",
      "\n",
      "At sunset, the sunlight has to travel through *more* of the atmosphere. This longer path causes even *more* scattering of blue light, leaving the longer wavelengths like red and orange to dominate.\n",
      "\n",
      "---\n",
      "\n",
      "**Resources for more information:**\n",
      "\n",
      "* **NASA - Rayleigh Scattering:** [https://science.nasa.gov/explorations/space/sun/rayleigh-scattering/](https://science.nasa.gov/explorations/space/sun/rayleigh-scattering/)\n",
      "* **Wikipedia - Rayleigh Scattering:** [https://en.wikipedia.org/wiki/Rayleigh_scattering](https://en.wikipedia.org/wiki/Rayleigh_scattering)\n",
      "\n",
      "Do you want to know more about:\n",
      "\n",
      "*   Why are sunsets red?\n",
      "*   How does the atmosphere affect the color of the sky?\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Result Calculation",
   "id": "76628fa5b1bccee2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T23:34:52.989767Z",
     "start_time": "2025-03-29T23:34:46.879484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ollama import chat\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ---- Utility: Extract score or binary ---- #\n",
    "\n",
    "def extract_score_from_response(text, mode=\"score\"):\n",
    "    \"\"\"\n",
    "    Extracts a number from model output or interprets binary label.\n",
    "    \"\"\"\n",
    "    if mode == \"score\":\n",
    "        match = re.search(r'(\\d+(\\.\\d+)?)', text)\n",
    "        if match:\n",
    "            val = float(match.group(1))\n",
    "            return min(max(val, 1.0), 10.0) / 10.0  # normalize 1-10 to 0-1\n",
    "    else:  # binary mode\n",
    "        text = text.lower()\n",
    "        if any(word in text for word in [\"yes\", \"high\", \"die\", \"likely\"]):\n",
    "            return 1.0\n",
    "        elif any(word in text for word in [\"no\", \"low\", \"survive\", \"unlikely\"]):\n",
    "            return 0.0\n",
    "    return 0.5  # fallback neutral\n",
    "\n",
    "# ---- Few-shot formatter ---- #\n",
    "\n",
    "def format_few_shot_examples(df_train, label_col, text_col, max_examples=3, mode=\"score\"):\n",
    "    \"\"\"\n",
    "    Creates few-shot context examples from labeled training set.\n",
    "    \"\"\"\n",
    "    examples = df_train.sample(n=max_examples, random_state=42)\n",
    "    formatted = \"\"\n",
    "    for _, row in examples.iterrows():\n",
    "        label = row[label_col]\n",
    "        label_text = f\"{int(label * 9 + 1)}/10\" if mode == \"score\" else (\"Yes\" if label == 1 else \"No\")\n",
    "        formatted += f\"Input:\\n{row[text_col]}\\nAnswer: {label_text}\\n\\n\"\n",
    "    return formatted.strip()\n",
    "\n",
    "# ---- Core inference function ---- #\n",
    "\n",
    "def query_llm_gemma(summary_text, few_shot_context=None):\n",
    "    \"\"\"\n",
    "    Calls gemma3:1b model via ollama.chat using user prompt.\n",
    "    \"\"\"\n",
    "    prompt = f\"{few_shot_context}\\n\\nInput:\\n{summary_text}\\nAnswer:\" if few_shot_context else f\"Input:\\n{summary_text}\\nAnswer:\"\n",
    "    \n",
    "    response = chat(model='gemma3:1b', messages=[{\n",
    "        'role': 'user',\n",
    "        'content': prompt\n",
    "    }])\n",
    "    \n",
    "    return response['message']['content']\n",
    "\n",
    "# ---- Full evaluation runner ---- #\n",
    "\n",
    "def evaluate_llm_predictions(summary_df, label_col, text_col='summary_statistical', mode=\"score\",\n",
    "                              few_shot=False, df_train_for_examples=None):\n",
    "    \"\"\"\n",
    "    Runs inference across all rows in summary_df and returns performance + predictions.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    labels = summary_df[label_col].values\n",
    "\n",
    "    # Build few-shot context once\n",
    "    context = None\n",
    "    if few_shot and df_train_for_examples is not None:\n",
    "        context = format_few_shot_examples(df_train_for_examples, label_col, text_col, max_examples=3, mode=mode)\n",
    "\n",
    "    for _, row in summary_df.iterrows():\n",
    "        summary = row[text_col]\n",
    "        llm_output = query_llm_gemma(summary, few_shot_context=context)\n",
    "        score = extract_score_from_response(llm_output, mode=mode)\n",
    "        predictions.append(score)\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    try:\n",
    "        auroc = roc_auc_score(labels, predictions)\n",
    "        auprc = average_precision_score(labels, predictions)\n",
    "    except:\n",
    "        auroc = None\n",
    "        auprc = None\n",
    "\n",
    "    return {\n",
    "        \"predictions\": predictions,\n",
    "        \"true_labels\": labels,\n",
    "        \"auroc\": auroc,\n",
    "        \"auprc\": auprc\n",
    "    }\n"
   ],
   "id": "acb7a611c9105d49",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Zero-Shot, Score-Mode",
   "id": "743e173f29230f25"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T23:37:44.192380Z",
     "start_time": "2025-03-29T23:37:41.369981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = evaluate_llm_predictions(\n",
    "    summary_df,\n",
    "    label_col='label',\n",
    "    text_col='summary_statistical',\n",
    "    mode='score'\n",
    ")\n",
    "print(\"Zero-shot score mode →\", results[\"auroc\"], results[\"auprc\"])\n"
   ],
   "id": "fd50bb88808cfcc4",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\PycharmProjects\\wwf\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mindex.pyx:196\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_llm_predictions\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43msummary_df\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlabel_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlabel\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtext_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msummary_statistical\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mscore\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\n\u001B[0;32m      6\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mZero-shot score mode →\u001B[39m\u001B[38;5;124m\"\u001B[39m, results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauroc\u001B[39m\u001B[38;5;124m\"\u001B[39m], results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauprc\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "Cell \u001B[1;32mIn[13], line 63\u001B[0m, in \u001B[0;36mevaluate_llm_predictions\u001B[1;34m(summary_df, label_col, text_col, mode, few_shot, df_train_for_examples)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;124;03mRuns inference across all rows in summary_df and returns performance + predictions.\u001B[39;00m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     62\u001B[0m predictions \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m---> 63\u001B[0m labels \u001B[38;5;241m=\u001B[39m \u001B[43msummary_df\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlabel_col\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mvalues\n\u001B[0;32m     65\u001B[0m \u001B[38;5;66;03m# Build few-shot context once\u001B[39;00m\n\u001B[0;32m     66\u001B[0m context \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\wwf\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   4100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   4101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 4102\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   4104\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\PycharmProjects\\wwf\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[0;32m   3808\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[0;32m   3809\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[0;32m   3810\u001B[0m     ):\n\u001B[0;32m   3811\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[1;32m-> 3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3817\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'label'"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Zero-Shot, Binary-Mode",
   "id": "f7f8785c9adaa90c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results = evaluate_llm_predictions(\n",
    "    summary_df,\n",
    "    label_col='label',\n",
    "    text_col='summary_trend',\n",
    "    mode='binary'\n",
    ")\n",
    "print(\"Zero-shot binary mode →\", results[\"auroc\"], results[\"auprc\"])\n"
   ],
   "id": "d321d998fa7a35da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Few-Shot, Score-Mode",
   "id": "45794f9ffe8ba3bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results = evaluate_llm_predictions(\n",
    "    summary_df,\n",
    "    label_col='label',\n",
    "    text_col='summary_statistical',\n",
    "    mode='score',\n",
    "    few_shot=True,\n",
    "    df_train_for_examples=train_df  # Must be labeled\n",
    ")\n",
    "print(\"Few-shot score mode →\", results[\"auroc\"], results[\"auprc\"])\n"
   ],
   "id": "b990cae0707a720b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Q4.2 Using LLs to retrieve embeddings",
   "id": "4e8de72bc89e77f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ollama import embedding\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_embedding_ollama(text: str, model_name='gemma3:1b'):\n",
    "    \"\"\"\n",
    "    Calls the Ollama embedding API for a given string.\n",
    "    \"\"\"\n",
    "    response = embedding(model=model_name, prompt=text)\n",
    "    return np.array(response.get(\"embedding\", []), dtype=float)\n",
    "\n",
    "def build_embeddings(df, summary_col=\"summary_statistical\", model_name='gemma3:1b'):\n",
    "    \"\"\"\n",
    "    For each row in the DataFrame, get an embedding and corresponding label.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        text = row[summary_col]\n",
    "        label = row[\"label\"]\n",
    "        emb = get_embedding_ollama(text, model_name=model_name)\n",
    "        embeddings.append(emb)\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.vstack(embeddings), np.array(labels)\n",
    "\n",
    "def train_and_evaluate(train_X, train_y, test_X, test_y):\n",
    "    clf = LogisticRegression(max_iter=2000)\n",
    "    clf.fit(train_X, train_y)\n",
    "    probs = clf.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    auroc = roc_auc_score(test_y, probs)\n",
    "    auprc = average_precision_score(test_y, probs)\n",
    "    return probs, auroc, auprc\n",
    "\n",
    "def visualize_tsne(embeddings, labels, title=\"t-SNE of Embeddings\"):\n",
    "    tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "    emb_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    scatter = plt.scatter(emb_2d[:,0], emb_2d[:,1], c=labels, cmap='coolwarm', alpha=0.7)\n",
    "    plt.colorbar(scatter, label=\"In-hospital Death (0/1)\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"TSNE-1\")\n",
    "    plt.ylabel(\"TSNE-2\")\n",
    "    plt.show()\n"
   ],
   "id": "6139098cde238d2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Replace df_train and df_test with your actual DataFrames\n",
    "\n",
    "train_X, train_y = build_embeddings(df_train, summary_col=\"summary_statistical\", model_name='gemma3:1b')\n",
    "test_X, test_y = build_embeddings(df_test, summary_col=\"summary_statistical\", model_name='gemma3:1b')\n",
    "\n",
    "probs, auroc, auprc = train_and_evaluate(train_X, train_y, test_X, test_y)\n",
    "print(f\"Test AuROC: {auroc:.4f}, Test AuPRC: {auprc:.4f}\")\n",
    "\n",
    "visualize_tsne(test_X, test_y, title=\"t-SNE of gemma3:1b Embeddings (Test Set)\")\n"
   ],
   "id": "92e51cedfeafddef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Q4.3 Using time-series foundation models",
   "id": "664018ffea9814f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T00:06:14.667775Z",
     "start_time": "2025-03-30T00:05:10.514842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# If not installed yet:\n",
    "!pip install git+https://github.com/amazon-science/chronos-forecasting.git"
   ],
   "id": "86002edfdf0f514f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/amazon-science/chronos-forecasting.git\n",
      "  Cloning https://github.com/amazon-science/chronos-forecasting.git to c:\\users\\tilma\\appdata\\local\\temp\\pip-req-build-h6u10xk3\n",
      "  Resolved https://github.com/amazon-science/chronos-forecasting.git to commit 94e20ea7e510ac4d665492b8bed8836a5143f16e\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting accelerate<1,>=0.32 (from chronos-forecasting==1.5.0)\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in c:\\users\\tilma\\pycharmprojects\\wwf\\venv\\lib\\site-packages (from chronos-forecasting==1.5.0) (2.6.0)\n",
      "Collecting transformers<5,>=4.48 (from chronos-forecasting==1.5.0)\n",
      "  Downloading transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\tilma\\pycharmprojects\\wwf\\venv\\lib\\site-packages (from accelerate<1,>=0.32->chronos-forecasting==1.5.0) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tilma\\pycharmprojects\\wwf\\venv\\lib\\site-packages (from accelerate<1,>=0.32->chronos-forecasting==1.5.0) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\tilma\\pycharmprojects\\wwf\\venv\\lib\\site-packages (from accelerate<1,>=0.32->chronos-forecasting==1.5.0) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\tilma\\pycharmprojects\\wwf\\venv\\lib\\site-packages (from accelerate<1,>=0.32->chronos-forecasting==1.5.0) (6.0.2)\n",
      "Collecting huggingface-hub>=0.21.0 (from accelerate<1,>=0.32->chronos-forecasting==1.5.0)\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors>=0.4.3 (from accelerate<1,>=0.32->chronos-forecasting==1.5.0)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\tilma\\pycharmprojects\\wwf\\venv\\lib\\site-packages (from torch<3,>=2.0->chronos-forecasting==1.5.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\tilma\\pycharmprojects\\wwf\\venv\\lib\\site-packages (from torch<3,>=2.0->chronos-forecasting==1.5.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\tilma\\pycharmprojects\\wwf\\venv\\lib\\site-packages (from torch<3,>=2.0->chronos-forecasting==1.5.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tilma\\pycharmprojects\\wwf\\venv\\lib\\site-packages (from torch<3,>=2.0->chronos-forecasting==1.5.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tilma\\pycharmprojects\\wwf\\venv\\lib\\site-packages (from torch<3,>=2.0->chronos-forecasting==1.5.0) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tilma\\pycharmprojects\\wwf\\venv\\lib\\site-packages (from torch<3,>=2.0->chronos-forecasting==1.5.0) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\tilma\\pycharmprojects\\wwf\\venv\\lib\\site-packages (from torch<3,>=2.0->chronos-forecasting==1.5.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tilma\\pycharmprojects\\wwf\\venv\\lib\\site-packages (from sympy==1.13.1->torch<3,>=2.0->chronos-forecasting==1.5.0) (1.3.0)\n",
      "Collecting regex!=2019.12.17 (from transformers<5,>=4.48->chronos-forecasting==1.5.0)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\tilma\\pycharmprojects\\wwf\\venv\\lib\\site-packages (from transformers<5,>=4.48->chronos-forecasting==1.5.0) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5,>=4.48->chronos-forecasting==1.5.0)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tilma\\pycharmprojects\\wwf\\venv\\lib\\site-packages (from transformers<5,>=4.48->chronos-forecasting==1.5.0) (4.66.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\tilma\\pycharmprojects\\wwf\\venv\\lib\\site-packages (from tqdm>=4.27->transformers<5,>=4.48->chronos-forecasting==1.5.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tilma\\pycharmprojects\\wwf\\venv\\lib\\site-packages (from jinja2->torch<3,>=2.0->chronos-forecasting==1.5.0) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tilma\\pycharmprojects\\wwf\\venv\\lib\\site-packages (from requests->transformers<5,>=4.48->chronos-forecasting==1.5.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tilma\\pycharmprojects\\wwf\\venv\\lib\\site-packages (from requests->transformers<5,>=4.48->chronos-forecasting==1.5.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tilma\\pycharmprojects\\wwf\\venv\\lib\\site-packages (from requests->transformers<5,>=4.48->chronos-forecasting==1.5.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tilma\\pycharmprojects\\wwf\\venv\\lib\\site-packages (from requests->transformers<5,>=4.48->chronos-forecasting==1.5.0) (2024.8.30)\n",
      "Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "Downloading transformers-4.50.3-py3-none-any.whl (10.2 MB)\n",
      "   ---------------------------------------- 0.0/10.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.8/10.2 MB 8.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.4/10.2 MB 11.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.7/10.2 MB 13.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.6/10.2 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.2 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.2/10.2 MB 11.1 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 15.5 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: chronos-forecasting\n",
      "  Building wheel for chronos-forecasting (pyproject.toml): started\n",
      "  Building wheel for chronos-forecasting (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for chronos-forecasting: filename=chronos_forecasting-1.5.0-py3-none-any.whl size=29511 sha256=95dd5fe33fe6382c5bb039038af5571e7db4d769c0e55defe313bd19cb57acd6\n",
      "  Stored in directory: C:\\Users\\tilma\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-uxkekx3o\\wheels\\b9\\a6\\b5\\75fca7306751a3bc92a63680f861f44a42a8776f6423cf0188\n",
      "Successfully built chronos-forecasting\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, accelerate, transformers, chronos-forecasting\n",
      "Successfully installed accelerate-0.34.2 chronos-forecasting-1.5.0 huggingface-hub-0.29.3 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.50.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/amazon-science/chronos-forecasting.git 'C:\\Users\\tilma\\AppData\\Local\\Temp\\pip-req-build-h6u10xk3'\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from chronos import ChronosPipeline\n",
    "\n",
    "pipeline = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/AileenNielsen/TimeSeriesAnalysisWithPython/master/data/AirPassengers.csv\")\n",
    "\n",
    "# context must be either a 1D tensor, a list of 1D tensors,\n",
    "# or a left-padded 2D tensor with batch as the first dimension\n",
    "context = torch.tensor(df[\"#Passengers\"])\n",
    "embeddings, tokenizer_state = pipeline.embed(context)\n",
    "\n",
    "# 1) Single Embedding + Linear Probe\n",
    "train_X, train_y = build_patient_embeddings_naive(pretrained_model, df_train)\n",
    "test_X, test_y = build_patient_embeddings_naive(pretrained_model, df_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=2000)\n",
    "clf.fit(train_X, train_y)\n",
    "probs = clf.predict_proba(test_X)[:,1]\n",
    "auroc = roc_auc_score(test_y, probs)\n",
    "auprc = average_precision_score(test_y, probs)\n",
    "print(f\"[Naive Aggregation] AuROC={auroc:.4f}, AuPRC={auprc:.4f}\")"
   ],
   "id": "6fa268502a2e4cd7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Making a smarter aggregation: \n",
    "Instead of simply averaging embeddings across variables, we can:\n",
    "\n",
    "Compute an embedding for each variable.\n",
    "\n",
    "Feed all variable embeddings into a small MLP that outputs a patient-level embedding.\n",
    "\n",
    "Then, either attach a logistic head for classification or just produce a final embedding for a standard logistic regression."
   ],
   "id": "ab490a84efe768c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "########################################\n",
    "# 2) Simple Neural Aggregator Approach #\n",
    "########################################\n",
    "\n",
    "class ChannelAggregator(nn.Module):\n",
    "    def __init__(self, embed_dim=128, hidden_dim=64, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(embed_dim*10, hidden_dim),  # if you expect ~10 variables\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "def build_multivar_embeddings(model, multivar_data):\n",
    "    embeddings = []\n",
    "    for var_name, series in multivar_data.items():\n",
    "        emb = get_univariate_embedding(model, series)\n",
    "        embeddings.append(emb.unsqueeze(0))\n",
    "    if len(embeddings) == 0:\n",
    "        return torch.zeros((1, 128))\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "def train_aggregator(pretrained_model, aggregator, df_patients, epochs=5, lr=1e-3, device='cpu'):\n",
    "    aggregator.to(device)\n",
    "    optimizer = torch.optim.Adam(aggregator.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Build full dataset in memory\n",
    "    X_list, y_list = [], []\n",
    "    for _, row in df_patients.iterrows():\n",
    "        data_dict = row['multivar_data']\n",
    "        emb_stack = build_multivar_embeddings(pretrained_model, data_dict)\n",
    "        X_list.append(emb_stack.unsqueeze(0)) # shape [1, num_vars, embed_dim]\n",
    "        y_list.append(row['label'])\n",
    "    \n",
    "    X_tensor = torch.cat(X_list, dim=0).float().to(device)\n",
    "    y_tensor = torch.tensor(y_list, dtype=torch.float).unsqueeze(-1).to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        aggregator.train()\n",
    "        optimizer.zero_grad()\n",
    "        logits = aggregator(X_tensor)\n",
    "        loss = criterion(logits, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss={loss.item():.4f}\")\n",
    "    \n",
    "    return aggregator\n",
    "\n",
    "def evaluate_aggregator(pretrained_model, aggregator, df_patients, device='cpu'):\n",
    "    aggregator.eval()\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, row in df_patients.iterrows():\n",
    "            data_dict = row['multivar_data']\n",
    "            emb_stack = build_multivar_embeddings(pretrained_model, data_dict).unsqueeze(0).to(device)\n",
    "            logit = aggregator(emb_stack)\n",
    "            all_logits.append(logit.item())\n",
    "            all_labels.append(row['label'])\n",
    "    \n",
    "    all_logits = np.array(all_logits)\n",
    "    all_labels = np.array(all_labels)\n",
    "    probs = 1 / (1 + np.exp(-all_logits))\n",
    "\n",
    "    auroc = roc_auc_score(all_labels, probs)\n",
    "    auprc = average_precision_score(all_labels, probs)\n",
    "    return auroc, auprc\n",
    "\n",
    "aggregator = ChannelAggregator(embed_dim=128, hidden_dim=64, out_dim=1)\n",
    "aggregator = train_aggregator(pretrained_model, aggregator, df_train, epochs=5, lr=1e-3, device='cpu')\n",
    "auroc, auprc = evaluate_aggregator(pretrained_model, aggregator, df_test)\n",
    "print(f\"[Neural Aggregator] AuROC={auroc:.4f}, AuPRC={auprc:.4f}\")"
   ],
   "id": "1ac39c3348377844"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
